{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vit_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzOe1JnYx1xJELN3218Bri",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fff45d45db71477eb29270887a00a8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d46e631b69224e26ba94f5f4c42ecf3f",
              "IPY_MODEL_5ddabd2fb142436eafc142fcb2009d17",
              "IPY_MODEL_d7c5c3337701443e896c7d821e792dd1"
            ],
            "layout": "IPY_MODEL_89d5cff8787e4bdda889b0a1fa3b034f"
          }
        },
        "d46e631b69224e26ba94f5f4c42ecf3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf35a05bda92403584cdce1d121e53dd",
            "placeholder": "​",
            "style": "IPY_MODEL_98ef13bbe39947f48eb0049644d3ffa5",
            "value": "100%"
          }
        },
        "5ddabd2fb142436eafc142fcb2009d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4734ea87158c4a94a50dcc1415422bf3",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_683f7831b234426eb99e627acc6cafa2",
            "value": 102530333
          }
        },
        "d7c5c3337701443e896c7d821e792dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e9c219f4406455aa78a2f2f9f9f0cb7",
            "placeholder": "​",
            "style": "IPY_MODEL_f3b0d1a621274016bb51fe46dcbef48b",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 226MB/s]"
          }
        },
        "89d5cff8787e4bdda889b0a1fa3b034f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf35a05bda92403584cdce1d121e53dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98ef13bbe39947f48eb0049644d3ffa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4734ea87158c4a94a50dcc1415422bf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "683f7831b234426eb99e627acc6cafa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e9c219f4406455aa78a2f2f9f9f0cb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3b0d1a621274016bb51fe46dcbef48b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D10752002/vit_domain_adaption_pytorch/blob/main/vit_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Eo0YwUudthWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3cf6e4d-3d42-4691-c84f-781428f46c08"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jUAqjadYkOM",
        "outputId": "8ab1b588-2f93-4646-e6f1-34aa8721a92d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 30.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting fsspec[http]>=2021.11.1\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 69.4 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 47.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.8 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 75.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 72.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 66.9 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 69.9 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.4.0 frozenlist-1.3.0 fsspec-2022.5.0 huggingface-hub-0.8.1 multidict-6.0.2 pyyaml-6.0 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "import datasets\n",
        "import os\n",
        "import subprocess\n",
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "import pickle\n",
        "from packaging import version\n",
        "from typing import Optional, List\n",
        "import torch.distributed as dist\n",
        "from torch import Tensor\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "from typing import Dict\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torchvision.ops.boxes import box_area\n",
        "from PIL import Image\n",
        "import io\n",
        "import copy\n",
        "import torch.utils.data\n",
        "from pycocotools import mask as coco_mask\n",
        "import torchvision.transforms as T\n",
        "import sys\n",
        "from typing import Iterable\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools.coco import COCO\n",
        "import pycocotools.mask as mask_util\n",
        "import contextlib\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "import functools\n",
        "import traceback\n",
        "if version.parse(torchvision.__version__) < version.parse('0.7'):\n",
        "  from torchvision.ops import _new_empty_tensor\n",
        "  from torchvision.ops.misc import _output_size\n",
        "import PIL"
      ],
      "metadata": {
        "id": "zYFUt0znti2H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_traceback(f):\n",
        "    @functools.wraps(f)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        try:\n",
        "            return f(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            print('Caught exception in worker thread:')\n",
        "            traceback.print_exc()\n",
        "            raise e\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "class IdGenerator():\n",
        "    '''\n",
        "    The class is designed to generate unique IDs that have meaningful RGB encoding.\n",
        "    Given semantic category unique ID will be generated and its RGB encoding will\n",
        "    have color close to the predefined semantic category color.\n",
        "    The RGB encoding used is ID = R * 256 * G + 256 * 256 + B.\n",
        "    Class constructor takes dictionary {id: category_info}, where all semantic\n",
        "    class ids are presented and category_info record is a dict with fields\n",
        "    'isthing' and 'color'\n",
        "    '''\n",
        "    def __init__(self, categories):\n",
        "        self.taken_colors = set([0, 0, 0])\n",
        "        self.categories = categories\n",
        "        for category in self.categories.values():\n",
        "            if category['isthing'] == 0:\n",
        "                self.taken_colors.add(tuple(category['color']))\n",
        "\n",
        "    def get_color(self, cat_id):\n",
        "        def random_color(base, max_dist=30):\n",
        "            new_color = base + np.random.randint(low=-max_dist,\n",
        "                                                 high=max_dist+1,\n",
        "                                                 size=3)\n",
        "            return tuple(np.maximum(0, np.minimum(255, new_color)))\n",
        "\n",
        "        category = self.categories[cat_id]\n",
        "        if category['isthing'] == 0:\n",
        "            return category['color']\n",
        "        base_color_array = category['color']\n",
        "        base_color = tuple(base_color_array)\n",
        "        if base_color not in self.taken_colors:\n",
        "            self.taken_colors.add(base_color)\n",
        "            return base_color\n",
        "        else:\n",
        "            while True:\n",
        "                color = random_color(base_color_array)\n",
        "                if color not in self.taken_colors:\n",
        "                    self.taken_colors.add(color)\n",
        "                    return color\n",
        "\n",
        "    def get_id(self, cat_id):\n",
        "        color = self.get_color(cat_id)\n",
        "        return rgb2id(color)\n",
        "\n",
        "    def get_id_and_color(self, cat_id):\n",
        "        color = self.get_color(cat_id)\n",
        "        return rgb2id(color), color\n",
        "\n",
        "\n",
        "def rgb2id(color):\n",
        "    if isinstance(color, np.ndarray) and len(color.shape) == 3:\n",
        "        if color.dtype == np.uint8:\n",
        "            color = color.astype(np.int32)\n",
        "        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n",
        "    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\n",
        "\n",
        "\n",
        "def id2rgb(id_map):\n",
        "    if isinstance(id_map, np.ndarray):\n",
        "        id_map_copy = id_map.copy()\n",
        "        rgb_shape = tuple(list(id_map.shape) + [3])\n",
        "        rgb_map = np.zeros(rgb_shape, dtype=np.uint8)\n",
        "        for i in range(3):\n",
        "            rgb_map[..., i] = id_map_copy % 256\n",
        "            id_map_copy //= 256\n",
        "        return rgb_map\n",
        "    color = []\n",
        "    for _ in range(3):\n",
        "        color.append(id_map % 256)\n",
        "        id_map //= 256\n",
        "    return color\n",
        "\n",
        "\n",
        "def save_json(d, file):\n",
        "    with open(file, 'w') as f:\n",
        "        json.dump(d, f)"
      ],
      "metadata": {
        "id": "LEYJ4qCBdBbr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/detrdataset.zip' -d '/content'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUq3ABZzXu8-",
        "outputId": "7a275431-d32c-4c10-8f89-c31bc011d91b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/detrdataset.zip\n",
            "   creating: /content/data/annotations/\n",
            "  inflating: /content/data/annotations/instances_target.coco.json  \n",
            "  inflating: /content/data/annotations/instances_train.coco.json  \n",
            "  inflating: /content/data/annotations/instances_validation.coco.json  \n",
            "   creating: /content/data/target/\n",
            "  inflating: /content/data/target/frankfurt_000000_000576_leftImg8bit_foggy_beta_0-02_png.rf.daec2de38f1f3b583876da331ba1d5a6.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_001016_leftImg8bit_foggy_beta_0-02_png.rf.17e17e93009863128b16dc5779a837ce.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_001236_leftImg8bit_foggy_beta_0-02_png.rf.3a2878e1df3ade0417deffa6d46c6ffe.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_001751_leftImg8bit_foggy_beta_0-02_png.rf.e3a5c4c5fe5ca66ee87fd11deb70ed8c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_002963_leftImg8bit_foggy_beta_0-02_png.rf.f905fcb8331be18b80184726d8a62ac5.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_003025_leftImg8bit_foggy_beta_0-02_png.rf.cee6ac9db1021dccc8369ef517a97d4c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_003357_leftImg8bit_foggy_beta_0-02_png.rf.04884122d6f47a423554e4ae9bbe9299.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_003920_leftImg8bit_foggy_beta_0-02_png.rf.2786d62cc4a0460c46d5b908dc2c6adc.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_004617_leftImg8bit_foggy_beta_0-02_png.rf.874b9b6dee83dada71c330de7f071452.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_005543_leftImg8bit_foggy_beta_0-02_png.rf.519984da0e184c3bb5ebe1f30f42dfdd.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_005898_leftImg8bit_foggy_beta_0-02_png.rf.6210218917247c01faf63abf6f454a3b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_006589_leftImg8bit_foggy_beta_0-02_png.rf.9c023ad95349d493d2c332f7bbbdb8f0.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_007365_leftImg8bit_foggy_beta_0-02_png.rf.4daba4d2404b1d882331026306dfdd0b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_008206_leftImg8bit_foggy_beta_0-02_png.rf.b7c83b36d5a936d667cb86a7d066cbdc.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_008451_leftImg8bit_foggy_beta_0-02_png.rf.e31b7091dcb970167bf0ec05c1006c81.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_009291_leftImg8bit_foggy_beta_0-02_png.rf.8b46fb5518878a1c1cf9a10f3b49772b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_009561_leftImg8bit_foggy_beta_0-02_png.rf.43786d20a272f39e5564738d406add52.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_009688_leftImg8bit_foggy_beta_0-02_png.rf.fa851876f75fb7eaf8996a064065847c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_010351_leftImg8bit_foggy_beta_0-02_png.rf.94e6d3fa4880be57cbcdd0d8e3abf309.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_010763_leftImg8bit_foggy_beta_0-02_png.rf.41412f784705087e3ed7452f41e7aaec.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_011461_leftImg8bit_foggy_beta_0-02_png.rf.7bcae4a7ba5e784de7907e6929602804.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_011810_leftImg8bit_foggy_beta_0-02_png.rf.429538f6a910f78a687f3794228efc30.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_012009_leftImg8bit_foggy_beta_0-02_png.rf.b5b179665349d1f55fcb0b7b957e1da8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_012121_leftImg8bit_foggy_beta_0-02_png.rf.873053bb0a452930a65c6c736d12ee53.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_012868_leftImg8bit_foggy_beta_0-02_png.rf.74dcba73c598db4c27ca03a250d0f607.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_013067_leftImg8bit_foggy_beta_0-02_png.rf.054945799fa652afc10f2bc0068466c8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_013382_leftImg8bit_foggy_beta_0-02_png.rf.75716cd289416f2a4f6e3af45c615ac1.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_013942_leftImg8bit_foggy_beta_0-02_png.rf.8d61171dec498f9eb4fda53abd2b243d.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_014480_leftImg8bit_foggy_beta_0-02_png.rf.98106de09d7cf99720960df33b66a126.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_015389_leftImg8bit_foggy_beta_0-02_png.rf.078a6ee1d42ad6e37035ad1bf872b03f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_016005_leftImg8bit_foggy_beta_0-02_png.rf.05eee7b91bb54dae32d17cd9730a5891.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_016286_leftImg8bit_foggy_beta_0-02_png.rf.1dacfed62f6d58034ab7ef282ed115a7.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_017228_leftImg8bit_foggy_beta_0-02_png.rf.22bf7be9ff27f0eb7d3e92b2f25db5b6.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_017476_leftImg8bit_foggy_beta_0-02_png.rf.45b148f61450f4581429c7b3450b11a5.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_018797_leftImg8bit_foggy_beta_0-02_png.rf.d7fbeb6a2302b221e4c51fb841bf8567.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_020215_leftImg8bit_foggy_beta_0-02_png.rf.94b0a45631e49a5caf56042ff58151ae.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_020321_leftImg8bit_foggy_beta_0-02_png.rf.596eb8bdc93bfe3c2464d5eee3ef8c75.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_020880_leftImg8bit_foggy_beta_0-02_png.rf.91f83533674c60546c4116113a89cc67.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_021667_leftImg8bit_foggy_beta_0-02_png.rf.5cf7ade604da935a80537d608e7ac728.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_021879_leftImg8bit_foggy_beta_0-02_png.rf.56fa1d075392dbb51639d14251fbf8c4.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_022254_leftImg8bit_foggy_beta_0-02_png.rf.2283113944073968663f7724afcb7e74.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_022797_leftImg8bit_foggy_beta_0-02_png.rf.a91fd0c9990530fb2773feec87cd13e4.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_000538_leftImg8bit_foggy_beta_0-02_png.rf.eb7d1f7b29dc1c25c853dbf2d58367dc.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_001464_leftImg8bit_foggy_beta_0-02_png.rf.a57591a486a18fdeb02073b607c79031.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_002512_leftImg8bit_foggy_beta_0-02_png.rf.22e7500c22598ea591b2f0441dfdaa2d.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_002646_leftImg8bit_foggy_beta_0-02_png.rf.9166b1b51cd837bb3b4b5c592d9ab335.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_002759_leftImg8bit_foggy_beta_0-02_png.rf.fb58b2eecc705ddf15f7fa0e1658cc7c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_003056_leftImg8bit_foggy_beta_0-02_png.rf.af4633a72af7cdf4b0c4c347f4265f17.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_003588_leftImg8bit_foggy_beta_0-02_png.rf.a4c56c373980f2ff5f6a0c75fd355909.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_004327_leftImg8bit_foggy_beta_0-02_png.rf.0f9f45652da68d5493f57c3c731a5673.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_004736_leftImg8bit_foggy_beta_0-02_png.rf.0555def550938533d2136ee73f14f5bd.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_004859_leftImg8bit_foggy_beta_0-02_png.rf.0ed4b2f573dd61cdb09a1afa7d5ebc18.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_005184_leftImg8bit_foggy_beta_0-02_png.rf.89cbea0ab16363108cc195e5cbb97558.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_005410_leftImg8bit_foggy_beta_0-02_png.rf.ced9aa7bb22c70d5b35a583324025254.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_007285_leftImg8bit_foggy_beta_0-02_png.rf.c915a8e065ad5f0446a048f88ccc3347.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_007407_leftImg8bit_foggy_beta_0-02_png.rf.66d437783a5c8ee983d93d42b2747106.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_007622_leftImg8bit_foggy_beta_0-02_png.rf.3ebb0c24c3f068c83c7e2bdc42f635ad.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_007857_leftImg8bit_foggy_beta_0-02_png.rf.db0468f386ceb3905a28fab0afc15e54.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_009058_leftImg8bit_foggy_beta_0-02_png.rf.a4abde4f19b117ed329c17ffd44653d1.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_009504_leftImg8bit_foggy_beta_0-02_png.rf.59ee4e081e6c3fabb1011b30ef108a29.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_009854_leftImg8bit_foggy_beta_0-02_png.rf.c5c08d6f8175f414f86f4e9d855ee798.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_010156_leftImg8bit_foggy_beta_0-02_png.rf.76d0504e6771d9cb2ea9add2dc0b103e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_010600_leftImg8bit_foggy_beta_0-02_png.rf.f75683a923c9b1816b5cbcb675e0bcc2.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_011162_leftImg8bit_foggy_beta_0-02_png.rf.9d33df0ac1e93499d797225f29cbd637.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_011835_leftImg8bit_foggy_beta_0-02_png.rf.200ff3065ef8b0e078570ff75abb15c8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_012519_leftImg8bit_foggy_beta_0-02_png.rf.7933c834d60a7808d729eea83f1c1a88.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_012699_leftImg8bit_foggy_beta_0-02_png.rf.afaca6c142244be29c08816056512f1f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_012738_leftImg8bit_foggy_beta_0-02_png.rf.c8057f5dd17b1e87041ece81687aa744.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_012870_leftImg8bit_foggy_beta_0-02_png.rf.bfb60b70c14c77d50ceab4ca288c3d61.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_013016_leftImg8bit_foggy_beta_0-02_png.rf.0f09839d06897f79e9ed4583d13f1254.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_014221_leftImg8bit_foggy_beta_0-02_png.rf.ea582d4f40b4e1a2e30fb9a8e9d24d50.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_014406_leftImg8bit_foggy_beta_0-02_png.rf.a576e7add4dd7bde655d38330c2a1cc0.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_014741_leftImg8bit_foggy_beta_0-02_png.rf.34cbf703b5c2fffa580120cbb949d58f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_015091_leftImg8bit_foggy_beta_0-02_png.rf.0b75fdcafc58dbbd70cf27cd97f70d38.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_016462_leftImg8bit_foggy_beta_0-02_png.rf.2709ad80f84af756f6c4102dd2d76486.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_017101_leftImg8bit_foggy_beta_0-02_png.rf.e83e3e4a8ca984525d26a0e087dd9f94.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_017459_leftImg8bit_foggy_beta_0-02_png.rf.0180a65355dcf2261424eb4036ac06c9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_017842_leftImg8bit_foggy_beta_0-02_png.rf.f8c03ac0baf6df622c503c566721ef20.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_018113_leftImg8bit_foggy_beta_0-02_png.rf.6adaf79832a91f9d9f552934eca3d45e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_019969_leftImg8bit_foggy_beta_0-02_png.rf.88b9695f261f546a2e26ae61378c591a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_020046_leftImg8bit_foggy_beta_0-02_png.rf.fb334b5495e6856304ab6921041c417b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_020287_leftImg8bit_foggy_beta_0-02_png.rf.11e6a3cfb387fb21f1409afad2897850.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_020693_leftImg8bit_foggy_beta_0-02_png.rf.a595721f78b675494363637f3ba04074.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_021406_leftImg8bit_foggy_beta_0-02_png.rf.b38a76e553e86541f4104b2ef821714b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_021825_leftImg8bit_foggy_beta_0-02_png.rf.9856e3e74654c03e98f2296af8d6f0e2.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_023235_leftImg8bit_foggy_beta_0-02_png.rf.3d23c9a258e70c38a478c12889d0ddb8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_024927_leftImg8bit_foggy_beta_0-02_png.rf.cfbcfa58c1ae4f5574566c6874421f48.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_025512_leftImg8bit_foggy_beta_0-02_png.rf.5ff2bc34b22ecdb26b485ed64660ef67.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_025713_leftImg8bit_foggy_beta_0-02_png.rf.fd7ffdd5207d96b1769d8f4ad0ab2a16.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_025921_leftImg8bit_foggy_beta_0-02_png.rf.39227f7efe3ab305ac72f168952685f1.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_027325_leftImg8bit_foggy_beta_0-02_png.rf.f8374b201fe60fd4440e52ec1661341f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_028232_leftImg8bit_foggy_beta_0-02_png.rf.1db2377f03c61078cd5f9f9494b0b730.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_028335_leftImg8bit_foggy_beta_0-02_png.rf.1b390762c65ae92a4d0283fbd2d15768.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_028590_leftImg8bit_foggy_beta_0-02_png.rf.77e117489b5e50bb772991dae6c3b977.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_028854_leftImg8bit_foggy_beta_0-02_png.rf.261570402b8bcf4be6b4a91e61c462e8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_029086_leftImg8bit_foggy_beta_0-02_png.rf.1ad1279a84c6fcb39f8a54b5021ba756.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_029236_leftImg8bit_foggy_beta_0-02_png.rf.f278dc63d5c7fe24627577932f631682.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_029600_leftImg8bit_foggy_beta_0-02_png.rf.7e019490be5fb198e7ff1633079bbf2a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_030067_leftImg8bit_foggy_beta_0-02_png.rf.d0801d455ad0c1c23da4ddddeed4561e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_030669_leftImg8bit_foggy_beta_0-02_png.rf.0f05afc32333f83b1001da6e281a1311.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_031416_leftImg8bit_foggy_beta_0-02_png.rf.a9c66eb72bf7e2d9b4cf5460f09f1e75.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_032018_leftImg8bit_foggy_beta_0-02_png.rf.4061d5b9bcb60adb8f4efc283d07003a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_032556_leftImg8bit_foggy_beta_0-02_png.rf.bf28770e138c8d26d70660dd5ff66c84.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_032942_leftImg8bit_foggy_beta_0-02_png.rf.722566198855a97d120885b1f974462c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_034047_leftImg8bit_foggy_beta_0-02_png.rf.0fc5aecf07aa2bcb1e96679f881d507c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_034816_leftImg8bit_foggy_beta_0-02_png.rf.6a18160d83dbadb4671abf57d1fbb385.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_035864_leftImg8bit_foggy_beta_0-02_png.rf.d82b9e91c355387c875267c5a4bb97c4.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_037705_leftImg8bit_foggy_beta_0-02_png.rf.239947b4bd67eb4b570809f2f6010f6c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_038245_leftImg8bit_foggy_beta_0-02_png.rf.b0f15334c83c39414cd14525528875cc.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_038418_leftImg8bit_foggy_beta_0-02_png.rf.90beea57a6dc7bc4e982a92d49d3cbff.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_038645_leftImg8bit_foggy_beta_0-02_png.rf.1ad234c2944324500e1f94df4ae8a2ed.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_038844_leftImg8bit_foggy_beta_0-02_png.rf.5fb724d3abc0540336669b15d616aa31.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_039895_leftImg8bit_foggy_beta_0-02_png.rf.f1ef35ac51e2f6ef6f84dd3ac3d0bb06.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_040575_leftImg8bit_foggy_beta_0-02_png.rf.98ff6c3555fde4e598dfb0ca4c7a72df.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_040732_leftImg8bit_foggy_beta_0-02_png.rf.ae5053ce6016fda6c0a79c71a9c4b292.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_041354_leftImg8bit_foggy_beta_0-02_png.rf.8496efb7c06f60958b55b494f78e1abe.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_041664_leftImg8bit_foggy_beta_0-02_png.rf.8ab9c3d25b55491dc5408d070da77664.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_042098_leftImg8bit_foggy_beta_0-02_png.rf.fb7081da440f777731f49493be939adb.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_042384_leftImg8bit_foggy_beta_0-02_png.rf.c6e3c905ee524d26ad82b1dfa5302690.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_042733_leftImg8bit_foggy_beta_0-02_png.rf.f6caeb0cebf9667ef54e4e4befd119b0.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_043395_leftImg8bit_foggy_beta_0-02_png.rf.5e1d3305a3f1b6be9b2e7628d0087a3d.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_043564_leftImg8bit_foggy_beta_0-02_png.rf.db915ab770e35db5681bbd3e13332f14.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_044227_leftImg8bit_foggy_beta_0-02_png.rf.0193ec2b67202c035fcd92dbf0b3ef3a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_044413_leftImg8bit_foggy_beta_0-02_png.rf.67e083c622d1b4334ee362998bd973e8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_044525_leftImg8bit_foggy_beta_0-02_png.rf.3fb85909e55b752ddb4e5688f811c92a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_044787_leftImg8bit_foggy_beta_0-02_png.rf.fd8b40aeba2eeca3c164625bc7c967c9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_046126_leftImg8bit_foggy_beta_0-02_png.rf.c8140aee96a70095b2b3135a006d0447.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_046272_leftImg8bit_foggy_beta_0-02_png.rf.2a81887ad73a4b8314ddde40f7c28c5f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_046779_leftImg8bit_foggy_beta_0-02_png.rf.5e509b1cc0a93819be2eb999fabbde07.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_047178_leftImg8bit_foggy_beta_0-02_png.rf.49fb73592c28cac46c2eb1be7c0a9c4a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_048196_leftImg8bit_foggy_beta_0-02_png.rf.6983858e53586677aef4a2ae4189e750.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_048355_leftImg8bit_foggy_beta_0-02_png.rf.d3377526108486cea71e5fe2884f6b2c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_048654_leftImg8bit_foggy_beta_0-02_png.rf.30d47f637ef3d39d888a05df01d561e5.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_049078_leftImg8bit_foggy_beta_0-02_png.rf.a636e32d175d31d18fc73f1a21a56ab9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_049209_leftImg8bit_foggy_beta_0-02_png.rf.b679216abe779d5c8d2c4c4e94533d8e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_049698_leftImg8bit_foggy_beta_0-02_png.rf.2c5a55a9d5599b7c8f6eec51fdc78a1c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_049770_leftImg8bit_foggy_beta_0-02_png.rf.24d9e4287e2c769a95f9e8b126d91a2f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_050149_leftImg8bit_foggy_beta_0-02_png.rf.4224717b45dd3abae0c5cc13a9d49086.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_051516_leftImg8bit_foggy_beta_0-02_png.rf.cf6d91e3cf9124e59f3e0e0dceb242f6.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_051737_leftImg8bit_foggy_beta_0-02_png.rf.8374dfb4b6de216328dbac7541ffea4c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_051807_leftImg8bit_foggy_beta_0-02_png.rf.f60be47ffa07646a3a5bb53140b1ec48.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_052120_leftImg8bit_foggy_beta_0-02_png.rf.452e0a2b0ffd8de0e752590b6dc6fe29.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_052594_leftImg8bit_foggy_beta_0-02_png.rf.b3f0c4ba64b30e46296201c2be075d1e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_053102_leftImg8bit_foggy_beta_0-02_png.rf.7eac9f76954efb23718776a5a4bdbd4d.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_054077_leftImg8bit_foggy_beta_0-02_png.rf.0584e36a27927766d78f52c41da20660.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_054219_leftImg8bit_foggy_beta_0-02_png.rf.cad7ef276196e7a02cd114631e44006b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_054415_leftImg8bit_foggy_beta_0-02_png.rf.93c3bc9d0b36a64c4ca26b2f1681f52f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_054640_leftImg8bit_foggy_beta_0-02_png.rf.726f3ab325e86c4cf868d11e9cc2a45f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_054884_leftImg8bit_foggy_beta_0-02_png.rf.51286a1fb3809742fa9194ba6846f548.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_055172_leftImg8bit_foggy_beta_0-02_png.rf.6f52c1f3e47ee6d8eaca3251d3db430a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_055387_leftImg8bit_foggy_beta_0-02_png.rf.a1542a8607cc70e7c9b8151bd3eac76e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_055538_leftImg8bit_foggy_beta_0-02_png.rf.2ccf671c7a677d7de5df826c21359242.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_055603_leftImg8bit_foggy_beta_0-02_png.rf.495186b4059571440412a4d85aa0643a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_055709_leftImg8bit_foggy_beta_0-02_png.rf.a2cf12246b3fb117fb9a44edb5d779b2.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_056580_leftImg8bit_foggy_beta_0-02_png.rf.6cc5d55b3d58a7b6062f69db793d6a53.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_057181_leftImg8bit_foggy_beta_0-02_png.rf.618117e7aca66d00e2df496de1a97e54.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_057478_leftImg8bit_foggy_beta_0-02_png.rf.9450c22fa08ea87e60eae8fdbdc8fd34.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_057954_leftImg8bit_foggy_beta_0-02_png.rf.e25ec6ba9be71fb7418853171dd76f43.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_058057_leftImg8bit_foggy_beta_0-02_png.rf.48f7fabb454c6f6ebd2c460975eb25cb.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_058504_leftImg8bit_foggy_beta_0-02_png.rf.8fe8d5b67d22bfc3b746ed44a09160e9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_059119_leftImg8bit_foggy_beta_0-02_png.rf.102fcdbc59b1b87d4e3d812004123f43.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_059642_leftImg8bit_foggy_beta_0-02_png.rf.aee8943f778162ac72b8b8cdb9effcb8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_059789_leftImg8bit_foggy_beta_0-02_png.rf.77be0a4dd0e563838b93a00e37bb577e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_060545_leftImg8bit_foggy_beta_0-02_png.rf.e93f8233099d5eee41912a35ecc559bb.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_061682_leftImg8bit_foggy_beta_0-02_png.rf.5a11daea6508f318c7fa4dbaa929a1f7.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_061763_leftImg8bit_foggy_beta_0-02_png.rf.57133dd710bd3aef49ee94327a9f4a78.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_062016_leftImg8bit_foggy_beta_0-02_png.rf.dac2604f53c7028f6631efbc504f8487.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_062250_leftImg8bit_foggy_beta_0-02_png.rf.83e56f1322da1bb178f27b8ea4836ec9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_062509_leftImg8bit_foggy_beta_0-02_png.rf.53dc3b28e5ea278620533d323eb0fc03.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_062653_leftImg8bit_foggy_beta_0-02_png.rf.a5ab35b72451d47aaf8451522ed923c5.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_062793_leftImg8bit_foggy_beta_0-02_png.rf.41f3259ab994f709d1f0744ae5e142fc.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_063045_leftImg8bit_foggy_beta_0-02_png.rf.f1d07e6fa7a72781cdf1f312e49dbb9a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_064130_leftImg8bit_foggy_beta_0-02_png.rf.be4873a2e0a11db55daa50bd0aaf5a1c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_064305_leftImg8bit_foggy_beta_0-02_png.rf.36d04a56e6d2179ba12dfb856ca7b216.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_064651_leftImg8bit_foggy_beta_0-02_png.rf.84f914dd1441de2bea33c6476f1f247b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_064798_leftImg8bit_foggy_beta_0-02_png.rf.2244aea6c3c6afb9d8e5069cbac3b3fb.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_064925_leftImg8bit_foggy_beta_0-02_png.rf.8a134511a371d01232f30ff70aca1ff5.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_065850_leftImg8bit_foggy_beta_0-02_png.rf.0fb5e8031e6ec8e9b20f5f45781c8351.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_066092_leftImg8bit_foggy_beta_0-02_png.rf.298a13ab55bd9c57dc78746494bc0da5.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_066438_leftImg8bit_foggy_beta_0-02_png.rf.470885c3a7a562c3628e628b0072b9f9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_066574_leftImg8bit_foggy_beta_0-02_png.rf.a0708dcfc55946ac70a361d7323f571d.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_066832_leftImg8bit_foggy_beta_0-02_png.rf.89449858bc6e73b0a0dd9d56c17997d3.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_067092_leftImg8bit_foggy_beta_0-02_png.rf.b6e0790d1fc2862c9de00202c5862a9d.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_067178_leftImg8bit_foggy_beta_0-02_png.rf.739ca84c7e1c1d50b13f375ac3dbf458.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_067295_leftImg8bit_foggy_beta_0-02_png.rf.c9676f6994e0f6fc67f7d2278b9bbc2a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_067474_leftImg8bit_foggy_beta_0-02_png.rf.29bfe019074e480ecb6f66b777d5dc9c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_067735_leftImg8bit_foggy_beta_0-02_png.rf.58b641f889119067020966d7ca05776f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_068063_leftImg8bit_foggy_beta_0-02_png.rf.8430f72953df9bc7bdfd86c1182e9b42.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_068208_leftImg8bit_foggy_beta_0-02_png.rf.dcfaa17436cecad3de4482ac44d676e2.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_068682_leftImg8bit_foggy_beta_0-02_png.rf.820f51d71cebd3cce923faa5a76ee054.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_068772_leftImg8bit_foggy_beta_0-02_png.rf.68fb7cf981ab2e7fb4503a30cc7240dc.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_069633_leftImg8bit_foggy_beta_0-02_png.rf.df59f324f9f7b5e26408ba5101a6a9de.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_070099_leftImg8bit_foggy_beta_0-02_png.rf.112500572edb02464441219a17461960.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_071288_leftImg8bit_foggy_beta_0-02_png.rf.280b01b7026f8a25e5f17fa859d72d8f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_071781_leftImg8bit_foggy_beta_0-02_png.rf.98385ba6dd6df075b08e0d5589ace87e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_072155_leftImg8bit_foggy_beta_0-02_png.rf.54708c798b0747263b9158659d85e7d1.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_072295_leftImg8bit_foggy_beta_0-02_png.rf.4f94e06d0c134a1ab88d46c60bfcfee9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_073088_leftImg8bit_foggy_beta_0-02_png.rf.e2932cad0c3cbd666da0868a316b1ca3.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_073243_leftImg8bit_foggy_beta_0-02_png.rf.d737628bcde1f5cc0c5b2456e7b7fc45.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_073464_leftImg8bit_foggy_beta_0-02_png.rf.3033491eb58131cf6b072df50f8180df.jpg  \n",
            "   creating: /content/data/train/\n",
            "  inflating: /content/data/train/aachen_000000_000019_leftImg8bit_png.rf.582961c9887d9dae3c8dd1610453db5f.jpg  \n",
            "  inflating: /content/data/train/aachen_000001_000019_leftImg8bit_png.rf.f91aa5821ab0425ddcbcd47a0cac899e.jpg  \n",
            "  inflating: /content/data/train/aachen_000004_000019_leftImg8bit_png.rf.142c5891e1df86382211343550db7023.jpg  \n",
            "  inflating: /content/data/train/aachen_000005_000019_leftImg8bit_png.rf.de26ff2bd0d7e188be5c08b6d989c2a2.jpg  \n",
            "  inflating: /content/data/train/aachen_000006_000019_leftImg8bit_png.rf.4fb5925d88109cf20d3504bbe9468322.jpg  \n",
            "  inflating: /content/data/train/aachen_000008_000019_leftImg8bit_png.rf.cad9c3ec56eded4864bdd989ace140f6.jpg  \n",
            "  inflating: /content/data/train/aachen_000009_000019_leftImg8bit_png.rf.a7e527f04f1d1c5facabd83b914a95ca.jpg  \n",
            "  inflating: /content/data/train/aachen_000010_000019_leftImg8bit_png.rf.675f7c3c4a4f3cbe26c09e4b412cef5a.jpg  \n",
            "  inflating: /content/data/train/aachen_000011_000019_leftImg8bit_png.rf.dd610a26bd9e6be7f93f12f43aeb36aa.jpg  \n",
            "  inflating: /content/data/train/aachen_000013_000019_leftImg8bit_png.rf.5fc98df03ca46477aa96f62cface6fba.jpg  \n",
            "  inflating: /content/data/train/aachen_000016_000019_leftImg8bit_png.rf.2e35b5dad96ada244066499b12f2f6eb.jpg  \n",
            "  inflating: /content/data/train/aachen_000018_000019_leftImg8bit_png.rf.77d20ae3a4d0e48af549a191bd33bad9.jpg  \n",
            "  inflating: /content/data/train/aachen_000020_000019_leftImg8bit_png.rf.6a9e74e6c1a5e5c677d3ca11d3d52f69.jpg  \n",
            "  inflating: /content/data/train/aachen_000021_000019_leftImg8bit_png.rf.0b389f3e83a0d9f0ce2f0d7fb273f034.jpg  \n",
            "  inflating: /content/data/train/aachen_000022_000019_leftImg8bit_png.rf.6a16d20bca3574c8277828adf4836134.jpg  \n",
            "  inflating: /content/data/train/aachen_000023_000019_leftImg8bit_png.rf.b6b31e1fa5ea04e9cf13e9a472b6f827.jpg  \n",
            "  inflating: /content/data/train/aachen_000024_000019_leftImg8bit_png.rf.8009b3386285391e44c5a21fd3e02eaa.jpg  \n",
            "  inflating: /content/data/train/aachen_000025_000019_leftImg8bit_png.rf.e687bdc864d15d67b649837c417d9a59.jpg  \n",
            "  inflating: /content/data/train/aachen_000026_000019_leftImg8bit_png.rf.8748f8bc57ee4d152e7fa5eaab0cd5d7.jpg  \n",
            "  inflating: /content/data/train/aachen_000027_000019_leftImg8bit_png.rf.1181733e113db61e61276b22526b3fc1.jpg  \n",
            "  inflating: /content/data/train/aachen_000028_000019_leftImg8bit_png.rf.ea4a138db348d6603161fea627a5278e.jpg  \n",
            "  inflating: /content/data/train/aachen_000030_000019_leftImg8bit_png.rf.65655c6374cb57a6675ecff27c9d6f35.jpg  \n",
            "  inflating: /content/data/train/aachen_000031_000019_leftImg8bit_png.rf.e2db2694efa79a571dfa721abb9fea52.jpg  \n",
            "  inflating: /content/data/train/aachen_000033_000019_leftImg8bit_png.rf.d205d12321923fc632b7ab25098fcad2.jpg  \n",
            "  inflating: /content/data/train/aachen_000034_000019_leftImg8bit_png.rf.f8c5d9982106ca613017c72024909a67.jpg  \n",
            "  inflating: /content/data/train/aachen_000036_000019_leftImg8bit_png.rf.db334a2a24a2faee8a0d665f3871c067.jpg  \n",
            "  inflating: /content/data/train/aachen_000037_000019_leftImg8bit_png.rf.72979012452111fba093cbc6582a753b.jpg  \n",
            "  inflating: /content/data/train/aachen_000038_000019_leftImg8bit_png.rf.d1361a986c4e094e54c934a5d6e96320.jpg  \n",
            "  inflating: /content/data/train/aachen_000039_000019_leftImg8bit_png.rf.0d7e6af1d670616da2dca51e52060f88.jpg  \n",
            "  inflating: /content/data/train/aachen_000042_000019_leftImg8bit_png.rf.8d4bbff500a1a12f8c726be8e961e564.jpg  \n",
            "  inflating: /content/data/train/aachen_000043_000019_leftImg8bit_png.rf.b2cd43d90e05ce870ca9baa3096e547e.jpg  \n",
            "  inflating: /content/data/train/aachen_000044_000019_leftImg8bit_png.rf.00fa4c7ce5b1518bef7a82301cdd6a05.jpg  \n",
            "  inflating: /content/data/train/aachen_000045_000019_leftImg8bit_png.rf.1e16d81e47f37dbd285a6eb7e448abc7.jpg  \n",
            "  inflating: /content/data/train/aachen_000046_000019_leftImg8bit_png.rf.2f2ec43d41e99f166c13ea0c1e0331d8.jpg  \n",
            "  inflating: /content/data/train/aachen_000047_000019_leftImg8bit_png.rf.17ce1c4c3dbc025982fa3cda51648a16.jpg  \n",
            "  inflating: /content/data/train/aachen_000050_000019_leftImg8bit_png.rf.f4709323a3f6dde6fc747d50054cc598.jpg  \n",
            "  inflating: /content/data/train/aachen_000051_000019_leftImg8bit_png.rf.547ab162758d1e43cd969afcecc6ec34.jpg  \n",
            "  inflating: /content/data/train/aachen_000052_000019_leftImg8bit_png.rf.d02bb848e044d2b2527c62637bc43e9f.jpg  \n",
            "  inflating: /content/data/train/aachen_000054_000019_leftImg8bit_png.rf.4d8a3479da146b6bed4d6c909234eed0.jpg  \n",
            "  inflating: /content/data/train/aachen_000055_000019_leftImg8bit_png.rf.bb998259571bbcbae4f82e08e3734e12.jpg  \n",
            "  inflating: /content/data/train/aachen_000057_000019_leftImg8bit_png.rf.13cfcba5e93781422271386003839a96.jpg  \n",
            "  inflating: /content/data/train/aachen_000058_000019_leftImg8bit_png.rf.d84b7b3b2df9e69f58cae9c4d5b360fd.jpg  \n",
            "  inflating: /content/data/train/aachen_000060_000019_leftImg8bit_png.rf.eff631cc3a3c23866ed331de3b3ffb3c.jpg  \n",
            "  inflating: /content/data/train/aachen_000063_000019_leftImg8bit_png.rf.6337ee59c0f594739be7b3eec4efc5e5.jpg  \n",
            "  inflating: /content/data/train/aachen_000065_000019_leftImg8bit_png.rf.266a2b60b1c86eb35e705bbccc7d4259.jpg  \n",
            "  inflating: /content/data/train/aachen_000067_000019_leftImg8bit_png.rf.2f948caed84ba80f408cd7bd033f1b73.jpg  \n",
            "  inflating: /content/data/train/aachen_000068_000019_leftImg8bit_png.rf.9280201c560cb90cb5a88da3e1a791ff.jpg  \n",
            "  inflating: /content/data/train/aachen_000069_000019_leftImg8bit_png.rf.d874145a3fba381eb78a867c54724ac5.jpg  \n",
            "  inflating: /content/data/train/aachen_000072_000019_leftImg8bit_png.rf.1530c551e18dae775c81c0f1286a8c64.jpg  \n",
            "  inflating: /content/data/train/aachen_000073_000019_leftImg8bit_png.rf.9a071eeb371fa4e82041e1279c448a02.jpg  \n",
            "  inflating: /content/data/train/aachen_000075_000019_leftImg8bit_png.rf.73cd658733f76de10456bcc0a0947a82.jpg  \n",
            "  inflating: /content/data/train/aachen_000076_000019_leftImg8bit_png.rf.6cf14dfe9991bec232596e2bf39ef568.jpg  \n",
            "  inflating: /content/data/train/aachen_000077_000019_leftImg8bit_png.rf.0aeda9d9faa9281cda0c0a6e9fd6b97d.jpg  \n",
            "  inflating: /content/data/train/aachen_000078_000019_leftImg8bit_png.rf.cc4adfa4551ca8d0d034f89526604295.jpg  \n",
            "  inflating: /content/data/train/aachen_000079_000019_leftImg8bit_png.rf.2ac3bba14dd21775a50bd617bc7be587.jpg  \n",
            "  inflating: /content/data/train/aachen_000080_000019_leftImg8bit_png.rf.2311c995e3abeda3b7c2ba2e90dddf80.jpg  \n",
            "  inflating: /content/data/train/aachen_000081_000019_leftImg8bit_png.rf.a90eed1c7a7545c6410037bb21ef5d36.jpg  \n",
            "  inflating: /content/data/train/aachen_000082_000019_leftImg8bit_png.rf.72a023062f990cdb18baea60b19e3d79.jpg  \n",
            "  inflating: /content/data/train/aachen_000083_000019_leftImg8bit_png.rf.30dc49787becc63a284e9c2a973806c4.jpg  \n",
            "  inflating: /content/data/train/aachen_000085_000019_leftImg8bit_png.rf.122d3e8ef5ad1c8aadde930be5e4b461.jpg  \n",
            "  inflating: /content/data/train/aachen_000087_000019_leftImg8bit_png.rf.6ca99b38326512df1416d294871720eb.jpg  \n",
            "  inflating: /content/data/train/aachen_000088_000019_leftImg8bit_png.rf.c82e3c76de2fd92381659a0a82ee1405.jpg  \n",
            "  inflating: /content/data/train/aachen_000089_000019_leftImg8bit_png.rf.506c340b39d22561c43b3272e0bfe0a6.jpg  \n",
            "  inflating: /content/data/train/aachen_000090_000019_leftImg8bit_png.rf.4d935aa535df87c3733362904cc8778f.jpg  \n",
            "  inflating: /content/data/train/aachen_000092_000019_leftImg8bit_png.rf.b76779489351cd819dfa0d7c8d1484a4.jpg  \n",
            "  inflating: /content/data/train/aachen_000093_000019_leftImg8bit_png.rf.356e7e9aa28a0a91dc88d71ba98657d9.jpg  \n",
            "  inflating: /content/data/train/aachen_000094_000019_leftImg8bit_png.rf.6d24805d7ba12b81f792fdb481548448.jpg  \n",
            "  inflating: /content/data/train/aachen_000095_000019_leftImg8bit_png.rf.dc2989dcd19abb2433b9065388619310.jpg  \n",
            "  inflating: /content/data/train/aachen_000096_000019_leftImg8bit_png.rf.0255b08c88af8b9200593ced5d056a61.jpg  \n",
            "  inflating: /content/data/train/aachen_000097_000019_leftImg8bit_png.rf.aca03dc4416adfc2a64452821cda6bb1.jpg  \n",
            "  inflating: /content/data/train/aachen_000098_000019_leftImg8bit_png.rf.7fbd90ec7b2226ad8a178f7e9d9030a5.jpg  \n",
            "  inflating: /content/data/train/aachen_000099_000019_leftImg8bit_png.rf.02d4fa4ae52edebf96f044ba83ed66e0.jpg  \n",
            "  inflating: /content/data/train/aachen_000101_000019_leftImg8bit_png.rf.49c37139cf2433206154633375f28229.jpg  \n",
            "  inflating: /content/data/train/aachen_000103_000019_leftImg8bit_png.rf.1d0680f932c2ac46c90b75cf203e0bd8.jpg  \n",
            "  inflating: /content/data/train/aachen_000104_000019_leftImg8bit_png.rf.acc8cb7e50c8fbf34574646ec432f9eb.jpg  \n",
            "  inflating: /content/data/train/aachen_000105_000019_leftImg8bit_png.rf.f7dca82179bbf100cdb5e8588dda6072.jpg  \n",
            "  inflating: /content/data/train/aachen_000106_000019_leftImg8bit_png.rf.4e69cfbc700b9b93f259ea5b5154fab8.jpg  \n",
            "  inflating: /content/data/train/aachen_000107_000019_leftImg8bit_png.rf.33cc496a658ab5442d351466769b29c5.jpg  \n",
            "  inflating: /content/data/train/aachen_000108_000019_leftImg8bit_png.rf.781c474237b83bb520170754abefc440.jpg  \n",
            "  inflating: /content/data/train/aachen_000109_000019_leftImg8bit_png.rf.14683461510891e9b39c1b6e455c98e4.jpg  \n",
            "  inflating: /content/data/train/aachen_000113_000019_leftImg8bit_png.rf.d0520d659de2a15f82958165a9211091.jpg  \n",
            "  inflating: /content/data/train/aachen_000114_000019_leftImg8bit_png.rf.f458aa9a3124ba0ed37b3d770b4281da.jpg  \n",
            "  inflating: /content/data/train/aachen_000115_000019_leftImg8bit_png.rf.3eac5ba7981b51c6f2b1c76b2acb52c8.jpg  \n",
            "  inflating: /content/data/train/aachen_000117_000019_leftImg8bit_png.rf.81ae718a167a947f37ac111f8e7b50f5.jpg  \n",
            "  inflating: /content/data/train/aachen_000118_000019_leftImg8bit_png.rf.20a269ca4d5f2f4396a58ce92b472de3.jpg  \n",
            "  inflating: /content/data/train/aachen_000119_000019_leftImg8bit_png.rf.91bb1b005bf551ab15f0c17cee5d3935.jpg  \n",
            "  inflating: /content/data/train/aachen_000124_000019_leftImg8bit_png.rf.0524efd0270f1cf4e053db2417193fee.jpg  \n",
            "  inflating: /content/data/train/aachen_000125_000019_leftImg8bit_png.rf.180b75c2204b7b051ed4e1d52da910fb.jpg  \n",
            "  inflating: /content/data/train/aachen_000126_000019_leftImg8bit_png.rf.e07728e3131d3c6043cadf083e77713f.jpg  \n",
            "  inflating: /content/data/train/aachen_000127_000019_leftImg8bit_png.rf.8f706816706f0e7f20ea71d3bf7d419f.jpg  \n",
            "  inflating: /content/data/train/aachen_000128_000019_leftImg8bit_png.rf.6ae359a7fd72496cb3b0d5fcc996d434.jpg  \n",
            "  inflating: /content/data/train/aachen_000129_000019_leftImg8bit_png.rf.631ca71349d4e6baf58833f8714d7851.jpg  \n",
            "  inflating: /content/data/train/aachen_000130_000019_leftImg8bit_png.rf.c7e5ac55a71ac12880822c051f2de163.jpg  \n",
            "  inflating: /content/data/train/aachen_000131_000019_leftImg8bit_png.rf.8e488844cce658f76733654af0e2cd7d.jpg  \n",
            "  inflating: /content/data/train/aachen_000132_000019_leftImg8bit_png.rf.afc8ad56b7a29d94906caadb16ac672c.jpg  \n",
            "  inflating: /content/data/train/aachen_000133_000019_leftImg8bit_png.rf.9fb3b29d309760ff05f0a972c034beaf.jpg  \n",
            "  inflating: /content/data/train/aachen_000134_000019_leftImg8bit_png.rf.ba30a4a6e827364d7e0f407eaad5b491.jpg  \n",
            "  inflating: /content/data/train/aachen_000135_000019_leftImg8bit_png.rf.bce0c7196e52636b2a82e1374d568de1.jpg  \n",
            "  inflating: /content/data/train/aachen_000136_000019_leftImg8bit_png.rf.a7e51c45a64d183f1699f2fd993e90a0.jpg  \n",
            "  inflating: /content/data/train/aachen_000137_000019_leftImg8bit_png.rf.ca65ac2cb350587c9ddff89514878f45.jpg  \n",
            "  inflating: /content/data/train/aachen_000140_000019_leftImg8bit_png.rf.96d744a6fd6b306f019f7d8b4fa8eea6.jpg  \n",
            "  inflating: /content/data/train/aachen_000141_000019_leftImg8bit_png.rf.ee35b8d44543bbedc4045d2378147f29.jpg  \n",
            "  inflating: /content/data/train/aachen_000142_000019_leftImg8bit_png.rf.d4e871c2194bb382986237ebba91e246.jpg  \n",
            "  inflating: /content/data/train/aachen_000143_000019_leftImg8bit_png.rf.d893d65b77e35d650e9a35fad263cb76.jpg  \n",
            "  inflating: /content/data/train/aachen_000145_000019_leftImg8bit_png.rf.71fee64855f3b61477e3f2edb72c1371.jpg  \n",
            "  inflating: /content/data/train/aachen_000146_000019_leftImg8bit_png.rf.34797ab10911181da932fdda12cc1bcb.jpg  \n",
            "  inflating: /content/data/train/aachen_000147_000019_leftImg8bit_png.rf.e274d8d63ba97a51f9e2903aec4af6ca.jpg  \n",
            "  inflating: /content/data/train/aachen_000148_000019_leftImg8bit_png.rf.1623e2e6f4b5e9eb8e7d08f783c8b62a.jpg  \n",
            "  inflating: /content/data/train/aachen_000149_000019_leftImg8bit_png.rf.b2785c1ee31130aea2c1e30aa9b8ee72.jpg  \n",
            "  inflating: /content/data/train/aachen_000151_000019_leftImg8bit_png.rf.8a799370f596d09430b4ba3b4031a756.jpg  \n",
            "  inflating: /content/data/train/aachen_000152_000019_leftImg8bit_png.rf.4a46069771bc7e953460f23ee9aabb81.jpg  \n",
            "  inflating: /content/data/train/aachen_000153_000019_leftImg8bit_png.rf.f1cba172cf391b9dce19cf26e020e6a1.jpg  \n",
            "  inflating: /content/data/train/aachen_000154_000019_leftImg8bit_png.rf.882ceaaa629155c07f0e14365fc88592.jpg  \n",
            "  inflating: /content/data/train/aachen_000156_000019_leftImg8bit_png.rf.eb4161ee4b7897524001f8644c6395bc.jpg  \n",
            "  inflating: /content/data/train/aachen_000157_000019_leftImg8bit_png.rf.8f4bdf1192aed5b585586cfa3735fce6.jpg  \n",
            "  inflating: /content/data/train/aachen_000158_000019_leftImg8bit_png.rf.7e135350c7c101d65f4d416a18fb4164.jpg  \n",
            "  inflating: /content/data/train/aachen_000159_000019_leftImg8bit_png.rf.27211f1b4f231e82fa8245345ead8339.jpg  \n",
            "  inflating: /content/data/train/aachen_000161_000019_leftImg8bit_png.rf.560dc17d398c591ebbadcdbd32ca05de.jpg  \n",
            "  inflating: /content/data/train/aachen_000162_000019_leftImg8bit_png.rf.9d7e0d3df4ebbe261a35cd9f4b0d8c51.jpg  \n",
            "  inflating: /content/data/train/aachen_000166_000019_leftImg8bit_png.rf.a42129c63a156108e76d48b9d909f921.jpg  \n",
            "  inflating: /content/data/train/aachen_000170_000019_leftImg8bit_png.rf.3c7a97c13c6f6ed36fc73d977f2f5f61.jpg  \n",
            "  inflating: /content/data/train/aachen_000171_000019_leftImg8bit_png.rf.adb9a75ba618dd928a760402a18ab860.jpg  \n",
            "  inflating: /content/data/train/aachen_000172_000019_leftImg8bit_png.rf.0aa2934f298162c62156f4d873d82232.jpg  \n",
            "  inflating: /content/data/train/aachen_000173_000019_leftImg8bit_png.rf.b4845aed54386c2cc0129fcca1eae8a7.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_000313_leftImg8bit_png.rf.cdb54888334f20611ebcd00c9af1deec.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_000600_leftImg8bit_png.rf.b4c24893839f2bcd9b06afde6f849e8a.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_000885_leftImg8bit_png.rf.01a02ae08b2bfb62d0e80947c9be8872.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_001097_leftImg8bit_png.rf.c7e62a71951ab1ab5842b4ff5127325f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_001519_leftImg8bit_png.rf.0ecd8b2735e614373916e1ceb3a1d7d6.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_001828_leftImg8bit_png.rf.bbd3318dc482c8ccf7945cf65fe6d951.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_002293_leftImg8bit_png.rf.2017c0aedd3274782c88d2efb46f97df.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_002562_leftImg8bit_png.rf.848ac562b826b2c581911d69053799a3.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_003005_leftImg8bit_png.rf.23b4c88d410a7bc0748b4cd8beb4e98b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_003245_leftImg8bit_png.rf.5fe32ed5747882d0885e87ca4f5e8b21.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_003674_leftImg8bit_png.rf.06dccce91b2cfb7485ad7e703350620b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_004032_leftImg8bit_png.rf.7c84310fc88c12ddee1373ceba32cb36.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_004229_leftImg8bit_png.rf.0a200538efdf332df0ed72d7fd83bb02.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_004748_leftImg8bit_png.rf.eefb5f1a511d74a60ba60eb0b1da8cf5.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_005537_leftImg8bit_png.rf.49cc4e08f14d7d86ded0ce4665fa5210.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_005936_leftImg8bit_png.rf.56f7b0c22522c08edb995cdbb2e72c76.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_006026_leftImg8bit_png.rf.ca46678b4a0b8c454ff2707273b5326f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_006484_leftImg8bit_png.rf.a0b9382e02d60b014ca464b0b3877d57.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_006746_leftImg8bit_png.rf.2bfe8191bd8d58f2bd3c9027e82e13c3.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_007150_leftImg8bit_png.rf.5d4767e7b9f38941d4d33a9afca6f0b9.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_007651_leftImg8bit_png.rf.f3990a0ce5318f8a2543694c79dc677f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_007950_leftImg8bit_png.rf.e74ba39da412ddbc4182b18334f2b4a6.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_008162_leftImg8bit_png.rf.ffa6fba76cc50401e9320689f9fa4e7e.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_008448_leftImg8bit_png.rf.0b0d4699fed0f591b4320949ead1221f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_008804_leftImg8bit_png.rf.2b7364087aa2828dd5ce1809af738f1d.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_009554_leftImg8bit_png.rf.a53264261013eb7a73b2da3dc815b5f7.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_009951_leftImg8bit_png.rf.f5bd1ba188821fc687b5784b6bbb9db4.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_010562_leftImg8bit_png.rf.1bc55b226f79b236b65caa5d31db800b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_010700_leftImg8bit_png.rf.0d6d60bcc7834f21fa8e1b6f91f9b92a.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_011255_leftImg8bit_png.rf.ae9061f18745641e2eb326804cefe956.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_011711_leftImg8bit_png.rf.ed5d78847f6960e7550b2aa598249c15.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_013209_leftImg8bit_png.rf.ae4b5c6c11d4c9e5049e97b38fbc8879.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_013705_leftImg8bit_png.rf.8d9bab8adc8b3defa4bcaca6965d7ab7.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_014332_leftImg8bit_png.rf.550c0517305e8ff5344fd234a6c0f97b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_014658_leftImg8bit_png.rf.ab3930592d123c03c355f0eace7ff85a.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_014803_leftImg8bit_png.rf.caf45c9c363708685e260fc7b51be23b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_015038_leftImg8bit_png.rf.62e139d2f97498715ef3fe8a6b6e1c35.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_015321_leftImg8bit_png.rf.7f182a92115bb07442599a7db5ba172b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_015645_leftImg8bit_png.rf.a3651463751f17ab64b54c52fc3bfd66.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_015880_leftImg8bit_png.rf.158cbe1a9b498263d75e4f7a13219964.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_016125_leftImg8bit_png.rf.e16afcadd8849ede0789ddc05384d331.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_016260_leftImg8bit_png.rf.7f720d63a85187ae68bc78a48b497f62.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_016591_leftImg8bit_png.rf.8db9f2f9eb6b552673739c6fc74e38f5.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_016758_leftImg8bit_png.rf.3346b9076cbbde3513e9c3a752d37ea1.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_017216_leftImg8bit_png.rf.4cbea2ce893ddd2cfdaf22de929c2636.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_017453_leftImg8bit_png.rf.860c1c92b266e77ab84ef397473e95b9.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_018195_leftImg8bit_png.rf.cf6f05a76903407c272b24b22f6582f0.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_019188_leftImg8bit_png.rf.8557e31d399f843d310e76e65f0d7c80.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_020673_leftImg8bit_png.rf.dbd3a86dcbd8805d713e5c4217080444.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_020776_leftImg8bit_png.rf.369980b8627d9677e26ac8bdf83a7547.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_020899_leftImg8bit_png.rf.6ce4b719674fe57df93bb54dc5bbb998.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021070_leftImg8bit_png.rf.8325c8ae110f28f6fe237a04f2e33636.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021325_leftImg8bit_png.rf.b68052e0f3ee178253e75a0169bf49d5.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021393_leftImg8bit_png.rf.2b23e701b246cc68999af0cb35d07e8f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021479_leftImg8bit_png.rf.d9aa04d2edc3394d394bfa35e16ae490.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021606_leftImg8bit_png.rf.6702f2e90e4555f7f1460db2c5e5850a.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_022210_leftImg8bit_png.rf.c1177d53e1dfd1ef8bf890b09aa67aee.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_022414_leftImg8bit_png.rf.5d566b2d947f4b399ee2af9f3f5d1232.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_023040_leftImg8bit_png.rf.8f9eebc460ecdc4bff9b56f530d9aa1c.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_023174_leftImg8bit_png.rf.42032eae5d1bb3c9541b1b7bc9a02cc2.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_023435_leftImg8bit_png.rf.17cecf48854cef1e1e075a80c94e4153.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_023648_leftImg8bit_png.rf.e95f5a8364203c3f5d24ffa4ba25879d.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024196_leftImg8bit_png.rf.585d15dd2c92cab2f73b441ad4807b8b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024343_leftImg8bit_png.rf.edaa4c6ef8c708146cdc968949a20b55.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024524_leftImg8bit_png.rf.ba7c88de97f03a740dc6dc48254f0b68.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024717_leftImg8bit_png.rf.64e781abffc3371bb926895177aeccef.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024855_leftImg8bit_png.rf.35403fc1d9bc0a72c447e6858bac2a02.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_025746_leftImg8bit_png.rf.ec311dad296652b3ec4e81f6674af666.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_025833_leftImg8bit_png.rf.9705275256673249d132f470ce6fb911.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_026056_leftImg8bit_png.rf.344f8ddabad1025860710b809d278c20.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_026634_leftImg8bit_png.rf.5a6ad1ef572d13fa578e36c86598e1bc.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_027057_leftImg8bit_png.rf.5d389009d11e6c33bae12dc45b09c3d9.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_027699_leftImg8bit_png.rf.488e9485ee5f3ff55db89fa6522c63a4.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_027951_leftImg8bit_png.rf.283ec4d088ca18f8d32b44f9039eb5ad.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_028297_leftImg8bit_png.rf.c83bce1734ea69e0227ad541015fb28c.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_028764_leftImg8bit_png.rf.a8e19726cd87755d1cc1bd4e22699652.jpg  \n",
            "   creating: /content/data/valid/\n",
            "  inflating: /content/data/valid/frankfurt_000000_000294_leftImg8bit_foggy_beta_0-02_png.rf.c2a0450b7bf7d3f39586ed9369db1249.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_002196_leftImg8bit_foggy_beta_0-02_png.rf.6f7dc8665913b1ac97d75abc32e1c6b0.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_009969_leftImg8bit_foggy_beta_0-02_png.rf.f1092c1728d6bfc2ee5a14e80101c80f.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_011007_leftImg8bit_foggy_beta_0-02_png.rf.f6e02d25a75ab42533435a05de19bb72.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_011074_leftImg8bit_foggy_beta_0-02_png.rf.56ab812c27f4bf8c262ed3f5f6222acb.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_013240_leftImg8bit_foggy_beta_0-02_png.rf.062243a5bab8b0bf8100380830699e75.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_015676_leftImg8bit_foggy_beta_0-02_png.rf.70d176088e6a4dc95368ee80d59c693c.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_019607_leftImg8bit_foggy_beta_0-02_png.rf.ceac590ad0fe853db0234ebc6ed99bfd.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_005703_leftImg8bit_foggy_beta_0-02_png.rf.1b0a3da88fb76254f03fe5c5df63f6b6.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_005898_leftImg8bit_foggy_beta_0-02_png.rf.508ffebb4c64389fecdf026f4f7e31fb.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_007973_leftImg8bit_foggy_beta_0-02_png.rf.5a6e038e836a96659f4c02ab8fd60395.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_008200_leftImg8bit_foggy_beta_0-02_png.rf.d67810c16fc5d31829740615a41144a3.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_008688_leftImg8bit_foggy_beta_0-02_png.rf.39e1f6e43688ae54b70a07ce427b16f4.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_010444_leftImg8bit_foggy_beta_0-02_png.rf.d371692fd3a44794f1bb906d2a277d6c.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_010830_leftImg8bit_foggy_beta_0-02_png.rf.ebb8813eb2a457153269e8e9e7797546.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_011715_leftImg8bit_foggy_beta_0-02_png.rf.ea56aabbc6ad6516cbe65ee69ff60d24.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_012038_leftImg8bit_foggy_beta_0-02_png.rf.06e9c543f930542bae200501a3daa97e.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_013496_leftImg8bit_foggy_beta_0-02_png.rf.a9fd161c65f206ed54a9a1ce2dd7cdf5.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_013710_leftImg8bit_foggy_beta_0-02_png.rf.4f3b3e249a39d329062006c5388c08f5.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_014565_leftImg8bit_foggy_beta_0-02_png.rf.29d612355e66c17cb172a6c267fe93cd.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_015328_leftImg8bit_foggy_beta_0-02_png.rf.7913c26d307ee3746497754ca5d1a8ff.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_015768_leftImg8bit_foggy_beta_0-02_png.rf.1a49279c7cd8448e4f6d7310ed9a246f.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_016029_leftImg8bit_foggy_beta_0-02_png.rf.ed14fdbf4b7476a85bb9e7950af6143c.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_016273_leftImg8bit_foggy_beta_0-02_png.rf.a36abc5c830faa632f050420bcc32dab.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_019698_leftImg8bit_foggy_beta_0-02_png.rf.b83de9b0883b3c13ce60ad1fb8ddcd1b.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_019854_leftImg8bit_foggy_beta_0-02_png.rf.ac46fa4c572a5351e7c08d35d2a15244.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_023369_leftImg8bit_foggy_beta_0-02_png.rf.4ccd47061d27da8c0ddffca61de6e1db.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_023769_leftImg8bit_foggy_beta_0-02_png.rf.37ecacee997b346c1aa1448c33bd5d68.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_030310_leftImg8bit_foggy_beta_0-02_png.rf.8abcb03fc0381f18801a4b45178b3a0e.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_031266_leftImg8bit_foggy_beta_0-02_png.rf.d1d8ca276f8ba4f28c81092313241116.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_032711_leftImg8bit_foggy_beta_0-02_png.rf.0aadc1deeabec14c5473a53656b104e0.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_033655_leftImg8bit_foggy_beta_0-02_png.rf.a5610aa28a4497523ec3cc4831bda826.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_035144_leftImg8bit_foggy_beta_0-02_png.rf.976d787f379ab714057690e2bb45a7bc.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_041074_leftImg8bit_foggy_beta_0-02_png.rf.548fb0eaa2d0ff693de37065abc9a2f9.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_041517_leftImg8bit_foggy_beta_0-02_png.rf.3f620f592f4d36d748c78f89ca085b0c.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_044658_leftImg8bit_foggy_beta_0-02_png.rf.3c7da5b5647de24fbc15e984a7315c8d.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_046504_leftImg8bit_foggy_beta_0-02_png.rf.63d41411923cb0da824977215f502527.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_047552_leftImg8bit_foggy_beta_0-02_png.rf.f74767f4e161a99ffc40d5b6353ee127.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_049298_leftImg8bit_foggy_beta_0-02_png.rf.34c6567ae5b5c87116f19b07dff776c1.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_050686_leftImg8bit_foggy_beta_0-02_png.rf.c4bb3ff3d9a67bd5a2bbda9d1b0fef35.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_055062_leftImg8bit_foggy_beta_0-02_png.rf.a198c51d4a93b8e54aaa472ff50a67da.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_055306_leftImg8bit_foggy_beta_0-02_png.rf.7ec0bdb833038aac7a41a81c9380f781.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_058176_leftImg8bit_foggy_beta_0-02_png.rf.500d9c87620c21e88b63eed037576fd9.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_058914_leftImg8bit_foggy_beta_0-02_png.rf.89e72270a2929ad7f023c35a7a974fdb.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_060135_leftImg8bit_foggy_beta_0-02_png.rf.9a752f294b6f7ca117b405254bd1f074.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_060422_leftImg8bit_foggy_beta_0-02_png.rf.a65b2fc9d1d8a2a320d7fcdb489dfd03.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_060906_leftImg8bit_foggy_beta_0-02_png.rf.abba856b74a69f8620b6e92a4d279d48.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_062396_leftImg8bit_foggy_beta_0-02_png.rf.d8f39d44f467a4b109c3fc57ecf27351.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_065160_leftImg8bit_foggy_beta_0-02_png.rf.3f0308cdce5b5cd900fc122826b489ff.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_065617_leftImg8bit_foggy_beta_0-02_png.rf.1809f7409f68761b1923313e8b40fea5.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                                #util misc.py\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "\n",
        "    # serialized to a Tensor\n",
        "    buffer = pickle.dumps(data)\n",
        "    storage = torch.ByteStorage.from_buffer(buffer)\n",
        "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
        "\n",
        "    # obtain Tensor size of each rank\n",
        "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
        "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
        "    dist.all_gather(size_list, local_size)\n",
        "    size_list = [int(size.item()) for size in size_list]\n",
        "    max_size = max(size_list)\n",
        "\n",
        "    # receiving Tensor from all ranks\n",
        "    # we pad the tensor because torch all_gather does not support\n",
        "    # gathering tensors of different shapes\n",
        "    tensor_list = []\n",
        "    for _ in size_list:\n",
        "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
        "    if local_size != max_size:\n",
        "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
        "        tensor = torch.cat((tensor, padding), dim=0)\n",
        "    dist.all_gather(tensor_list, tensor)\n",
        "\n",
        "    data_list = []\n",
        "    for size, tensor in zip(size_list, tensor_list):\n",
        "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
        "        data_list.append(pickle.loads(buffer))\n",
        "\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.no_grad():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "\n",
        "\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(\n",
        "                \"{}: {}\".format(name, str(meter))\n",
        "            )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}',\n",
        "                'max mem: {memory:.0f}'\n",
        "            ])\n",
        "        else:\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}'\n",
        "            ])\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
        "            header, total_time_str, total_time / len(iterable)))\n",
        "\n",
        "\n",
        "def get_sha():\n",
        "    cwd = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "    def _run(command):\n",
        "        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n",
        "    sha = 'N/A'\n",
        "    diff = \"clean\"\n",
        "    branch = 'N/A'\n",
        "    try:\n",
        "        sha = _run(['git', 'rev-parse', 'HEAD'])\n",
        "        subprocess.check_output(['git', 'diff'], cwd=cwd)\n",
        "        diff = _run(['git', 'diff-index', 'HEAD'])\n",
        "        diff = \"has uncommited changes\" if diff else \"clean\"\n",
        "        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n",
        "    except Exception:\n",
        "        pass\n",
        "    message = f\"sha: {sha}, status: {diff}, branch: {branch}\"\n",
        "    return message\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = list(zip(*batch))\n",
        "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
        "    return tuple(batch)\n",
        "\n",
        "\n",
        "def _max_by_axis(the_list):\n",
        "    # type: (List[List[int]]) -> List[int]\n",
        "    maxes = the_list[0]\n",
        "    for sublist in the_list[1:]:\n",
        "        for index, item in enumerate(sublist):\n",
        "            maxes[index] = max(maxes[index], item)\n",
        "    return maxes\n",
        "\n",
        "\n",
        "class NestedTensor(object):\n",
        "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
        "        self.tensors = tensors\n",
        "        self.mask = mask\n",
        "\n",
        "    def to(self, device):\n",
        "        # type: (Device) -> NestedTensor # noqa\n",
        "        cast_tensor = self.tensors.to(device)\n",
        "        mask = self.mask\n",
        "        if mask is not None:\n",
        "            assert mask is not None\n",
        "            cast_mask = mask.to(device)\n",
        "        else:\n",
        "            cast_mask = None\n",
        "        return NestedTensor(cast_tensor, cast_mask)\n",
        "\n",
        "    def decompose(self):\n",
        "        return self.tensors, self.mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.tensors)\n",
        "\n",
        "\n",
        "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
        "    # TODO make this more general\n",
        "    if tensor_list[0].ndim == 3:\n",
        "        if torchvision._is_tracing():\n",
        "            # nested_tensor_from_tensor_list() does not export well to ONNX\n",
        "            # call _onnx_nested_tensor_from_tensor_list() instead\n",
        "            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n",
        "\n",
        "        # TODO make it support different-sized images\n",
        "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
        "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
        "        batch_shape = [len(tensor_list)] + max_size\n",
        "        b, c, h, w = batch_shape\n",
        "        dtype = tensor_list[0].dtype\n",
        "        device = tensor_list[0].device\n",
        "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
        "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
        "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
        "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "            m[: img.shape[1], :img.shape[2]] = False\n",
        "    else:\n",
        "        raise ValueError('not supported')\n",
        "    return NestedTensor(tensor, mask)\n",
        "\n",
        "\n",
        "# _onnx_nested_tensor_from_tensor_list() is an implementation of\n",
        "# nested_tensor_from_tensor_list() that is supported by ONNX tracing.\n",
        "@torch.jit.unused\n",
        "def _onnx_nested_tensor_from_tensor_list(tensor_list: List[Tensor]) -> NestedTensor:\n",
        "    max_size = []\n",
        "    for i in range(tensor_list[0].dim()):\n",
        "        max_size_i = torch.max(torch.stack([img.shape[i] for img in tensor_list]).to(torch.float32)).to(torch.int64)\n",
        "        max_size.append(max_size_i)\n",
        "    max_size = tuple(max_size)\n",
        "\n",
        "    # work around for\n",
        "    # pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "    # m[: img.shape[1], :img.shape[2]] = False\n",
        "    # which is not yet supported in onnx\n",
        "    padded_imgs = []\n",
        "    padded_masks = []\n",
        "    for img in tensor_list:\n",
        "        padding = [(s1 - s2) for s1, s2 in zip(max_size, tuple(img.shape))]\n",
        "        padded_img = torch.nn.functional.pad(img, (0, padding[2], 0, padding[1], 0, padding[0]))\n",
        "        padded_imgs.append(padded_img)\n",
        "\n",
        "        m = torch.zeros_like(img[0], dtype=torch.int, device=img.device)\n",
        "        padded_mask = torch.nn.functional.pad(m, (0, padding[2], 0, padding[1]), \"constant\", 1)\n",
        "        padded_masks.append(padded_mask.to(torch.bool))\n",
        "\n",
        "    tensor = torch.stack(padded_imgs)\n",
        "    mask = torch.stack(padded_masks)\n",
        "\n",
        "    return NestedTensor(tensor, mask=mask)\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop('force', False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
        "        args.rank = int(os.environ[\"RANK\"])\n",
        "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
        "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
        "    elif 'SLURM_PROCID' in os.environ:\n",
        "        args.rank = int(os.environ['SLURM_PROCID'])\n",
        "        args.gpu = args.rank % torch.cuda.device_count()\n",
        "    else:\n",
        "        print('Not using distributed mode')\n",
        "        args.distributed = False\n",
        "        return\n",
        "\n",
        "    args.distributed = True\n",
        "\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    args.dist_backend = 'nccl'\n",
        "    print('| distributed init (rank {}): {}'.format(\n",
        "        args.rank, args.dist_url), flush=True)\n",
        "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                         world_size=args.world_size, rank=args.rank)\n",
        "    torch.distributed.barrier()\n",
        "    setup_for_distributed(args.rank == 0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    if target.numel() == 0:\n",
        "        return [torch.zeros([], device=output.device)]\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
        "    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n",
        "    \"\"\"\n",
        "    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n",
        "    This will eventually be supported natively by PyTorch, and this\n",
        "    class can go away.\n",
        "    \"\"\"\n",
        "    if version.parse(torchvision.__version__) < version.parse('0.7'):\n",
        "        if input.numel() > 0:\n",
        "            return torch.nn.functional.interpolate(\n",
        "                input, size, scale_factor, mode, align_corners\n",
        "            )\n",
        "\n",
        "        output_shape = _output_size(2, input, size, scale_factor)\n",
        "        output_shape = list(input.shape[:-2]) + list(output_shape)\n",
        "        return _new_empty_tensor(input, output_shape)\n",
        "    else:\n",
        "        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)"
      ],
      "metadata": {
        "id": "_sXoHHZpupKf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEmbeddingSine(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a more standard version of the position embedding, very similar to the one\n",
        "    used by the Attention is all you need paper, generalized to work on images.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
        "        super().__init__()\n",
        "        self.num_pos_feats = num_pos_feats\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        if scale is not None and normalize is False:\n",
        "            raise ValueError(\"normalize should be True if scale is passed\")\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        x = tensor_list.tensors\n",
        "        mask = tensor_list.mask\n",
        "        assert mask is not None\n",
        "        not_mask = ~mask\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "        if self.normalize:\n",
        "            eps = 1e-6\n",
        "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos\n",
        "\n",
        "\n",
        "class PositionEmbeddingLearned(nn.Module):\n",
        "    \"\"\"\n",
        "    Absolute pos embedding, learned.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_pos_feats=256):\n",
        "        super().__init__()\n",
        "        self.row_embed = nn.Embedding(50, num_pos_feats)\n",
        "        self.col_embed = nn.Embedding(50, num_pos_feats)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.uniform_(self.row_embed.weight)\n",
        "        nn.init.uniform_(self.col_embed.weight)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        x = tensor_list.tensors\n",
        "        h, w = x.shape[-2:]\n",
        "        i = torch.arange(w, device=x.device)\n",
        "        j = torch.arange(h, device=x.device)\n",
        "        x_emb = self.col_embed(i)\n",
        "        y_emb = self.row_embed(j)\n",
        "        pos = torch.cat([\n",
        "            x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
        "            y_emb.unsqueeze(1).repeat(1, w, 1),\n",
        "        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
        "        return pos\n",
        "\n",
        "\n",
        "def build_position_encoding(args):\n",
        "    N_steps = args.hidden_dim // 2\n",
        "    if args.position_embedding in ('v2', 'sine'):\n",
        "        # TODO find a better way of exposing other arguments\n",
        "        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "    elif args.position_embedding in ('v3', 'learned'):\n",
        "        position_embedding = PositionEmbeddingLearned(N_steps)\n",
        "    else:\n",
        "        raise ValueError(f\"not supported {args.position_embedding}\")\n",
        "\n",
        "    return position_embedding"
      ],
      "metadata": {
        "id": "z7tUSid5d-aL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " class FrozenBatchNorm2d(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
        "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
        "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
        "    produce nans.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n):\n",
        "        super(FrozenBatchNorm2d, self).__init__()\n",
        "        self.register_buffer(\"weight\", torch.ones(n))\n",
        "        self.register_buffer(\"bias\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_var\", torch.ones(n))\n",
        "\n",
        "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
        "                              missing_keys, unexpected_keys, error_msgs):\n",
        "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
        "        if num_batches_tracked_key in state_dict:\n",
        "            del state_dict[num_batches_tracked_key]\n",
        "\n",
        "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
        "            state_dict, prefix, local_metadata, strict,\n",
        "            missing_keys, unexpected_keys, error_msgs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # move reshapes to the beginning\n",
        "        # to make it fuser-friendly\n",
        "        w = self.weight.reshape(1, -1, 1, 1)\n",
        "        b = self.bias.reshape(1, -1, 1, 1)\n",
        "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
        "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
        "        eps = 1e-5\n",
        "        scale = w * (rv + eps).rsqrt()\n",
        "        bias = b - rm * scale\n",
        "        return x * scale + bias\n",
        "\n",
        "\n",
        "class BackboneBase(nn.Module):\n",
        "\n",
        "    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):\n",
        "        super().__init__()\n",
        "        for name, parameter in backbone.named_parameters():\n",
        "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
        "                parameter.requires_grad_(False)\n",
        "        if return_interm_layers:\n",
        "            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n",
        "        else:\n",
        "            return_layers = {'layer4': \"0\"}\n",
        "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        xs = self.body(tensor_list.tensors)\n",
        "        out: Dict[str, NestedTensor] = {}\n",
        "        for name, x in xs.items():\n",
        "            m = tensor_list.mask\n",
        "            assert m is not None\n",
        "            mask = torch.nn.functional.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
        "            out[name] = NestedTensor(x, mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Backbone(BackboneBase):\n",
        "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
        "    def __init__(self, name: str,\n",
        "                 train_backbone: bool,\n",
        "                 return_interm_layers: bool,\n",
        "                 dilation: bool):\n",
        "        backbone = getattr(torchvision.models, name)(\n",
        "            replace_stride_with_dilation=[False, False, dilation],\n",
        "            pretrained=is_main_process(), norm_layer=FrozenBatchNorm2d)\n",
        "        num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n",
        "        super().__init__(backbone, train_backbone, num_channels, return_interm_layers)\n",
        "\n",
        "\n",
        "class Joiner(nn.Sequential):\n",
        "    def __init__(self, backbone, position_embedding):\n",
        "        super().__init__(backbone, position_embedding)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        xs = self[0](tensor_list)\n",
        "        out: List[NestedTensor] = []\n",
        "        pos = []\n",
        "        for name, x in xs.items():\n",
        "            out.append(x)\n",
        "            # position encoding\n",
        "            pos.append(self[1](x).to(x.tensors.dtype))\n",
        "\n",
        "        return out, pos\n",
        "\n",
        "\n",
        "def build_backbone(args):\n",
        "    position_embedding = build_position_encoding(args)\n",
        "    train_backbone = args.lr_backbone > 0\n",
        "    return_interm_layers = args.masks\n",
        "    backbone = Backbone(args.backbone, train_backbone, return_interm_layers, args.dilation)\n",
        "    model = Joiner(backbone, position_embedding)\n",
        "    model.num_channels = backbone.num_channels\n",
        "    return model"
      ],
      "metadata": {
        "id": "iZbz9oRtdxbW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(-1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "\n",
        "def box_xyxy_to_cxcywh(x):\n",
        "    x0, y0, x1, y1 = x.unbind(-1)\n",
        "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
        "         (x1 - x0), (y1 - y0)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "\n",
        "# modified from torchvision to also return the union\n",
        "def box_iou(boxes1, boxes2):\n",
        "    area1 = box_area(boxes1)\n",
        "    area2 = box_area(boxes2)\n",
        "\n",
        "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
        "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "\n",
        "    union = area1[:, None] + area2 - inter\n",
        "\n",
        "    iou = inter / union\n",
        "    return iou, union\n",
        "\n",
        "\n",
        "def generalized_box_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    Generalized IoU from https://giou.stanford.edu/\n",
        "    The boxes should be in [x0, y0, x1, y1] format\n",
        "    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
        "    and M = len(boxes2)\n",
        "    \"\"\"\n",
        "    # degenerate boxes gives inf / nan results\n",
        "    # so do an early check\n",
        "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
        "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
        "    iou, union = box_iou(boxes1, boxes2)\n",
        "\n",
        "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    area = wh[:, :, 0] * wh[:, :, 1]\n",
        "\n",
        "    return iou - (area - union) / area\n",
        "\n",
        "\n",
        "def masks_to_boxes(masks):\n",
        "    \"\"\"Compute the bounding boxes around the provided masks\n",
        "    The masks should be in format [N, H, W] where N is the number of masks, (H, W) are the spatial dimensions.\n",
        "    Returns a [N, 4] tensors, with the boxes in xyxy format\n",
        "    \"\"\"\n",
        "    if masks.numel() == 0:\n",
        "        return torch.zeros((0, 4), device=masks.device)\n",
        "\n",
        "    h, w = masks.shape[-2:]\n",
        "\n",
        "    y = torch.arange(0, h, dtype=torch.float)\n",
        "    x = torch.arange(0, w, dtype=torch.float)\n",
        "    y, x = torch.meshgrid(y, x)\n",
        "\n",
        "    x_mask = (masks * x.unsqueeze(0))\n",
        "    x_max = x_mask.flatten(1).max(-1)[0]\n",
        "    x_min = x_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
        "\n",
        "    y_mask = (masks * y.unsqueeze(0))\n",
        "    y_max = y_mask.flatten(1).max(-1)[0]\n",
        "    y_min = y_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
        "\n",
        "    return torch.stack([x_min, y_min, x_max, y_max], 1)"
      ],
      "metadata": {
        "id": "_9wRuXfUebzm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
        "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
        "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
        "    while the others are un-matched (and thus treated as non-objects).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n",
        "        \"\"\"Creates the matcher\n",
        "        Params:\n",
        "            cost_class: This is the relative weight of the classification error in the matching cost\n",
        "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
        "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" Performs the matching\n",
        "        Params:\n",
        "            outputs: This is a dict that contains at least these entries:\n",
        "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
        "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
        "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
        "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
        "                           objects in the target) containing the class labels\n",
        "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
        "        Returns:\n",
        "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
        "                - index_i is the indices of the selected predictions (in order)\n",
        "                - index_j is the indices of the corresponding selected targets (in order)\n",
        "            For each batch element, it holds:\n",
        "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
        "        \"\"\"\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "\n",
        "        # We flatten to compute the cost matrices in a batch\n",
        "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
        "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
        "\n",
        "        # Also concat the target labels and boxes\n",
        "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
        "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
        "\n",
        "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
        "        # but approximate it in 1 - proba[target class].\n",
        "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
        "        cost_class = -out_prob[:, tgt_ids]\n",
        "\n",
        "        # Compute the L1 cost between boxes\n",
        "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
        "\n",
        "        # Compute the giou cost betwen boxes\n",
        "        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))\n",
        "\n",
        "        # Final cost matrix\n",
        "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "\n",
        "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
        "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "\n",
        "\n",
        "def build_matcher(args):\n",
        "    return HungarianMatcher(cost_class=args.set_cost_class, cost_bbox=args.set_cost_bbox, cost_giou=args.set_cost_giou)"
      ],
      "metadata": {
        "id": "r40lsWnJeNEc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from panopticapi.utils import id2rgb, rgb2id\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "\n",
        "class DETRsegm(nn.Module):\n",
        "    def __init__(self, detr, freeze_detr=False):\n",
        "        super().__init__()\n",
        "        self.detr = detr\n",
        "\n",
        "        if freeze_detr:\n",
        "            for p in self.parameters():\n",
        "                p.requires_grad_(False)\n",
        "\n",
        "        hidden_dim, nheads = detr.transformer.d_model, detr.transformer.nhead\n",
        "        self.bbox_attention = MHAttentionMap(hidden_dim, hidden_dim, nheads, dropout=0.0)\n",
        "        self.mask_head = MaskHeadSmallConv(hidden_dim + nheads, [1024, 512, 256], hidden_dim)\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        if isinstance(samples, (list, torch.Tensor)):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        features, pos = self.detr.backbone(samples)\n",
        "\n",
        "        bs = features[-1].tensors.shape[0]\n",
        "\n",
        "        src, mask = features[-1].decompose()\n",
        "        assert mask is not None\n",
        "        src_proj = self.detr.input_proj(src)\n",
        "        hs, memory = self.detr.transformer(src_proj, mask, self.detr.query_embed.weight, pos[-1])\n",
        "\n",
        "        outputs_class = self.detr.class_embed(hs)\n",
        "        outputs_coord = self.detr.bbox_embed(hs).sigmoid()\n",
        "        out = {\"pred_logits\": outputs_class[-1], \"pred_boxes\": outputs_coord[-1]}\n",
        "        if self.detr.aux_loss:\n",
        "            out['aux_outputs'] = self.detr._set_aux_loss(outputs_class, outputs_coord)\n",
        "\n",
        "        # FIXME h_boxes takes the last one computed, keep this in mind\n",
        "        bbox_mask = self.bbox_attention(hs[-1], memory, mask=mask)\n",
        "\n",
        "        seg_masks = self.mask_head(src_proj, bbox_mask, [features[2].tensors, features[1].tensors, features[0].tensors])\n",
        "        outputs_seg_masks = seg_masks.view(bs, self.detr.num_queries, seg_masks.shape[-2], seg_masks.shape[-1])\n",
        "\n",
        "        out[\"pred_masks\"] = outputs_seg_masks\n",
        "        return out\n",
        "\n",
        "\n",
        "def _expand(tensor, length: int):\n",
        "    return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)\n",
        "\n",
        "\n",
        "class MaskHeadSmallConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple convolutional head, using group norm.\n",
        "    Upsampling is done using a FPN approach\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, fpn_dims, context_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]\n",
        "        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
        "        self.gn1 = torch.nn.GroupNorm(8, dim)\n",
        "        self.lay2 = torch.nn.Conv2d(dim, inter_dims[1], 3, padding=1)\n",
        "        self.gn2 = torch.nn.GroupNorm(8, inter_dims[1])\n",
        "        self.lay3 = torch.nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)\n",
        "        self.gn3 = torch.nn.GroupNorm(8, inter_dims[2])\n",
        "        self.lay4 = torch.nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)\n",
        "        self.gn4 = torch.nn.GroupNorm(8, inter_dims[3])\n",
        "        self.lay5 = torch.nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)\n",
        "        self.gn5 = torch.nn.GroupNorm(8, inter_dims[4])\n",
        "        self.out_lay = torch.nn.Conv2d(inter_dims[4], 1, 3, padding=1)\n",
        "\n",
        "        self.dim = dim\n",
        "\n",
        "        self.adapter1 = torch.nn.Conv2d(fpn_dims[0], inter_dims[1], 1)\n",
        "        self.adapter2 = torch.nn.Conv2d(fpn_dims[1], inter_dims[2], 1)\n",
        "        self.adapter3 = torch.nn.Conv2d(fpn_dims[2], inter_dims[3], 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor, bbox_mask: Tensor, fpns: List[Tensor]):\n",
        "        x = torch.cat([_expand(x, bbox_mask.shape[1]), bbox_mask.flatten(0, 1)], 1)\n",
        "\n",
        "        x = self.lay1(x)\n",
        "        x = self.gn1(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.lay2(x)\n",
        "        x = self.gn2(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        cur_fpn = self.adapter1(fpns[0])\n",
        "        if cur_fpn.size(0) != x.size(0):\n",
        "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
        "        x = cur_fpn + torch.nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "        x = self.lay3(x)\n",
        "        x = self.gn3(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        cur_fpn = self.adapter2(fpns[1])\n",
        "        if cur_fpn.size(0) != x.size(0):\n",
        "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
        "        x = cur_fpn + torch.nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "        x = self.lay4(x)\n",
        "        x = self.gn4(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        cur_fpn = self.adapter3(fpns[2])\n",
        "        if cur_fpn.size(0) != x.size(0):\n",
        "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
        "        x = cur_fpn + torch.nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "        x = self.lay5(x)\n",
        "        x = self.gn5(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        x = self.out_lay(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MHAttentionMap(nn.Module):\n",
        "    \"\"\"This is a 2D attention module, which only returns the attention softmax (no multiplication by value)\"\"\"\n",
        "\n",
        "    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0.0, bias=True):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
        "        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
        "\n",
        "        nn.init.zeros_(self.k_linear.bias)\n",
        "        nn.init.zeros_(self.q_linear.bias)\n",
        "        nn.init.xavier_uniform_(self.k_linear.weight)\n",
        "        nn.init.xavier_uniform_(self.q_linear.weight)\n",
        "        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5\n",
        "\n",
        "    def forward(self, q, k, mask: Optional[Tensor] = None):\n",
        "        q = self.q_linear(q)\n",
        "        k = torch.nn.functional.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)\n",
        "        qh = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)\n",
        "        kh = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])\n",
        "        weights = torch.einsum(\"bqnc,bnchw->bqnhw\", qh * self.normalize_fact, kh)\n",
        "\n",
        "        if mask is not None:\n",
        "            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
        "        weights = torch.nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())\n",
        "        weights = self.dropout(weights)\n",
        "        return weights\n",
        "\n",
        "\n",
        "def dice_loss(inputs, targets, num_boxes):\n",
        "    \"\"\"\n",
        "    Compute the DICE loss, similar to generalized IOU for masks\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "    \"\"\"\n",
        "    inputs = inputs.sigmoid()\n",
        "    inputs = inputs.flatten(1)\n",
        "    numerator = 2 * (inputs * targets).sum(1)\n",
        "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
        "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
        "    return loss.sum() / num_boxes\n",
        "\n",
        "\n",
        "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n",
        "    \"\"\"\n",
        "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
        "                positive vs negative examples. Default = -1 (no weighting).\n",
        "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
        "               balance easy vs hard examples.\n",
        "    Returns:\n",
        "        Loss tensor\n",
        "    \"\"\"\n",
        "    prob = inputs.sigmoid()\n",
        "    ce_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
        "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    return loss.mean(1).sum() / num_boxes\n",
        "\n",
        "\n",
        "class PostProcessSegm(nn.Module):\n",
        "    def __init__(self, threshold=0.5):\n",
        "        super().__init__()\n",
        "        self.threshold = threshold\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n",
        "        assert len(orig_target_sizes) == len(max_target_sizes)\n",
        "        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n",
        "        outputs_masks = outputs[\"pred_masks\"].squeeze(2)\n",
        "        outputs_masks = torch.nn.functional.interpolate(outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False)\n",
        "        outputs_masks = (outputs_masks.sigmoid() > self.threshold).cpu()\n",
        "\n",
        "        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n",
        "            img_h, img_w = t[0], t[1]\n",
        "            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n",
        "            results[i][\"masks\"] = torch.nn.functional.interpolate(\n",
        "                results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\"\n",
        "            ).byte()\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class PostProcessPanoptic(nn.Module):\n",
        "    \"\"\"This class converts the output of the model to the final panoptic result, in the format expected by the\n",
        "    coco panoptic API \"\"\"\n",
        "\n",
        "    def __init__(self, is_thing_map, threshold=0.85):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "           is_thing_map: This is a whose keys are the class ids, and the values a boolean indicating whether\n",
        "                          the class is  a thing (True) or a stuff (False) class\n",
        "           threshold: confidence threshold: segments with confidence lower than this will be deleted\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.threshold = threshold\n",
        "        self.is_thing_map = is_thing_map\n",
        "\n",
        "    def forward(self, outputs, processed_sizes, target_sizes=None):\n",
        "        \"\"\" This function computes the panoptic prediction from the model's predictions.\n",
        "        Parameters:\n",
        "            outputs: This is a dict coming directly from the model. See the model doc for the content.\n",
        "            processed_sizes: This is a list of tuples (or torch tensors) of sizes of the images that were passed to the\n",
        "                             model, ie the size after data augmentation but before batching.\n",
        "            target_sizes: This is a list of tuples (or torch tensors) corresponding to the requested final size\n",
        "                          of each prediction. If left to None, it will default to the processed_sizes\n",
        "            \"\"\"\n",
        "        if target_sizes is None:\n",
        "            target_sizes = processed_sizes\n",
        "        assert len(processed_sizes) == len(target_sizes)\n",
        "        out_logits, raw_masks, raw_boxes = outputs[\"pred_logits\"], outputs[\"pred_masks\"], outputs[\"pred_boxes\"]\n",
        "        assert len(out_logits) == len(raw_masks) == len(target_sizes)\n",
        "        preds = []\n",
        "\n",
        "        def to_tuple(tup):\n",
        "            if isinstance(tup, tuple):\n",
        "                return tup\n",
        "            return tuple(tup.cpu().tolist())\n",
        "\n",
        "        for cur_logits, cur_masks, cur_boxes, size, target_size in zip(\n",
        "            out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes\n",
        "        ):\n",
        "            # we filter empty queries and detection below threshold\n",
        "            scores, labels = cur_logits.softmax(-1).max(-1)\n",
        "            keep = labels.ne(outputs[\"pred_logits\"].shape[-1] - 1) & (scores > self.threshold)\n",
        "            cur_scores, cur_classes = cur_logits.softmax(-1).max(-1)\n",
        "            cur_scores = cur_scores[keep]\n",
        "            cur_classes = cur_classes[keep]\n",
        "            cur_masks = cur_masks[keep]\n",
        "            cur_masks = interpolate(cur_masks[:, None], to_tuple(size), mode=\"bilinear\").squeeze(1)\n",
        "            cur_boxes = box_cxcywh_to_xyxy(cur_boxes[keep])\n",
        "\n",
        "            h, w = cur_masks.shape[-2:]\n",
        "            assert len(cur_boxes) == len(cur_classes)\n",
        "\n",
        "            # It may be that we have several predicted masks for the same stuff class.\n",
        "            # In the following, we track the list of masks ids for each stuff class (they are merged later on)\n",
        "            cur_masks = cur_masks.flatten(1)\n",
        "            stuff_equiv_classes = defaultdict(lambda: [])\n",
        "            for k, label in enumerate(cur_classes):\n",
        "                if not self.is_thing_map[label.item()]:\n",
        "                    stuff_equiv_classes[label.item()].append(k)\n",
        "\n",
        "            def get_ids_area(masks, scores, dedup=False):\n",
        "                # This helper function creates the final panoptic segmentation image\n",
        "                # It also returns the area of the masks that appears on the image\n",
        "\n",
        "                m_id = masks.transpose(0, 1).softmax(-1)\n",
        "\n",
        "                if m_id.shape[-1] == 0:\n",
        "                    # We didn't detect any mask :(\n",
        "                    m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)\n",
        "                else:\n",
        "                    m_id = m_id.argmax(-1).view(h, w)\n",
        "\n",
        "                if dedup:\n",
        "                    # Merge the masks corresponding to the same stuff class\n",
        "                    for equiv in stuff_equiv_classes.values():\n",
        "                        if len(equiv) > 1:\n",
        "                            for eq_id in equiv:\n",
        "                                m_id.masked_fill_(m_id.eq(eq_id), equiv[0])\n",
        "\n",
        "                final_h, final_w = to_tuple(target_size)\n",
        "\n",
        "                seg_img = Image.fromarray(id2rgb(m_id.view(h, w).cpu().numpy()))\n",
        "                seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)\n",
        "\n",
        "                np_seg_img = (\n",
        "                    torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes())).view(final_h, final_w, 3).numpy()\n",
        "                )\n",
        "                m_id = torch.from_numpy(rgb2id(np_seg_img))\n",
        "\n",
        "                area = []\n",
        "                for i in range(len(scores)):\n",
        "                    area.append(m_id.eq(i).sum().item())\n",
        "                return area, seg_img\n",
        "\n",
        "            area, seg_img = get_ids_area(cur_masks, cur_scores, dedup=True)\n",
        "            if cur_classes.numel() > 0:\n",
        "                # We know filter empty masks as long as we find some\n",
        "                while True:\n",
        "                    filtered_small = torch.as_tensor(\n",
        "                        [area[i] <= 4 for i, c in enumerate(cur_classes)], dtype=torch.bool, device=keep.device\n",
        "                    )\n",
        "                    if filtered_small.any().item():\n",
        "                        cur_scores = cur_scores[~filtered_small]\n",
        "                        cur_classes = cur_classes[~filtered_small]\n",
        "                        cur_masks = cur_masks[~filtered_small]\n",
        "                        area, seg_img = get_ids_area(cur_masks, cur_scores)\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "            else:\n",
        "                cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)\n",
        "\n",
        "            segments_info = []\n",
        "            for i, a in enumerate(area):\n",
        "                cat = cur_classes[i].item()\n",
        "                segments_info.append({\"id\": i, \"isthing\": self.is_thing_map[cat], \"category_id\": cat, \"area\": a})\n",
        "            del cur_classes\n",
        "\n",
        "            with io.BytesIO() as out:\n",
        "                seg_img.save(out, format=\"PNG\")\n",
        "                predictions = {\"png_string\": out.getvalue(), \"segments_info\": segments_info}\n",
        "            preds.append(predictions)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "s35pEGzafkaI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n",
        "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False,\n",
        "                 return_intermediate_dec=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
        "                                                dropout, activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
        "                                                dropout, activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n",
        "                                          return_intermediate=return_intermediate_dec)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        # flatten NxCxHxW to HWxNxC\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed, query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        output = src\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, src_mask=mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self.return_intermediate = return_intermediate\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        output = tgt\n",
        "\n",
        "        intermediate = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, memory, tgt_mask=tgt_mask,\n",
        "                           memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos, query_pos=query_pos)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.append(self.norm(output))\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.pop()\n",
        "                intermediate.append(output)\n",
        "\n",
        "        if self.return_intermediate:\n",
        "            return torch.stack(intermediate)\n",
        "\n",
        "        return output.unsqueeze(0)\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask: Optional[Tensor] = None,\n",
        "                     src_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(self, src,\n",
        "                    src_mask: Optional[Tensor] = None,\n",
        "                    src_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None):\n",
        "        src2 = self.norm1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src2 = self.norm2(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(self, src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self, tgt, memory,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self, tgt, memory,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt2 = self.norm2(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt2 = self.norm3(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
        "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def build_transformer(args):\n",
        "    return Transformer(\n",
        "        d_model=args.hidden_dim,\n",
        "        dropout=args.dropout,\n",
        "        nhead=args.nheads,\n",
        "        dim_feedforward=args.dim_feedforward,\n",
        "        num_encoder_layers=args.enc_layers,\n",
        "        num_decoder_layers=args.dec_layers,\n",
        "        normalize_before=args.pre_norm,\n",
        "        return_intermediate_dec=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"relu\":\n",
        "        return torch.nn.functional.relu\n",
        "    if activation == \"gelu\":\n",
        "        return torch.nn.functional.gelu\n",
        "    if activation == \"glu\":\n",
        "        return torch.nn.functional.glu\n",
        "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")"
      ],
      "metadata": {
        "id": "iBVuKFedfv5a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "# Autograd Function objects are what record operation history on tensors,\n",
        "# and define formulas for the forward and backprop.\n",
        "\n",
        "class GradientReversalFn(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        # Store context for backprop\n",
        "        ctx.alpha = alpha\n",
        "        \n",
        "        # Forward pass is a no-op\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Backward pass is just to -alpha the gradient\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        # Must return same number as inputs to forward()\n",
        "        return output, None"
      ],
      "metadata": {
        "id": "GgySY6tH3uGa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DETR(nn.Module):\n",
        "    \"\"\" This is the DETR module that performs object detection \"\"\"\n",
        "    def __init__(self, backbone, transformer, num_classes, num_queries, aux_loss=False):\n",
        "        \"\"\" Initializes the model.\n",
        "        Parameters:\n",
        "            backbone: torch module of the backbone to be used. See backbone.py\n",
        "            transformer: torch module of the transformer architecture. See transformer.py\n",
        "            num_classes: number of object classes\n",
        "            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
        "                         DETR can detect in a single image. For COCO, we recommend 100 queries.\n",
        "            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.d_model\n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n",
        "        self.backbone = backbone\n",
        "        self.aux_loss = aux_loss\n",
        "        self.num_channels_backbone = backbone.num_channels\n",
        "        self.domain_classifier = nn.Sequential(\n",
        "            nn.Conv2d(2048, 1024, kernel_size=1, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(19*19, 100), nn.BatchNorm1d(100),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(100, 2),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        \"\"\" The forward expects a NestedTensor, which consists of:\n",
        "               - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
        "               - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n",
        "            It returns a dict with the following elements:\n",
        "               - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
        "                                Shape= [batch_size x num_queries x (num_classes + 1)]\n",
        "               - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n",
        "                               (center_x, center_y, height, width). These values are normalized in [0, 1],\n",
        "                               relative to the size of each individual image (disregarding possible padding).\n",
        "                               See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
        "               - \"aux_outputs\": Optional, only returned when auxilary losses are activated. It is a list of\n",
        "                                dictionnaries containing the two above keys for each decoder layer.\n",
        "        \"\"\"\n",
        "        if isinstance(samples, (list, torch.Tensor)):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        features, pos = self.backbone(samples)\n",
        "\n",
        "        src, mask = features[-1].decompose()\n",
        "        assert mask is not None\n",
        "        src1=src\n",
        "        # print(\"dim after resnet \", src.size())\n",
        "        rev_features = GradientReversalFn.apply(src1, 0.1)\n",
        "\n",
        "        domain_pred = self.domain_classifier(rev_features)\n",
        "        # print(\"domain_pred\", domain_pred)\n",
        "\n",
        "\n",
        "        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n",
        "\n",
        "        outputs_class = self.class_embed(hs)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n",
        "        if self.aux_loss:\n",
        "            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n",
        "\n",
        "        return out, domain_pred\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def _set_aux_loss(self, outputs_class, outputs_coord):\n",
        "        # this is a workaround to make torchscript happy, as torchscript\n",
        "        # doesn't support dictionary with non-homogeneous values, such\n",
        "        # as a dict having both a Tensor and a list.\n",
        "        return [{'pred_logits': a, 'pred_boxes': b}\n",
        "                for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n",
        "\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\" This class computes the loss for DETR.\n",
        "    The process happens in two steps:\n",
        "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
        "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
        "        \"\"\" Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of object categories, omitting the special no-object category\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
        "            eos_coef: relative classification weight applied to the no-object category\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer('empty_weight', empty_weight)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
        "        \"\"\"\n",
        "        assert 'pred_logits' in outputs\n",
        "        src_logits = outputs['pred_logits']\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                    dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        loss_ce = torch.nn.functional.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
        "        losses = {'loss_ce': loss_ce}\n",
        "\n",
        "        if log:\n",
        "            # TODO this should probably be a separate loss, not hacked in this one here\n",
        "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
        "        return losses\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
        "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
        "        \"\"\"\n",
        "        pred_logits = outputs['pred_logits']\n",
        "        device = pred_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
        "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
        "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = torch.nn.functional.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {'cardinality_error': card_err}\n",
        "        return losses\n",
        "\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
        "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
        "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
        "        \"\"\"\n",
        "        assert 'pred_boxes' in outputs\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_bbox = torch.nn.functional.l1_loss(src_boxes, target_boxes, reduction='none')\n",
        "\n",
        "        losses = {}\n",
        "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
        "\n",
        "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
        "            box_cxcywh_to_xyxy(src_boxes),\n",
        "            box_cxcywh_to_xyxy(target_boxes)))\n",
        "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
        "        return losses\n",
        "\n",
        "    def loss_masks(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
        "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
        "        \"\"\"\n",
        "        assert \"pred_masks\" in outputs\n",
        "\n",
        "        src_idx = self._get_src_permutation_idx(indices)\n",
        "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
        "        src_masks = outputs[\"pred_masks\"]\n",
        "        src_masks = src_masks[src_idx]\n",
        "        masks = [t[\"masks\"] for t in targets]\n",
        "        # TODO use valid to mask invalid areas due to padding in loss\n",
        "        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
        "        target_masks = target_masks.to(src_masks)\n",
        "        target_masks = target_masks[tgt_idx]\n",
        "\n",
        "        # upsample predictions to the target size\n",
        "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
        "                                mode=\"bilinear\", align_corners=False)\n",
        "        src_masks = src_masks[:, 0].flatten(1)\n",
        "\n",
        "        target_masks = target_masks.flatten(1)\n",
        "        target_masks = target_masks.view(src_masks.shape)\n",
        "        losses = {\n",
        "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
        "            \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
        "        }\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            'labels': self.loss_labels,\n",
        "            'cardinality': self.loss_cardinality,\n",
        "            'boxes': self.loss_boxes,\n",
        "            'masks': self.loss_masks\n",
        "        }\n",
        "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
        "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
        "\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs_without_aux, targets)\n",
        "\n",
        "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "        if is_dist_avail_and_initialized():\n",
        "            torch.distributed.all_reduce(num_boxes)\n",
        "        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()\n",
        "\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
        "        if 'aux_outputs' in outputs:\n",
        "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
        "                indices = self.matcher(aux_outputs, targets)\n",
        "                for loss in self.losses:\n",
        "                    if loss == 'masks':\n",
        "                        # Intermediate masks losses are too costly to compute, we ignore them.\n",
        "                        continue\n",
        "                    kwargs = {}\n",
        "                    if loss == 'labels':\n",
        "                        # Logging is enabled only for the last layer\n",
        "                        kwargs = {'log': False}\n",
        "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)\n",
        "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
        "                    losses.update(l_dict)\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "class PostProcess(nn.Module):\n",
        "    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, target_sizes):\n",
        "        \"\"\" Perform the computation\n",
        "        Parameters:\n",
        "            outputs: raw outputs of the model\n",
        "            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n",
        "                          For evaluation, this must be the original image size (before any data augmentation)\n",
        "                          For visualization, this should be the image size after data augment, but before padding\n",
        "        \"\"\"\n",
        "        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']\n",
        "\n",
        "        assert len(out_logits) == len(target_sizes)\n",
        "        assert target_sizes.shape[1] == 2\n",
        "\n",
        "        prob = torch.nn.functional.softmax(out_logits, -1)\n",
        "        scores, labels = prob[..., :-1].max(-1)\n",
        "\n",
        "        # convert to [x0, y0, x1, y1] format\n",
        "        boxes = box_cxcywh_to_xyxy(out_bbox)\n",
        "        # and from relative [0, 1] to absolute [0, height] coordinates\n",
        "        img_h, img_w = target_sizes.unbind(1)\n",
        "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
        "        boxes = boxes * scale_fct[:, None, :]\n",
        "\n",
        "        results = [{'scores': s, 'labels': l, 'boxes': b} for s, l, b in zip(scores, labels, boxes)]\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = torch.nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def build(args):\n",
        "    # the `num_classes` naming here is somewhat misleading.\n",
        "    # it indeed corresponds to `max_obj_id + 1`, where max_obj_id\n",
        "    # is the maximum id for a class in your dataset. For example,\n",
        "    # COCO has a max_obj_id of 90, so we pass `num_classes` to be 91.\n",
        "    # As another example, for a dataset that has a single class with id 1,\n",
        "    # you should pass `num_classes` to be 2 (max_obj_id + 1).\n",
        "    # For more details on this, check the following discussion\n",
        "    # https://github.com/facebookresearch/detr/issues/108#issuecomment-650269223\n",
        "    num_classes = 20 if args.dataset_file != 'coco' else 9\n",
        "    if args.dataset_file == \"coco_panoptic\":\n",
        "        # for panoptic, we just add a num_classes that is large enough to hold\n",
        "        # max_obj_id + 1, but the exact value doesn't really matter\n",
        "        num_classes = 250\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    backbone = build_backbone(args)\n",
        "\n",
        "    transformer = build_transformer(args)\n",
        "\n",
        "    model = DETR(\n",
        "        backbone,\n",
        "        transformer,\n",
        "        num_classes=num_classes,\n",
        "        num_queries=args.num_queries,\n",
        "        aux_loss=args.aux_loss,\n",
        "    )\n",
        "    if args.masks:\n",
        "        model = DETRsegm(model, freeze_detr=(args.frozen_weights is not None))\n",
        "    matcher = build_matcher(args)\n",
        "    weight_dict = {'loss_ce': 1, 'loss_bbox': args.bbox_loss_coef}\n",
        "    weight_dict['loss_giou'] = args.giou_loss_coef\n",
        "    if args.masks:\n",
        "        weight_dict[\"loss_mask\"] = args.mask_loss_coef\n",
        "        weight_dict[\"loss_dice\"] = args.dice_loss_coef\n",
        "    # TODO this is a hack\n",
        "    if args.aux_loss:\n",
        "        aux_weight_dict = {}\n",
        "        for i in range(args.dec_layers - 1):\n",
        "            aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})\n",
        "        weight_dict.update(aux_weight_dict)\n",
        "\n",
        "    losses = ['labels', 'boxes', 'cardinality']\n",
        "    if args.masks:\n",
        "        losses += [\"masks\"]\n",
        "    criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict,\n",
        "                             eos_coef=args.eos_coef, losses=losses)\n",
        "    criterion.to(device)\n",
        "    postprocessors = {'bbox': PostProcess()}\n",
        "    if args.masks:\n",
        "        postprocessors['segm'] = PostProcessSegm()\n",
        "        if args.dataset_file == \"coco_panoptic\":\n",
        "            is_thing_map = {i: i <= 90 for i in range(201)}\n",
        "            postprocessors[\"panoptic\"] = PostProcessPanoptic(is_thing_map, threshold=0.85)\n",
        "\n",
        "    return model, criterion, postprocessors"
      ],
      "metadata": {
        "id": "4krKVB3edRMJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(args):\n",
        "    return build(args)"
      ],
      "metadata": {
        "id": "JJbCcgswvAJx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.functional as F\n",
        "\n",
        "def crop(image, target, region):\n",
        "    cropped_image = F.crop(image, *region)\n",
        "\n",
        "    target = target.copy()\n",
        "    i, j, h, w = region\n",
        "\n",
        "    # should we do something wrt the original size?\n",
        "    target[\"size\"] = torch.tensor([h, w])\n",
        "\n",
        "    fields = [\"labels\", \"area\", \"iscrowd\"]\n",
        "\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        max_size = torch.as_tensor([w, h], dtype=torch.float32)\n",
        "        cropped_boxes = boxes - torch.as_tensor([j, i, j, i])\n",
        "        cropped_boxes = torch.min(cropped_boxes.reshape(-1, 2, 2), max_size)\n",
        "        cropped_boxes = cropped_boxes.clamp(min=0)\n",
        "        area = (cropped_boxes[:, 1, :] - cropped_boxes[:, 0, :]).prod(dim=1)\n",
        "        target[\"boxes\"] = cropped_boxes.reshape(-1, 4)\n",
        "        target[\"area\"] = area\n",
        "        fields.append(\"boxes\")\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        # FIXME should we update the area here if there are no boxes?\n",
        "        target['masks'] = target['masks'][:, i:i + h, j:j + w]\n",
        "        fields.append(\"masks\")\n",
        "\n",
        "    # remove elements for which the boxes or masks that have zero area\n",
        "    if \"boxes\" in target or \"masks\" in target:\n",
        "        # favor boxes selection when defining which elements to keep\n",
        "        # this is compatible with previous implementation\n",
        "        if \"boxes\" in target:\n",
        "            cropped_boxes = target['boxes'].reshape(-1, 2, 2)\n",
        "            keep = torch.all(cropped_boxes[:, 1, :] > cropped_boxes[:, 0, :], dim=1)\n",
        "        else:\n",
        "            keep = target['masks'].flatten(1).any(1)\n",
        "\n",
        "        for field in fields:\n",
        "            target[field] = target[field][keep]\n",
        "\n",
        "    return cropped_image, target\n",
        "\n",
        "\n",
        "def hflip(image, target):\n",
        "    flipped_image = F.hflip(image)\n",
        "\n",
        "    w, h = image.size\n",
        "\n",
        "    target = target.copy()\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n",
        "        target[\"boxes\"] = boxes\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = target['masks'].flip(-1)\n",
        "\n",
        "    return flipped_image, target\n",
        "\n",
        "\n",
        "def resize(image, target, size, max_size=None):\n",
        "    # size can be min_size (scalar) or (w, h) tuple\n",
        "\n",
        "    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n",
        "        w, h = image_size\n",
        "        if max_size is not None:\n",
        "            min_original_size = float(min((w, h)))\n",
        "            max_original_size = float(max((w, h)))\n",
        "            if max_original_size / min_original_size * size > max_size:\n",
        "                size = int(round(max_size * min_original_size / max_original_size))\n",
        "\n",
        "        if (w <= h and w == size) or (h <= w and h == size):\n",
        "            return (h, w)\n",
        "\n",
        "        if w < h:\n",
        "            ow = size\n",
        "            oh = int(size * h / w)\n",
        "        else:\n",
        "            oh = size\n",
        "            ow = int(size * w / h)\n",
        "\n",
        "        return (oh, ow)\n",
        "\n",
        "    def get_size(image_size, size, max_size=None):\n",
        "        if isinstance(size, (list, tuple)):\n",
        "            return size[::-1]\n",
        "        else:\n",
        "            return get_size_with_aspect_ratio(image_size, size, max_size)\n",
        "\n",
        "    size = get_size(image.size, size, max_size)\n",
        "    rescaled_image = F.resize(image, size)\n",
        "\n",
        "    if target is None:\n",
        "        return rescaled_image, None\n",
        "\n",
        "    ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(rescaled_image.size, image.size))\n",
        "    ratio_width, ratio_height = ratios\n",
        "\n",
        "    target = target.copy()\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        scaled_boxes = boxes * torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height])\n",
        "        target[\"boxes\"] = scaled_boxes\n",
        "\n",
        "    if \"area\" in target:\n",
        "        area = target[\"area\"]\n",
        "        scaled_area = area * (ratio_width * ratio_height)\n",
        "        target[\"area\"] = scaled_area\n",
        "\n",
        "    h, w = size\n",
        "    target[\"size\"] = torch.tensor([h, w])\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = interpolate(\n",
        "            target['masks'][:, None].float(), size, mode=\"nearest\")[:, 0] > 0.5\n",
        "\n",
        "    return rescaled_image, target\n",
        "\n",
        "\n",
        "def pad(image, target, padding):\n",
        "    # assumes that we only pad on the bottom right corners\n",
        "    padded_image = F.pad(image, (0, 0, padding[0], padding[1]))\n",
        "    if target is None:\n",
        "        return padded_image, None\n",
        "    target = target.copy()\n",
        "    # should we do something wrt the original size?\n",
        "    target[\"size\"] = torch.tensor(padded_image.size[::-1])\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = torch.nn.functional.pad(target['masks'], (0, padding[0], 0, padding[1]))\n",
        "    return padded_image, target\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        region = T.RandomCrop.get_params(img, self.size)\n",
        "        return crop(img, target, region)\n",
        "\n",
        "\n",
        "class RandomSizeCrop(object):\n",
        "    def __init__(self, min_size: int, max_size: int):\n",
        "        self.min_size = min_size\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, img: PIL.Image.Image, target: dict):\n",
        "        w = random.randint(self.min_size, min(img.width, self.max_size))\n",
        "        h = random.randint(self.min_size, min(img.height, self.max_size))\n",
        "        region = T.RandomCrop.get_params(img, [h, w])\n",
        "        return crop(img, target, region)\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        image_width, image_height = img.size\n",
        "        crop_height, crop_width = self.size\n",
        "        crop_top = int(round((image_height - crop_height) / 2.))\n",
        "        crop_left = int(round((image_width - crop_width) / 2.))\n",
        "        return crop(img, target, (crop_top, crop_left, crop_height, crop_width))\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random() < self.p:\n",
        "            return hflip(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class RandomResize(object):\n",
        "    def __init__(self, sizes, max_size=None):\n",
        "        assert isinstance(sizes, (list, tuple))\n",
        "        self.sizes = sizes\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, img, target=None):\n",
        "        size = random.choice(self.sizes)\n",
        "        return resize(img, target, size, self.max_size)\n",
        "\n",
        "\n",
        "class RandomPad(object):\n",
        "    def __init__(self, max_pad):\n",
        "        self.max_pad = max_pad\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        pad_x = random.randint(0, self.max_pad)\n",
        "        pad_y = random.randint(0, self.max_pad)\n",
        "        return pad(img, target, (pad_x, pad_y))\n",
        "\n",
        "\n",
        "class RandomSelect(object):\n",
        "    \"\"\"\n",
        "    Randomly selects between transforms1 and transforms2,\n",
        "    with probability p for transforms1 and (1 - p) for transforms2\n",
        "    \"\"\"\n",
        "    def __init__(self, transforms1, transforms2, p=0.5):\n",
        "        self.transforms1 = transforms1\n",
        "        self.transforms2 = transforms2\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random() < self.p:\n",
        "            return self.transforms1(img, target)\n",
        "        return self.transforms2(img, target)\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, img, target):\n",
        "        return F.to_tensor(img), target\n",
        "\n",
        "\n",
        "class RandomErasing(object):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.eraser = T.RandomErasing(*args, **kwargs)\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        return self.eraser(img), target\n",
        "\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target=None):\n",
        "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "        if target is None:\n",
        "            return image, None\n",
        "        target = target.copy()\n",
        "        h, w = image.shape[-2:]\n",
        "        if \"boxes\" in target:\n",
        "            boxes = target[\"boxes\"]\n",
        "            boxes = box_xyxy_to_cxcywh(boxes)\n",
        "            boxes = boxes / torch.tensor([w, h, w, h], dtype=torch.float32)\n",
        "            target[\"boxes\"] = boxes\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + \"(\"\n",
        "        for t in self.transforms:\n",
        "            format_string += \"\\n\"\n",
        "            format_string += \"    {0}\".format(t)\n",
        "        format_string += \"\\n)\"\n",
        "        return format_string"
      ],
      "metadata": {
        "id": "pGFF3M8cdneg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class CocoPanoptic:\n",
        "    def __init__(self, img_folder, ann_folder, ann_file, transforms=None, return_masks=True):\n",
        "        with open(ann_file, 'r') as f:\n",
        "            self.coco = json.load(f)\n",
        "\n",
        "        # sort 'images' field so that they are aligned with 'annotations'\n",
        "        # i.e., in alphabetical order\n",
        "        self.coco['images'] = sorted(self.coco['images'], key=lambda x: x['id'])\n",
        "        # sanity check\n",
        "        if \"annotations\" in self.coco:\n",
        "            for img, ann in zip(self.coco['images'], self.coco['annotations']):\n",
        "                assert img['file_name'][:-4] == ann['file_name'][:-4]\n",
        "\n",
        "        self.img_folder = img_folder\n",
        "        self.ann_folder = ann_folder\n",
        "        self.ann_file = ann_file\n",
        "        self.transforms = transforms\n",
        "        self.return_masks = return_masks\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ann_info = self.coco['annotations'][idx] if \"annotations\" in self.coco else self.coco['images'][idx]\n",
        "        img_path = Path(self.img_folder) / ann_info['file_name'].replace('.png', '.jpg')\n",
        "        ann_path = Path(self.ann_folder) / ann_info['file_name']\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        w, h = img.size\n",
        "        if \"segments_info\" in ann_info:\n",
        "            masks = np.asarray(Image.open(ann_path), dtype=np.uint32)\n",
        "            masks = rgb2id(masks)\n",
        "\n",
        "            ids = np.array([ann['id'] for ann in ann_info['segments_info']])\n",
        "            masks = masks == ids[:, None, None]\n",
        "\n",
        "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "            labels = torch.tensor([ann['category_id'] for ann in ann_info['segments_info']], dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target['image_id'] = torch.tensor([ann_info['image_id'] if \"image_id\" in ann_info else ann_info[\"id\"]])\n",
        "        if self.return_masks:\n",
        "            target['masks'] = masks\n",
        "        target['labels'] = labels\n",
        "\n",
        "        target[\"boxes\"] = masks_to_boxes(masks)\n",
        "\n",
        "        target['size'] = torch.as_tensor([int(h), int(w)])\n",
        "        target['orig_size'] = torch.as_tensor([int(h), int(w)])\n",
        "        if \"segments_info\" in ann_info:\n",
        "            for name in ['iscrowd', 'area']:\n",
        "                target[name] = torch.tensor([ann[name] for ann in ann_info['segments_info']])\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.coco['images'])\n",
        "\n",
        "    def get_height_and_width(self, idx):\n",
        "        img_info = self.coco['images'][idx]\n",
        "        height = img_info['height']\n",
        "        width = img_info['width']\n",
        "        return height, width\n",
        "\n",
        "\n",
        "def build_coco_panoptic(image_set, args):\n",
        "    img_folder_root = Path(args.coco_path)\n",
        "    ann_folder_root = Path(args.coco_panoptic_path)\n",
        "    assert img_folder_root.exists(), f'provided COCO path {img_folder_root} does not exist'\n",
        "    assert ann_folder_root.exists(), f'provided COCO path {ann_folder_root} does not exist'\n",
        "    mode = 'panoptic'\n",
        "    PATHS = {\n",
        "        \"train\": (\"train\", Path(\"annotations\") / f'{mode}_train.coco.json'),\n",
        "        \"val\": (\"valid\", Path(\"annotations\") / f'{mode}_validation.coco.json'),\n",
        "    }\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    img_folder_path = img_folder_root / img_folder\n",
        "    ann_folder = ann_folder_root / f'{mode}_{img_folder}'\n",
        "    ann_file = ann_folder_root / ann_file\n",
        "\n",
        "    dataset = CocoPanoptic(img_folder_path, ann_folder, ann_file,\n",
        "                           transforms=make_coco_transforms(image_set), return_masks=args.masks)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "sIpvAUtOl2fx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms, return_masks):\n",
        "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
        "        self._transforms = transforms\n",
        "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = {'image_id': image_id, 'annotations': target}\n",
        "        img, target = self.prepare(img, target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def convert_coco_poly_to_mask(segmentations, height, width):\n",
        "    masks = []\n",
        "    for polygons in segmentations:\n",
        "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
        "        mask = coco_mask.decode(rles)\n",
        "        if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "        mask = mask.any(dim=2)\n",
        "        masks.append(mask)\n",
        "    if masks:\n",
        "        masks = torch.stack(masks, dim=0)\n",
        "    else:\n",
        "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "    return masks\n",
        "\n",
        "\n",
        "class ConvertCocoPolysToMask(object):\n",
        "    def __init__(self, return_masks=False):\n",
        "        self.return_masks = return_masks\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "        image_id = torch.tensor([image_id])\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
        "\n",
        "        boxes = [obj[\"bbox\"] for obj in anno]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64)\n",
        "\n",
        "        if self.return_masks:\n",
        "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
        "            masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
        "\n",
        "        keypoints = None\n",
        "        if anno and \"keypoints\" in anno[0]:\n",
        "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
        "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
        "            num_keypoints = keypoints.shape[0]\n",
        "            if num_keypoints:\n",
        "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        if self.return_masks:\n",
        "            masks = masks[keep]\n",
        "        if keypoints is not None:\n",
        "            keypoints = keypoints[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        if self.return_masks:\n",
        "            target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        if keypoints is not None:\n",
        "            target[\"keypoints\"] = keypoints\n",
        "\n",
        "        # for conversion to coco api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
        "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
        "        target[\"area\"] = area[keep]\n",
        "        target[\"iscrowd\"] = iscrowd[keep]\n",
        "\n",
        "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
        "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def make_coco_transforms(image_set):\n",
        "\n",
        "    normalize = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
        "\n",
        "    if image_set == 'train':\n",
        "        return Compose([\n",
        "            RandomHorizontalFlip(),\n",
        "            # RandomSelect(\n",
        "            #     RandomResize(scales, max_size=1333),\n",
        "            #     Compose([\n",
        "            #         RandomResize([400, 500, 600]),\n",
        "            #         RandomResize(scales, max_size=1333),\n",
        "            #     ])\n",
        "            # ),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    if image_set == 'target':\n",
        "        return Compose([\n",
        "            RandomHorizontalFlip(),\n",
        "            # RandomSelect(\n",
        "            #     RandomResize(scales, max_size=1333),\n",
        "            #     Compose([\n",
        "            #         RandomResize([400, 500, 600]),\n",
        "            #         RandomResize(scales, max_size=1333),\n",
        "            #     ])\n",
        "            # ),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    if image_set == 'val':\n",
        "        return Compose([\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    raise ValueError(f'unknown {image_set}')\n",
        "\n",
        "\n",
        "def build_coco(image_set, args):\n",
        "    root = Path(args.coco_path)\n",
        "    assert root.exists(), f'provided COCO path {root} does not exist'\n",
        "    mode = 'instances'\n",
        "    PATHS = {\n",
        "        \"train\": (root / \"train\", root / \"annotations\" / f'{mode}_train.coco.json'),\n",
        "        \"target\": (root / \"target\", root / \"annotations\" / f'{mode}_target.coco.json'),\n",
        "        \"val\": (root / \"valid\", root / \"annotations\" / f'{mode}_validation.coco.json'),\n",
        "    }\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), return_masks=args.masks)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "9EDouCpOtaKZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_coco_api_from_dataset(dataset):\n",
        "    for _ in range(10):\n",
        "        # if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        #     break\n",
        "        if isinstance(dataset, torch.utils.data.Subset):\n",
        "            dataset = dataset.dataset\n",
        "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        return dataset.coco\n",
        "\n",
        "\n",
        "def build_dataset(image_set, args):\n",
        "    if args.dataset_file == 'coco':\n",
        "        return build_coco(image_set, args)\n",
        "    if args.dataset_file == 'coco_panoptic':\n",
        "        # to avoid making panopticapi required for coco\n",
        "        # from .coco_panoptic import build as build_coco_panoptic\n",
        "        return build_coco_panoptic(image_set, args)\n",
        "    raise ValueError(f'dataset {args.dataset_file} not supported')"
      ],
      "metadata": {
        "id": "v02HZoMxkgib"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoEvaluator(object):\n",
        "    def __init__(self, coco_gt, iou_types):\n",
        "        assert isinstance(iou_types, (list, tuple))\n",
        "        coco_gt = copy.deepcopy(coco_gt)\n",
        "        self.coco_gt = coco_gt\n",
        "\n",
        "        self.iou_types = iou_types\n",
        "        self.coco_eval = {}\n",
        "        for iou_type in iou_types:\n",
        "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
        "\n",
        "        self.img_ids = []\n",
        "        self.eval_imgs = {k: [] for k in iou_types}\n",
        "\n",
        "    def update(self, predictions):\n",
        "        img_ids = list(np.unique(list(predictions.keys())))\n",
        "        self.img_ids.extend(img_ids)\n",
        "\n",
        "        for iou_type in self.iou_types:\n",
        "            results = self.prepare(predictions, iou_type)\n",
        "\n",
        "            # suppress pycocotools prints\n",
        "            with open(os.devnull, 'w') as devnull:\n",
        "                with contextlib.redirect_stdout(devnull):\n",
        "                    coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()\n",
        "            coco_eval = self.coco_eval[iou_type]\n",
        "\n",
        "            coco_eval.cocoDt = coco_dt\n",
        "            coco_eval.params.imgIds = list(img_ids)\n",
        "            img_ids, eval_imgs = _evaluate(coco_eval)\n",
        "\n",
        "            self.eval_imgs[iou_type].append(eval_imgs)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for iou_type in self.iou_types:\n",
        "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
        "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
        "\n",
        "    def accumulate(self):\n",
        "        for coco_eval in self.coco_eval.values():\n",
        "            coco_eval.accumulate()\n",
        "\n",
        "    def summarize(self):\n",
        "        for iou_type, coco_eval in self.coco_eval.items():\n",
        "            print(\"IoU metric: {}\".format(iou_type))\n",
        "            coco_eval.summarize()\n",
        "\n",
        "    def prepare(self, predictions, iou_type):\n",
        "        if iou_type == \"bbox\":\n",
        "            return self.prepare_for_coco_detection(predictions)\n",
        "        elif iou_type == \"segm\":\n",
        "            return self.prepare_for_coco_segmentation(predictions)\n",
        "        elif iou_type == \"keypoints\":\n",
        "            return self.prepare_for_coco_keypoint(predictions)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n",
        "\n",
        "    def prepare_for_coco_detection(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"bbox\": box,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, box in enumerate(boxes)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_segmentation(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            scores = prediction[\"scores\"]\n",
        "            labels = prediction[\"labels\"]\n",
        "            masks = prediction[\"masks\"]\n",
        "\n",
        "            masks = masks > 0.5\n",
        "\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            rles = [\n",
        "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0]\n",
        "                for mask in masks\n",
        "            ]\n",
        "            for rle in rles:\n",
        "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"segmentation\": rle,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, rle in enumerate(rles)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_keypoint(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "            keypoints = prediction[\"keypoints\"]\n",
        "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        'keypoints': keypoint,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, keypoint in enumerate(keypoints)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "\n",
        "def merge(img_ids, eval_imgs):\n",
        "    all_img_ids = all_gather(img_ids)\n",
        "    all_eval_imgs = all_gather(eval_imgs)\n",
        "\n",
        "    merged_img_ids = []\n",
        "    for p in all_img_ids:\n",
        "        merged_img_ids.extend(p)\n",
        "\n",
        "    merged_eval_imgs = []\n",
        "    for p in all_eval_imgs:\n",
        "        merged_eval_imgs.append(p)\n",
        "\n",
        "    merged_img_ids = np.array(merged_img_ids)\n",
        "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
        "\n",
        "    # keep only unique (and in sorted order) images\n",
        "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
        "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
        "\n",
        "    return merged_img_ids, merged_eval_imgs\n",
        "\n",
        "\n",
        "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
        "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
        "    img_ids = list(img_ids)\n",
        "    eval_imgs = list(eval_imgs.flatten())\n",
        "\n",
        "    coco_eval.evalImgs = eval_imgs\n",
        "    coco_eval.params.imgIds = img_ids\n",
        "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
        "\n",
        "\n",
        "#################################################################\n",
        "# From pycocotools, just removed the prints and fixed\n",
        "# a Python3 bug about unicode not defined\n",
        "#################################################################\n",
        "\n",
        "\n",
        "def _evaluate(self):\n",
        "    '''\n",
        "    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
        "    :return: None\n",
        "    '''\n",
        "    # tic = time.time()\n",
        "    # print('Running per image evaluation...')\n",
        "    p = self.params\n",
        "    # add backward compatibility if useSegm is specified in params\n",
        "    if p.useSegm is not None:\n",
        "        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
        "        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
        "    # print('Evaluate annotation type *{}*'.format(p.iouType))\n",
        "    p.imgIds = list(np.unique(p.imgIds))\n",
        "    if p.useCats:\n",
        "        p.catIds = list(np.unique(p.catIds))\n",
        "    p.maxDets = sorted(p.maxDets)\n",
        "    self.params = p\n",
        "\n",
        "    self._prepare()\n",
        "    # loop through images, area range, max detection number\n",
        "    catIds = p.catIds if p.useCats else [-1]\n",
        "\n",
        "    if p.iouType == 'segm' or p.iouType == 'bbox':\n",
        "        computeIoU = self.computeIoU\n",
        "    elif p.iouType == 'keypoints':\n",
        "        computeIoU = self.computeOks\n",
        "    self.ious = {\n",
        "        (imgId, catId): computeIoU(imgId, catId)\n",
        "        for imgId in p.imgIds\n",
        "        for catId in catIds}\n",
        "\n",
        "    evaluateImg = self.evaluateImg\n",
        "    maxDet = p.maxDets[-1]\n",
        "    evalImgs = [\n",
        "        evaluateImg(imgId, catId, areaRng, maxDet)\n",
        "        for catId in catIds\n",
        "        for areaRng in p.areaRng\n",
        "        for imgId in p.imgIds\n",
        "    ]\n",
        "    # this is NOT in the pycocotools code, but could be done outside\n",
        "    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n",
        "    self._paramsEval = copy.deepcopy(self.params)\n",
        "    # toc = time.time()\n",
        "    # print('DONE (t={:0.2f}s).'.format(toc-tic))\n",
        "    return p.imgIds, evalImgs\n",
        "\n",
        "#################################################################\n",
        "# end of straight copy from pycocotools, just removing the prints\n",
        "#################################################################"
      ],
      "metadata": {
        "id": "a2K6mtEoohni"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from evaluation import pq_compute\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "\n",
        "class PanopticEvaluator(object):\n",
        "    def __init__(self, ann_file, ann_folder, output_dir=\"panoptic_eval\"):\n",
        "        self.gt_json = ann_file\n",
        "        self.gt_folder = ann_folder\n",
        "        if is_main_process():\n",
        "            if not os.path.exists(output_dir):\n",
        "                os.mkdir(output_dir)\n",
        "        self.output_dir = output_dir\n",
        "        self.predictions = []\n",
        "\n",
        "    def update(self, predictions):\n",
        "        for p in predictions:\n",
        "            with open(os.path.join(self.output_dir, p[\"file_name\"]), \"wb\") as f:\n",
        "                f.write(p.pop(\"png_string\"))\n",
        "\n",
        "        self.predictions += predictions\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        all_predictions = all_gather(self.predictions)\n",
        "        merged_predictions = []\n",
        "        for p in all_predictions:\n",
        "            merged_predictions += p\n",
        "        self.predictions = merged_predictions\n",
        "\n",
        "    def summarize(self):\n",
        "        if is_main_process():\n",
        "            json_data = {\"annotations\": self.predictions}\n",
        "            predictions_json = os.path.join(self.output_dir, \"predictions.json\")\n",
        "            with open(predictions_json, \"w\") as f:\n",
        "                f.write(json.dumps(json_data))\n",
        "            return pq_compute(self.gt_json, predictions_json, gt_folder=self.gt_folder, pred_folder=self.output_dir)\n",
        "        return None"
      ],
      "metadata": {
        "id": "XnnXPGQno-wp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module, data_loader: Iterable,\n",
        "                    data_loader1: Iterable, optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device, epoch: int, max_norm: float = 0):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 10\n",
        "    j=0\n",
        "    loss_fn_domain = torch.nn.NLLLoss()\n",
        "\n",
        "    target_dataloader_list=[]\n",
        "\n",
        "    for samples1, targets1 in data_loader1:\n",
        "        target_dataloader_list.append(samples1)\n",
        "        target_dataloader_list.append(targets1)\n",
        "\n",
        "    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        samples = samples.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        samples1 = target_dataloader_list[2*j]\n",
        "        targets1 = target_dataloader_list[2*j+1]\n",
        "        samples1 = samples1.to(device)\n",
        "        targets1 = [{k: v.to(device) for k, v in t.items()} for t in targets1]\n",
        "\n",
        "        outputs, outputs_domain = model(samples)\n",
        "        # print(outputs_domain)\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        print(\"loss_dict\", loss_dict)\n",
        "        weight_dict = criterion.weight_dict\n",
        "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "        \n",
        "        # for k in loss_dict.keys():\n",
        "        #     if k in weight_dict and k == 'domain_loss':\n",
        "        domain_predictions = outputs_domain\n",
        "        domain_predictions = domain_predictions.to(device)\n",
        "\n",
        "\n",
        "        outputs1, outputs_domain1 = model(samples1)\n",
        "        # weight_dict1 = criterion.weight_dict\n",
        "        # losses1 = sum(loss_dict1[k] * weight_dict1[k] for k in loss_dict1.keys() if k in weight_dict1 and k != 'domain_loss')\n",
        "        \n",
        "        # for k in loss_dict.keys():\n",
        "        #     if k in weight_dict and k == 'domain_loss':\n",
        "        domain_predictions1 = outputs_domain1\n",
        "        domain_predictions1 = domain_predictions1.to(device)\n",
        "\n",
        "        y_s_domain = torch.zeros(2, dtype=torch.long) # generate source domain labels\n",
        "        y_t_domain1 = torch.ones(2, dtype=torch.long) # generate target domain labels\n",
        "\n",
        "        y_s_domain=y_s_domain.to(device)\n",
        "        y_t_domain1=y_t_domain1.to(device)\n",
        "\n",
        "        loss_s_domain = loss_fn_domain(domain_predictions, y_s_domain)\n",
        "        loss_t_domain = loss_fn_domain(domain_predictions1, y_t_domain1)\n",
        "\n",
        "\n",
        "        losses = losses + loss_s_domain + loss_t_domain\n",
        "\n",
        "        print(\"total loss with DAN is: \", losses)\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
        "\n",
        "        loss_value = losses_reduced_scaled.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        if max_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
        "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "        j+=1\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, output_dir):\n",
        "    model.eval()\n",
        "    criterion.eval()\n",
        "\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Test:'\n",
        "\n",
        "    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
        "    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n",
        "    # coco_evaluator.coco_eval[iou_types[0]].params.iouThrs = [0, 0.1, 0.5, 0.75]\n",
        "\n",
        "    panoptic_evaluator = None\n",
        "    if 'panoptic' in postprocessors.keys():\n",
        "        panoptic_evaluator = PanopticEvaluator(\n",
        "            data_loader.dataset.ann_file,\n",
        "            data_loader.dataset.ann_folder,\n",
        "            output_dir=os.path.join(output_dir, \"panoptic_eval\"),\n",
        "        )\n",
        "\n",
        "    for samples, targets in metric_logger.log_every(data_loader, 10, header):\n",
        "        samples = samples.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        outputs, _ = model(samples)\n",
        "        print(outputs)\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
        "                             **loss_dict_reduced_scaled,\n",
        "                             **loss_dict_reduced_unscaled)\n",
        "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "\n",
        "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
        "        results = postprocessors['bbox'](outputs, orig_target_sizes)\n",
        "        if 'segm' in postprocessors.keys():\n",
        "            target_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n",
        "            results = postprocessors['segm'](results, outputs, orig_target_sizes, target_sizes)\n",
        "        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
        "        if coco_evaluator is not None:\n",
        "            coco_evaluator.update(res)\n",
        "\n",
        "        if panoptic_evaluator is not None:\n",
        "            res_pano = postprocessors[\"panoptic\"](outputs, target_sizes, orig_target_sizes)\n",
        "            for i, target in enumerate(targets):\n",
        "                image_id = target[\"image_id\"].item()\n",
        "                file_name = f\"{image_id:012d}.png\"\n",
        "                res_pano[i][\"image_id\"] = image_id\n",
        "                res_pano[i][\"file_name\"] = file_name\n",
        "\n",
        "            panoptic_evaluator.update(res_pano)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    if coco_evaluator is not None:\n",
        "        coco_evaluator.synchronize_between_processes()\n",
        "    if panoptic_evaluator is not None:\n",
        "        panoptic_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    if coco_evaluator is not None:\n",
        "        coco_evaluator.accumulate()\n",
        "        coco_evaluator.summarize()\n",
        "    panoptic_res = None\n",
        "    if panoptic_evaluator is not None:\n",
        "        panoptic_res = panoptic_evaluator.summarize()\n",
        "    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "    if coco_evaluator is not None:\n",
        "        if 'bbox' in postprocessors.keys():\n",
        "            stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n",
        "        if 'segm' in postprocessors.keys():\n",
        "            stats['coco_eval_masks'] = coco_evaluator.coco_eval['segm'].stats.tolist()\n",
        "    if panoptic_res is not None:\n",
        "        stats['PQ_all'] = panoptic_res[\"All\"]\n",
        "        stats['PQ_th'] = panoptic_res[\"Things\"]\n",
        "        stats['PQ_st'] = panoptic_res[\"Stuff\"]\n",
        "    return stats, coco_evaluator"
      ],
      "metadata": {
        "id": "bYNMLy8jpUIX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_args_parser():\n",
        "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
        "    parser.add_argument('-f')\n",
        "    parser.add_argument('--lr', default=1e-4, type=float)\n",
        "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
        "    parser.add_argument('--batch_size', default=2, type=int)\n",
        "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
        "    parser.add_argument('--epochs', default=300, type=int)\n",
        "    parser.add_argument('--lr_drop', default=200, type=int)\n",
        "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
        "                        help='gradient clipping max norm')\n",
        "\n",
        "    # Model parameters\n",
        "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
        "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
        "    # * Backbone\n",
        "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
        "                        help=\"Name of the convolutional backbone to use\")\n",
        "    parser.add_argument('--dilation', action='store_true',\n",
        "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
        "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
        "                        help=\"Type of positional embedding to use on top of the image features\")\n",
        "\n",
        "    # * Transformer\n",
        "    parser.add_argument('--enc_layers', default=6, type=int,\n",
        "                        help=\"Number of encoding layers in the transformer\")\n",
        "    parser.add_argument('--dec_layers', default=6, type=int,\n",
        "                        help=\"Number of decoding layers in the transformer\")\n",
        "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
        "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
        "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
        "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
        "    parser.add_argument('--dropout', default=0.1, type=float,\n",
        "                        help=\"Dropout applied in the transformer\")\n",
        "    parser.add_argument('--nheads', default=8, type=int,\n",
        "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
        "    parser.add_argument('--num_queries', default=100, type=int,\n",
        "                        help=\"Number of query slots\")\n",
        "    parser.add_argument('--pre_norm', action='store_true')\n",
        "\n",
        "    # * Segmentation\n",
        "    parser.add_argument('--masks', action='store_true',\n",
        "                        help=\"Train segmentation head if the flag is provided\")\n",
        "\n",
        "    # Loss\n",
        "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
        "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
        "    # * Matcher\n",
        "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
        "                        help=\"Class coefficient in the matching cost\")\n",
        "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
        "                        help=\"L1 box coefficient in the matching cost\")\n",
        "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
        "                        help=\"giou box coefficient in the matching cost\")\n",
        "    # * Loss coefficients\n",
        "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
        "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
        "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
        "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
        "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
        "                        help=\"Relative classification weight of the no-object class\")\n",
        "\n",
        "    # dataset parameters\n",
        "    parser.add_argument('--dataset_file', default='coco')\n",
        "    parser.add_argument('--coco_path', default='/content/data', type=str)\n",
        "    parser.add_argument('--coco_panoptic_path', type=str)\n",
        "    parser.add_argument('--remove_difficult', action='store_true')\n",
        "\n",
        "    parser.add_argument('--output_dir', default='',\n",
        "                        help='path where to save, empty for no saving')\n",
        "    parser.add_argument('--device', default='cuda',\n",
        "                        help='device to use for training / testing')\n",
        "    parser.add_argument('--seed', default=42, type=int)\n",
        "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
        "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
        "                        help='start epoch')\n",
        "    parser.add_argument('--eval', action='store_true')\n",
        "    parser.add_argument('--num_workers', default=2, type=int)\n",
        "\n",
        "    # distributed training parameters\n",
        "    parser.add_argument('--world_size', default=1, type=int,\n",
        "                        help='number of distributed processes')\n",
        "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
        "    return parser\n",
        "\n",
        "def main_train(args):\n",
        "    init_distributed_mode(args)\n",
        "    # print(\"git:\\n  {}\\n\".format(get_sha()))\n",
        "\n",
        "    if args.frozen_weights is not None:\n",
        "        assert args.masks, \"Frozen training is meant for segmentation only\"\n",
        "    print(args)\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    # fix the seed for reproducibility\n",
        "    seed = args.seed + get_rank()\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    model, criterion, postprocessors = build_model(args)\n",
        "    model.to(device)\n",
        "\n",
        "    model_without_ddp = model\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "        model_without_ddp = model.module\n",
        "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('number of params:', n_parameters)\n",
        "\n",
        "    param_dicts = [\n",
        "        {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "        {\n",
        "            \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "            \"lr\": args.lr_backbone,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
        "                                  weight_decay=args.weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
        "\n",
        "    dataset_train = build_dataset(image_set='train', args=args)\n",
        "    dataset_target = build_dataset(image_set='target', args=args)\n",
        "    dataset_val = build_dataset(image_set='val', args=args)\n",
        "\n",
        "    if args.distributed:\n",
        "        sampler_train = DistributedSampler(dataset_train)\n",
        "        sampler_target= DistributedSampler(dataset_target)\n",
        "        sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
        "    else:\n",
        "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
        "        sampler_target = torch.utils.data.RandomSampler(dataset_target)\n",
        "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
        "\n",
        "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
        "        sampler_train, args.batch_size, drop_last=True) \n",
        "    batch_sampler_target = torch.utils.data.BatchSampler(\n",
        "        sampler_target, args.batch_size, drop_last=True) \n",
        "\n",
        "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
        "                                   collate_fn=collate_fn, num_workers=args.num_workers)\n",
        "    data_loader_target = DataLoader(dataset_target, batch_sampler=batch_sampler_target,\n",
        "                                   collate_fn=collate_fn, num_workers=args.num_workers)\n",
        "    data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
        "                                 drop_last=False, collate_fn=collate_fn, num_workers=args.num_workers)\n",
        "\n",
        "    if args.dataset_file == \"coco_panoptic\":\n",
        "        # We also evaluate AP during panoptic training, on original coco DS\n",
        "        coco_val = build_coco(\"val\", args)\n",
        "        base_ds = get_coco_api_from_dataset(coco_val)\n",
        "    else:\n",
        "        base_ds = get_coco_api_from_dataset(dataset_val)\n",
        "\n",
        "    if args.frozen_weights is not None:\n",
        "        checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
        "        model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
        "\n",
        "    output_dir = Path(args.output_dir)\n",
        "    if args.resume:\n",
        "        if args.resume.startswith('https'):\n",
        "            checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                args.resume, map_location='cpu', check_hash=True)\n",
        "        else:\n",
        "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "        if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "            args.start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    if args.eval:\n",
        "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
        "                                              data_loader_val, base_ds, device, args.output_dir)\n",
        "        if args.output_dir:\n",
        "            save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
        "        return\n",
        "\n",
        "    print(\"Start training\")\n",
        "    start_time = time.time()\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        if args.distributed:\n",
        "            sampler_train.set_epoch(epoch)\n",
        "        train_stats = train_one_epoch(\n",
        "            model, criterion, data_loader_train, data_loader_target, optimizer, device, epoch,\n",
        "            args.clip_max_norm)\n",
        "        lr_scheduler.step()\n",
        "        if args.output_dir:\n",
        "            checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
        "            # extra checkpoint before LR drop and every 100 epochs\n",
        "            if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 100 == 0:\n",
        "                checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
        "            for checkpoint_path in checkpoint_paths:\n",
        "                save_on_master({\n",
        "                    'model': model_without_ddp.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'args': args,\n",
        "                }, checkpoint_path)\n",
        "\n",
        "        test_stats, coco_evaluator = evaluate(\n",
        "            model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
        "        )\n",
        "\n",
        "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                     **{f'test_{k}': v for k, v in test_stats.items()},\n",
        "                     'epoch': epoch,\n",
        "                     'n_parameters': n_parameters}\n",
        "\n",
        "        if args.output_dir and is_main_process():\n",
        "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
        "                f.write(json.dumps(log_stats) + \"\\n\")\n",
        "\n",
        "            # for evaluation logs\n",
        "            if coco_evaluator is not None:\n",
        "                (output_dir / 'eval').mkdir(exist_ok=True)\n",
        "                if \"bbox\" in coco_evaluator.coco_eval:\n",
        "                    filenames = ['latest.pth']\n",
        "                    if epoch % 50 == 0:\n",
        "                        filenames.append(f'{epoch:03}.pth')\n",
        "                    for name in filenames:\n",
        "                        torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
        "                                   output_dir / \"eval\" / name)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Training time {}'.format(total_time_str))"
      ],
      "metadata": {
        "id": "_B1ZqWzSubXV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])\n",
        "args = parser.parse_args()\n",
        "if args.output_dir:\n",
        "  Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "main_train(args)"
      ],
      "metadata": {
        "id": "fFbD4iyPufcF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fff45d45db71477eb29270887a00a8f5",
            "d46e631b69224e26ba94f5f4c42ecf3f",
            "5ddabd2fb142436eafc142fcb2009d17",
            "d7c5c3337701443e896c7d821e792dd1",
            "89d5cff8787e4bdda889b0a1fa3b034f",
            "cf35a05bda92403584cdce1d121e53dd",
            "98ef13bbe39947f48eb0049644d3ffa5",
            "4734ea87158c4a94a50dcc1415422bf3",
            "683f7831b234426eb99e627acc6cafa2",
            "0e9c219f4406455aa78a2f2f9f9f0cb7",
            "f3b0d1a621274016bb51fe46dcbef48b"
          ]
        },
        "outputId": "3d871f72-dd3b-4099-cd41-5fbe37f89bea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Namespace(aux_loss=True, backbone='resnet50', batch_size=2, bbox_loss_coef=5, clip_max_norm=0.1, coco_panoptic_path=None, coco_path='/content/data', dataset_file='coco', dec_layers=6, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dropout=0.1, enc_layers=6, eos_coef=0.1, epochs=300, eval=False, f='/root/.local/share/jupyter/runtime/kernel-3a92c131-e783-4fed-beb8-66043a0b5c7b.json', frozen_weights=None, giou_loss_coef=2, hidden_dim=256, lr=0.0001, lr_backbone=1e-05, lr_drop=200, mask_loss_coef=1, masks=False, nheads=8, num_queries=100, num_workers=2, output_dir='', position_embedding='sine', pre_norm=False, remove_difficult=False, resume='', seed=42, set_cost_bbox=5, set_cost_class=1, set_cost_giou=2, start_epoch=0, weight_decay=0.0001, world_size=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fff45d45db71477eb29270887a00a8f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of params: 43944457\n",
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.8075,  1.4889, -1.5083,  ..., -1.6730, -0.3693,  2.4858],\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6730, -0.3693,  2.4858],\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6730, -0.3693,  2.4858],\n",
            "         ...,\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6730, -0.3693,  2.4858],\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6730, -0.3693,  2.4858],\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6730, -0.3693,  2.4858]],\n",
            "\n",
            "        [[-5.8076,  1.4889, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8076,  1.4889, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8076,  1.4889, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         ...,\n",
            "         [-5.8076,  1.4889, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8076,  1.4889, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8076,  1.4889, -1.5083,  ..., -1.6731, -0.3694,  2.4858]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618]],\n",
            "\n",
            "        [[0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.3316,  1.6069, -1.6137,  ..., -1.4580,  0.0146,  2.5886],\n",
            "         [-5.3316,  1.6069, -1.6137,  ..., -1.4580,  0.0146,  2.5886],\n",
            "         [-5.3316,  1.6069, -1.6137,  ..., -1.4580,  0.0146,  2.5886],\n",
            "         ...,\n",
            "         [-5.3316,  1.6069, -1.6137,  ..., -1.4580,  0.0146,  2.5886],\n",
            "         [-5.3316,  1.6069, -1.6137,  ..., -1.4580,  0.0146,  2.5886],\n",
            "         [-5.3316,  1.6069, -1.6137,  ..., -1.4580,  0.0146,  2.5886]],\n",
            "\n",
            "        [[-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         ...,\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787]],\n",
            "\n",
            "        [[0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.9433,  1.7290, -1.3442,  ..., -1.4307,  0.1043,  2.7423],\n",
            "         [-5.9433,  1.7290, -1.3442,  ..., -1.4307,  0.1043,  2.7423],\n",
            "         [-5.9433,  1.7290, -1.3442,  ..., -1.4307,  0.1043,  2.7423],\n",
            "         ...,\n",
            "         [-5.9433,  1.7290, -1.3442,  ..., -1.4307,  0.1043,  2.7423],\n",
            "         [-5.9433,  1.7290, -1.3442,  ..., -1.4307,  0.1043,  2.7423],\n",
            "         [-5.9433,  1.7290, -1.3442,  ..., -1.4307,  0.1043,  2.7423]],\n",
            "\n",
            "        [[-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         ...,\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834]],\n",
            "\n",
            "        [[0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.6035,  1.4693, -2.0537,  ..., -1.8769, -0.2304,  2.6400],\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400],\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8769, -0.2304,  2.6400],\n",
            "         ...,\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8769, -0.2304,  2.6400],\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400],\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8769, -0.2304,  2.6400]],\n",
            "\n",
            "        [[-5.6036,  1.4693, -2.0537,  ..., -1.8768, -0.2305,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0537,  ..., -1.8768, -0.2305,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0537,  ..., -1.8768, -0.2305,  2.6401],\n",
            "         ...,\n",
            "         [-5.6036,  1.4693, -2.0537,  ..., -1.8768, -0.2305,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0537,  ..., -1.8768, -0.2305,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0537,  ..., -1.8768, -0.2305,  2.6401]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742]],\n",
            "\n",
            "        [[0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742]]], device='cuda:0')}]}\n",
            "{'pred_logits': tensor([[[-5.4704,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4704,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4704,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         ...,\n",
            "         [-5.4704,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4704,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4704,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917]],\n",
            "\n",
            "        [[-5.4705,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         ...,\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776]],\n",
            "\n",
            "        [[0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776]]], device='cuda:0'), 'aux_outputs': [{'pred_logits': tensor([[[-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         ...,\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170]],\n",
            "\n",
            "        [[-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         ...,\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584]],\n",
            "\n",
            "        [[0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.8076,  1.4890, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8076,  1.4890, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8076,  1.4890, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         ...,\n",
            "         [-5.8076,  1.4890, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8076,  1.4890, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8076,  1.4890, -1.5083,  ..., -1.6731, -0.3694,  2.4858]],\n",
            "\n",
            "        [[-5.8075,  1.4890, -1.5083,  ..., -1.6730, -0.3694,  2.4858],\n",
            "         [-5.8075,  1.4890, -1.5083,  ..., -1.6730, -0.3694,  2.4858],\n",
            "         [-5.8075,  1.4890, -1.5083,  ..., -1.6730, -0.3694,  2.4858],\n",
            "         ...,\n",
            "         [-5.8075,  1.4890, -1.5083,  ..., -1.6730, -0.3694,  2.4858],\n",
            "         [-5.8075,  1.4890, -1.5083,  ..., -1.6730, -0.3694,  2.4858],\n",
            "         [-5.8075,  1.4890, -1.5083,  ..., -1.6730, -0.3694,  2.4858]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618]],\n",
            "\n",
            "        [[0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         ...,\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886]],\n",
            "\n",
            "        [[-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         ...,\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787]],\n",
            "\n",
            "        [[0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1044,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1044,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1044,  2.7423],\n",
            "         ...,\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1044,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1044,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1044,  2.7423]],\n",
            "\n",
            "        [[-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         ...,\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834]],\n",
            "\n",
            "        [[0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.6036,  1.4693, -2.0538,  ..., -1.8767, -0.2304,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8767, -0.2304,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8767, -0.2304,  2.6401],\n",
            "         ...,\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8767, -0.2304,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8767, -0.2304,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8767, -0.2304,  2.6401]],\n",
            "\n",
            "        [[-5.6036,  1.4693, -2.0538,  ..., -1.8767, -0.2304,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8767, -0.2304,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8767, -0.2304,  2.6401],\n",
            "         ...,\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8767, -0.2304,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8767, -0.2304,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8767, -0.2304,  2.6401]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742]],\n",
            "\n",
            "        [[0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742]]], device='cuda:0')}]}\n",
            "{'pred_logits': tensor([[[-5.4704,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4704,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4704,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         ...,\n",
            "         [-5.4704,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4704,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4704,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917]],\n",
            "\n",
            "        [[-5.4705,  1.6022, -1.5717,  ..., -1.6560, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6560, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6560, -0.2252,  2.7917],\n",
            "         ...,\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6560, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6560, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6560, -0.2252,  2.7917]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776]],\n",
            "\n",
            "        [[0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776]]], device='cuda:0'), 'aux_outputs': [{'pred_logits': tensor([[[-5.5680,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5680,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5680,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         ...,\n",
            "         [-5.5680,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5680,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5680,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170]],\n",
            "\n",
            "        [[-5.5680,  1.9251, -1.8473,  ..., -2.1409, -0.2441,  3.1171],\n",
            "         [-5.5680,  1.9251, -1.8473,  ..., -2.1409, -0.2441,  3.1171],\n",
            "         [-5.5680,  1.9251, -1.8473,  ..., -2.1409, -0.2441,  3.1171],\n",
            "         ...,\n",
            "         [-5.5680,  1.9251, -1.8473,  ..., -2.1409, -0.2441,  3.1171],\n",
            "         [-5.5680,  1.9251, -1.8473,  ..., -2.1409, -0.2441,  3.1171],\n",
            "         [-5.5680,  1.9251, -1.8473,  ..., -2.1409, -0.2441,  3.1171]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584]],\n",
            "\n",
            "        [[0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.8075,  1.4890, -1.5083,  ..., -1.6730, -0.3694,  2.4858],\n",
            "         [-5.8075,  1.4890, -1.5083,  ..., -1.6730, -0.3694,  2.4858],\n",
            "         [-5.8075,  1.4890, -1.5083,  ..., -1.6730, -0.3694,  2.4858],\n",
            "         ...,\n",
            "         [-5.8075,  1.4890, -1.5083,  ..., -1.6730, -0.3694,  2.4858],\n",
            "         [-5.8075,  1.4890, -1.5083,  ..., -1.6730, -0.3694,  2.4858],\n",
            "         [-5.8075,  1.4890, -1.5083,  ..., -1.6730, -0.3694,  2.4858]],\n",
            "\n",
            "        [[-5.8075,  1.4889, -1.5083,  ..., -1.6730, -0.3693,  2.4858],\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6730, -0.3693,  2.4858],\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6730, -0.3693,  2.4858],\n",
            "         ...,\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6730, -0.3693,  2.4858],\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6730, -0.3693,  2.4858],\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6730, -0.3693,  2.4858]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4142, 0.0259, 0.0618]],\n",
            "\n",
            "        [[0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.3318,  1.6069, -1.6138,  ..., -1.4579,  0.0145,  2.5886],\n",
            "         [-5.3318,  1.6069, -1.6138,  ..., -1.4579,  0.0145,  2.5886],\n",
            "         [-5.3318,  1.6069, -1.6138,  ..., -1.4579,  0.0145,  2.5886],\n",
            "         ...,\n",
            "         [-5.3318,  1.6069, -1.6138,  ..., -1.4579,  0.0145,  2.5886],\n",
            "         [-5.3318,  1.6069, -1.6138,  ..., -1.4579,  0.0145,  2.5886],\n",
            "         [-5.3318,  1.6069, -1.6138,  ..., -1.4579,  0.0145,  2.5886]],\n",
            "\n",
            "        [[-5.3316,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         [-5.3316,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         [-5.3316,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         ...,\n",
            "         [-5.3316,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         [-5.3316,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886],\n",
            "         [-5.3316,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5886]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787]],\n",
            "\n",
            "        [[0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.9434,  1.7290, -1.3443,  ..., -1.4305,  0.1044,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4305,  0.1044,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4305,  0.1044,  2.7423],\n",
            "         ...,\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4305,  0.1044,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4305,  0.1044,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4305,  0.1044,  2.7423]],\n",
            "\n",
            "        [[-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         ...,\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834]],\n",
            "\n",
            "        [[0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.6036,  1.4693, -2.0538,  ..., -1.8766, -0.2304,  2.6402],\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8766, -0.2304,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8766, -0.2304,  2.6401],\n",
            "         ...,\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8766, -0.2304,  2.6402],\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8766, -0.2304,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0538,  ..., -1.8766, -0.2304,  2.6401]],\n",
            "\n",
            "        [[-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400],\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400],\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400],\n",
            "         ...,\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400],\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400],\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742]],\n",
            "\n",
            "        [[0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742]]], device='cuda:0')}]}\n",
            "{'pred_logits': tensor([[[-5.4705,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         ...,\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6559, -0.2252,  2.7917]],\n",
            "\n",
            "        [[-5.4705,  1.6022, -1.5717,  ..., -1.6560, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6560, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6560, -0.2252,  2.7917],\n",
            "         ...,\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6560, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6560, -0.2252,  2.7917],\n",
            "         [-5.4705,  1.6022, -1.5717,  ..., -1.6560, -0.2252,  2.7917]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776]],\n",
            "\n",
            "        [[0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776],\n",
            "         [0.7928, 0.4262, 0.0368, 0.0776]]], device='cuda:0'), 'aux_outputs': [{'pred_logits': tensor([[[-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         ...,\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2442,  3.1170]],\n",
            "\n",
            "        [[-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2441,  3.1171],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2441,  3.1171],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2441,  3.1171],\n",
            "         ...,\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2441,  3.1171],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2441,  3.1171],\n",
            "         [-5.5681,  1.9251, -1.8473,  ..., -2.1409, -0.2441,  3.1171]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584]],\n",
            "\n",
            "        [[0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584],\n",
            "         [0.7635, 0.4166, 0.0258, 0.0584]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.8076,  1.4890, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8076,  1.4890, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8076,  1.4890, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         ...,\n",
            "         [-5.8076,  1.4890, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8076,  1.4890, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8076,  1.4890, -1.5083,  ..., -1.6731, -0.3694,  2.4858]],\n",
            "\n",
            "        [[-5.8075,  1.4889, -1.5083,  ..., -1.6731, -0.3694,  2.4858],\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6731, -0.3693,  2.4858],\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6731, -0.3693,  2.4858],\n",
            "         ...,\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6731, -0.3693,  2.4858],\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6731, -0.3693,  2.4858],\n",
            "         [-5.8075,  1.4889, -1.5083,  ..., -1.6731, -0.3693,  2.4858]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618]],\n",
            "\n",
            "        [[0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618],\n",
            "         [0.6463, 0.4143, 0.0259, 0.0618]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         ...,\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         [-5.3318,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887]],\n",
            "\n",
            "        [[-5.3317,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         [-5.3317,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         [-5.3317,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         ...,\n",
            "         [-5.3317,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         [-5.3317,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887],\n",
            "         [-5.3317,  1.6069, -1.6137,  ..., -1.4580,  0.0145,  2.5887]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787]],\n",
            "\n",
            "        [[0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787],\n",
            "         [0.6860, 0.4251, 0.0371, 0.0787]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         ...,\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3443,  ..., -1.4306,  0.1043,  2.7423]],\n",
            "\n",
            "        [[-5.9434,  1.7290, -1.3442,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3442,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3442,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         ...,\n",
            "         [-5.9434,  1.7290, -1.3442,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3442,  ..., -1.4306,  0.1043,  2.7423],\n",
            "         [-5.9434,  1.7290, -1.3442,  ..., -1.4306,  0.1043,  2.7423]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834]],\n",
            "\n",
            "        [[0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834],\n",
            "         [0.5784, 0.4281, 0.0399, 0.0834]]], device='cuda:0')}, {'pred_logits': tensor([[[-5.6036,  1.4693, -2.0537,  ..., -1.8767, -0.2305,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0537,  ..., -1.8767, -0.2305,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0537,  ..., -1.8767, -0.2305,  2.6401],\n",
            "         ...,\n",
            "         [-5.6036,  1.4693, -2.0537,  ..., -1.8767, -0.2304,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0537,  ..., -1.8767, -0.2304,  2.6401],\n",
            "         [-5.6036,  1.4693, -2.0537,  ..., -1.8767, -0.2304,  2.6401]],\n",
            "\n",
            "        [[-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400],\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400],\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400],\n",
            "         ...,\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400],\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400],\n",
            "         [-5.6035,  1.4693, -2.0537,  ..., -1.8768, -0.2304,  2.6400]]],\n",
            "       device='cuda:0'), 'pred_boxes': tensor([[[0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742]],\n",
            "\n",
            "        [[0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742],\n",
            "         [0.5903, 0.4291, 0.0349, 0.0742]]], device='cuda:0')}]}\n",
            "Test:  [24/25]  eta: 0:00:00  class_error: 100.00  loss: 39.8045 (40.6456)  loss_ce: 1.4845 (1.5983)  loss_bbox: 2.1217 (2.1845)  loss_giou: 3.5488 (3.5298)  loss_ce_0: 1.5477 (1.6469)  loss_bbox_0: 1.9645 (2.0366)  loss_giou_0: 3.5608 (3.5639)  loss_ce_1: 1.4234 (1.5565)  loss_bbox_1: 1.5754 (1.6417)  loss_giou_1: 3.3916 (3.3941)  loss_ce_2: 1.4030 (1.5509)  loss_bbox_2: 1.7232 (1.7978)  loss_giou_2: 3.3716 (3.3882)  loss_ce_3: 1.3819 (1.5715)  loss_bbox_3: 1.5041 (1.5443)  loss_giou_3: 3.1220 (3.2084)  loss_ce_4: 1.4185 (1.6085)  loss_bbox_4: 1.5154 (1.5507)  loss_giou_4: 3.2056 (3.2729)  loss_ce_unscaled: 1.4845 (1.5983)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4243 (0.4369)  loss_giou_unscaled: 1.7744 (1.7649)  cardinality_error_unscaled: 21.0000 (21.2000)  loss_ce_0_unscaled: 1.5477 (1.6469)  loss_bbox_0_unscaled: 0.3929 (0.4073)  loss_giou_0_unscaled: 1.7804 (1.7819)  cardinality_error_0_unscaled: 21.0000 (21.2000)  loss_ce_1_unscaled: 1.4234 (1.5565)  loss_bbox_1_unscaled: 0.3151 (0.3283)  loss_giou_1_unscaled: 1.6958 (1.6970)  cardinality_error_1_unscaled: 21.0000 (21.2000)  loss_ce_2_unscaled: 1.4030 (1.5509)  loss_bbox_2_unscaled: 0.3446 (0.3596)  loss_giou_2_unscaled: 1.6858 (1.6941)  cardinality_error_2_unscaled: 21.0000 (21.2000)  loss_ce_3_unscaled: 1.3819 (1.5715)  loss_bbox_3_unscaled: 0.3008 (0.3089)  loss_giou_3_unscaled: 1.5610 (1.6042)  cardinality_error_3_unscaled: 79.0000 (78.8000)  loss_ce_4_unscaled: 1.4185 (1.6085)  loss_bbox_4_unscaled: 0.3031 (0.3101)  loss_giou_4_unscaled: 1.6028 (1.6364)  cardinality_error_4_unscaled: 21.0000 (21.2000)  time: 0.2805  data: 0.0100  max mem: 1509\n",
            "Test: Total time: 0:00:07 (0.2833 s / it)\n",
            "Averaged stats: class_error: 100.00  loss: 39.8045 (40.6456)  loss_ce: 1.4845 (1.5983)  loss_bbox: 2.1217 (2.1845)  loss_giou: 3.5488 (3.5298)  loss_ce_0: 1.5477 (1.6469)  loss_bbox_0: 1.9645 (2.0366)  loss_giou_0: 3.5608 (3.5639)  loss_ce_1: 1.4234 (1.5565)  loss_bbox_1: 1.5754 (1.6417)  loss_giou_1: 3.3916 (3.3941)  loss_ce_2: 1.4030 (1.5509)  loss_bbox_2: 1.7232 (1.7978)  loss_giou_2: 3.3716 (3.3882)  loss_ce_3: 1.3819 (1.5715)  loss_bbox_3: 1.5041 (1.5443)  loss_giou_3: 3.1220 (3.2084)  loss_ce_4: 1.4185 (1.6085)  loss_bbox_4: 1.5154 (1.5507)  loss_giou_4: 3.2056 (3.2729)  loss_ce_unscaled: 1.4845 (1.5983)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.4243 (0.4369)  loss_giou_unscaled: 1.7744 (1.7649)  cardinality_error_unscaled: 21.0000 (21.2000)  loss_ce_0_unscaled: 1.5477 (1.6469)  loss_bbox_0_unscaled: 0.3929 (0.4073)  loss_giou_0_unscaled: 1.7804 (1.7819)  cardinality_error_0_unscaled: 21.0000 (21.2000)  loss_ce_1_unscaled: 1.4234 (1.5565)  loss_bbox_1_unscaled: 0.3151 (0.3283)  loss_giou_1_unscaled: 1.6958 (1.6970)  cardinality_error_1_unscaled: 21.0000 (21.2000)  loss_ce_2_unscaled: 1.4030 (1.5509)  loss_bbox_2_unscaled: 0.3446 (0.3596)  loss_giou_2_unscaled: 1.6858 (1.6941)  cardinality_error_2_unscaled: 21.0000 (21.2000)  loss_ce_3_unscaled: 1.3819 (1.5715)  loss_bbox_3_unscaled: 0.3008 (0.3089)  loss_giou_3_unscaled: 1.5610 (1.6042)  cardinality_error_3_unscaled: 79.0000 (78.8000)  loss_ce_4_unscaled: 1.4185 (1.6085)  loss_bbox_4_unscaled: 0.3031 (0.3101)  loss_giou_4_unscaled: 1.6028 (1.6364)  cardinality_error_4_unscaled: 21.0000 (21.2000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "loss_dict {'loss_ce': tensor(1.3091, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(78.5714, device='cuda:0'), 'loss_bbox': tensor(0.1548, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9007, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(18.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3746, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1690, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8988, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(6.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4028, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1380, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8142, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(18., device='cuda:0'), 'loss_ce_2': tensor(1.4194, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1237, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8097, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(5., device='cuda:0'), 'loss_ce_3': tensor(1.3588, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1401, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8982, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(18.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4729, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1220, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7950, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(21.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.1897, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [14]  [  0/100]  eta: 0:00:49  lr: 0.000100  class_error: 78.57  loss: 22.8094 (22.8094)  loss_ce: 1.3091 (1.3091)  loss_bbox: 0.7742 (0.7742)  loss_giou: 1.8013 (1.8013)  loss_ce_0: 1.3746 (1.3746)  loss_bbox_0: 0.8451 (0.8451)  loss_giou_0: 1.7976 (1.7976)  loss_ce_1: 1.4028 (1.4028)  loss_bbox_1: 0.6900 (0.6900)  loss_giou_1: 1.6285 (1.6285)  loss_ce_2: 1.4194 (1.4194)  loss_bbox_2: 0.6184 (0.6184)  loss_giou_2: 1.6194 (1.6194)  loss_ce_3: 1.3588 (1.3588)  loss_bbox_3: 0.7007 (0.7007)  loss_giou_3: 1.7964 (1.7964)  loss_ce_4: 1.4729 (1.4729)  loss_bbox_4: 0.6101 (0.6101)  loss_giou_4: 1.5901 (1.5901)  loss_ce_unscaled: 1.3091 (1.3091)  class_error_unscaled: 78.5714 (78.5714)  loss_bbox_unscaled: 0.1548 (0.1548)  loss_giou_unscaled: 0.9007 (0.9007)  cardinality_error_unscaled: 18.5000 (18.5000)  loss_ce_0_unscaled: 1.3746 (1.3746)  loss_bbox_0_unscaled: 0.1690 (0.1690)  loss_giou_0_unscaled: 0.8988 (0.8988)  cardinality_error_0_unscaled: 6.5000 (6.5000)  loss_ce_1_unscaled: 1.4028 (1.4028)  loss_bbox_1_unscaled: 0.1380 (0.1380)  loss_giou_1_unscaled: 0.8142 (0.8142)  cardinality_error_1_unscaled: 18.0000 (18.0000)  loss_ce_2_unscaled: 1.4194 (1.4194)  loss_bbox_2_unscaled: 0.1237 (0.1237)  loss_giou_2_unscaled: 0.8097 (0.8097)  cardinality_error_2_unscaled: 5.0000 (5.0000)  loss_ce_3_unscaled: 1.3588 (1.3588)  loss_bbox_3_unscaled: 0.1401 (0.1401)  loss_giou_3_unscaled: 0.8982 (0.8982)  cardinality_error_3_unscaled: 18.5000 (18.5000)  loss_ce_4_unscaled: 1.4729 (1.4729)  loss_bbox_4_unscaled: 0.1220 (0.1220)  loss_giou_4_unscaled: 0.7950 (0.7950)  cardinality_error_4_unscaled: 21.5000 (21.5000)  time: 0.4954  data: 0.1560  max mem: 1509\n",
            "loss_dict {'loss_ce': tensor(0.8517, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(40., device='cuda:0'), 'loss_bbox': tensor(0.1458, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7270, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(19., device='cuda:0'), 'loss_ce_0': tensor(0.8978, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1630, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8400, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(12.5000, device='cuda:0'), 'loss_ce_1': tensor(0.9966, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1436, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.6971, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(31., device='cuda:0'), 'loss_ce_2': tensor(0.9532, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1341, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.6963, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(20., device='cuda:0'), 'loss_ce_3': tensor(0.9285, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1357, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7420, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(21.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9290, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1233, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7249, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(21.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(20.0275, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1181, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(54., device='cuda:0'), 'loss_bbox': tensor(0.2318, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2386, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(7., device='cuda:0'), 'loss_ce_0': tensor(1.1849, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2292, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1459, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(6., device='cuda:0'), 'loss_ce_1': tensor(1.2840, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1912, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9824, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(15., device='cuda:0'), 'loss_ce_2': tensor(1.2106, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2228, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1250, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(3.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1815, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2392, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2093, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(6.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1591, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2208, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1448, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(4., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.8933, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.7784, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60., device='cuda:0'), 'loss_bbox': tensor(0.1073, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0672, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(34., device='cuda:0'), 'loss_ce_0': tensor(0.9486, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1395, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9588, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(25.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0851, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1217, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9316, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(47.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0379, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1127, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9789, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(37., device='cuda:0'), 'loss_ce_3': tensor(0.8600, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1098, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0859, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(28., device='cuda:0'), 'loss_ce_4': tensor(0.8312, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0966, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9693, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(31.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.3506, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9111, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(32.2581, device='cuda:0'), 'loss_bbox': tensor(0.1392, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0251, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(22.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9802, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1237, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9231, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(9.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0920, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1163, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8272, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(39., device='cuda:0'), 'loss_ce_2': tensor(1.0811, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1319, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9274, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(18.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9482, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1297, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9815, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(18.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9573, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1344, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9732, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(17., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.5612, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.6945, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(0., device='cuda:0'), 'loss_bbox': tensor(0.1425, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9015, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(31.5000, device='cuda:0'), 'loss_ce_0': tensor(0.8736, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1380, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7585, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(24.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0466, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0996, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.6481, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(50.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9448, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0914, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.6133, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(35.5000, device='cuda:0'), 'loss_ce_3': tensor(0.7393, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1103, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7138, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(30.5000, device='cuda:0'), 'loss_ce_4': tensor(0.7440, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1109, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7829, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(32., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(18.7491, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1982, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(53.3333, device='cuda:0'), 'loss_bbox': tensor(0.1511, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9291, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(23.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2802, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1591, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8809, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(14., device='cuda:0'), 'loss_ce_1': tensor(1.3838, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1811, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9757, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2978, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1560, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8916, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(21.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2895, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1512, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9336, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(26.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2953, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1621, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8581, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(24., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.8746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1102, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(50., device='cuda:0'), 'loss_bbox': tensor(0.1463, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0131, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(17., device='cuda:0'), 'loss_ce_0': tensor(1.0961, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1493, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8809, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(1.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1269, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2385, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0329, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(37.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1638, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1751, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9405, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(19., device='cuda:0'), 'loss_ce_3': tensor(1.1169, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1633, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9601, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(19.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9760, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1723, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9219, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(17., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.7376, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9426, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(32.4324, device='cuda:0'), 'loss_bbox': tensor(0.1430, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9179, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(17.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1280, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1368, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9090, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(6.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1488, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1817, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8733, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(40., device='cuda:0'), 'loss_ce_2': tensor(1.1482, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1424, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8959, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(25., device='cuda:0'), 'loss_ce_3': tensor(1.0234, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1580, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9295, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(26.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9139, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1475, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9690, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(21.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.2359, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3886, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(68.6274, device='cuda:0'), 'loss_bbox': tensor(0.1290, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0540, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(11.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4801, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1049, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8874, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(6.5000, device='cuda:0'), 'loss_ce_1': tensor(1.6336, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1155, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8337, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(37., device='cuda:0'), 'loss_ce_2': tensor(1.5307, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0989, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8192, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(22., device='cuda:0'), 'loss_ce_3': tensor(1.4836, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1000, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8255, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(21., device='cuda:0'), 'loss_ce_4': tensor(1.4962, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1170, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8836, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(13.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.3343, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.8154, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(20.8333, device='cuda:0'), 'loss_bbox': tensor(0.1028, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9527, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(27.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0004, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0893, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8463, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(20., device='cuda:0'), 'loss_ce_1': tensor(1.0548, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1287, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9127, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(54., device='cuda:0'), 'loss_ce_2': tensor(1.0034, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1028, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8454, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(33.5000, device='cuda:0'), 'loss_ce_3': tensor(0.8825, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1065, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8911, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(39.5000, device='cuda:0'), 'loss_ce_4': tensor(0.7868, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1186, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9411, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(38., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(20.9533, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [14]  [ 10/100]  eta: 0:00:25  lr: 0.000100  class_error: 20.83  loss: 21.8414 (21.7790)  loss_ce: 0.9426 (1.0107)  loss_bbox: 0.7149 (0.7243)  loss_giou: 1.9053 (1.9503)  loss_ce_0: 1.0961 (1.1131)  loss_bbox_0: 0.6977 (0.7281)  loss_giou_0: 1.7749 (1.8054)  loss_ce_1: 1.1269 (1.2050)  loss_bbox_1: 0.6900 (0.7527)  loss_giou_1: 1.7467 (1.7325)  loss_ce_2: 1.1482 (1.1628)  loss_bbox_2: 0.6594 (0.6781)  loss_giou_2: 1.7832 (1.7352)  loss_ce_3: 1.0234 (1.0738)  loss_bbox_3: 0.6783 (0.7017)  loss_giou_3: 1.8589 (1.8492)  loss_ce_4: 0.9573 (1.0511)  loss_bbox_4: 0.6163 (0.6934)  loss_giou_4: 1.8438 (1.8116)  loss_ce_unscaled: 0.9426 (1.0107)  class_error_unscaled: 50.0000 (44.5505)  loss_bbox_unscaled: 0.1430 (0.1449)  loss_giou_unscaled: 0.9527 (0.9752)  cardinality_error_unscaled: 19.0000 (20.8636)  loss_ce_0_unscaled: 1.0961 (1.1131)  loss_bbox_0_unscaled: 0.1395 (0.1456)  loss_giou_0_unscaled: 0.8874 (0.9027)  cardinality_error_0_unscaled: 9.5000 (12.0909)  loss_ce_1_unscaled: 1.1269 (1.2050)  loss_bbox_1_unscaled: 0.1380 (0.1505)  loss_giou_1_unscaled: 0.8733 (0.8663)  cardinality_error_1_unscaled: 39.0000 (38.0000)  loss_ce_2_unscaled: 1.1482 (1.1628)  loss_bbox_2_unscaled: 0.1319 (0.1356)  loss_giou_2_unscaled: 0.8916 (0.8676)  cardinality_error_2_unscaled: 21.5000 (21.8636)  loss_ce_3_unscaled: 1.0234 (1.0738)  loss_bbox_3_unscaled: 0.1357 (0.1403)  loss_giou_3_unscaled: 0.9295 (0.9246)  cardinality_error_3_unscaled: 21.5000 (23.3182)  loss_ce_4_unscaled: 0.9573 (1.0511)  loss_bbox_4_unscaled: 0.1233 (0.1387)  loss_giou_4_unscaled: 0.9219 (0.9058)  cardinality_error_4_unscaled: 21.5000 (21.9545)  time: 0.2827  data: 0.0229  max mem: 1509\n",
            "loss_dict {'loss_ce': tensor(1.1722, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(53.8462, device='cuda:0'), 'loss_bbox': tensor(0.0968, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8802, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(25., device='cuda:0'), 'loss_ce_0': tensor(1.3953, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0992, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8487, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(13.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4575, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1422, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9507, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(46.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3880, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1148, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8924, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(33.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3528, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0909, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8265, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(46., device='cuda:0'), 'loss_ce_4': tensor(1.2749, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1010, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8583, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(42., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.1581, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3628, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(42.3077, device='cuda:0'), 'loss_bbox': tensor(0.1303, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8466, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(35., device='cuda:0'), 'loss_ce_0': tensor(1.4713, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1084, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8105, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(23., device='cuda:0'), 'loss_ce_1': tensor(1.6532, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1321, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8053, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58., device='cuda:0'), 'loss_ce_2': tensor(1.5367, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1177, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8129, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(43.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4590, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1145, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8078, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(48., device='cuda:0'), 'loss_ce_4': tensor(1.4141, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1288, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8362, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(45.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.7910, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9210, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(36.8421, device='cuda:0'), 'loss_bbox': tensor(0.0864, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8588, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(32.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0981, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1038, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7502, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(37., device='cuda:0'), 'loss_ce_1': tensor(1.2449, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1342, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8323, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(65.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1191, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0892, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7363, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1127, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0965, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7518, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(52.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0232, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0890, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7637, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(48.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(20.2931, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0906, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(31.5789, device='cuda:0'), 'loss_bbox': tensor(0.1673, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1403, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(29., device='cuda:0'), 'loss_ce_0': tensor(1.2023, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1362, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9886, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(32.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3142, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1209, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8804, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(52.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3042, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1233, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8805, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(43., device='cuda:0'), 'loss_ce_3': tensor(1.2298, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1356, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9431, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(45.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0904, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1547, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1377, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(41., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.7900, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0418, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(40.6250, device='cuda:0'), 'loss_bbox': tensor(0.1190, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9975, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(36.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1480, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1016, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8716, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(33., device='cuda:0'), 'loss_ce_1': tensor(1.3073, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0999, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8910, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(55., device='cuda:0'), 'loss_ce_2': tensor(1.2103, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0978, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8796, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(48.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1409, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1063, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9082, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(48.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1313, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1578, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0286, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(45.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.9399, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1657, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(66.6667, device='cuda:0'), 'loss_bbox': tensor(0.2861, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2063, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(24.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2744, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2035, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9656, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(29., device='cuda:0'), 'loss_ce_1': tensor(1.4057, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1801, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9160, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(53., device='cuda:0'), 'loss_ce_2': tensor(1.3384, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1638, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8981, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(58.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3180, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2072, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0265, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(52., device='cuda:0'), 'loss_ce_4': tensor(1.2751, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3101, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2739, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(47.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.5134, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0158, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(36.1702, device='cuda:0'), 'loss_bbox': tensor(0.1456, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0788, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(21.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1735, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1064, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9748, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(30.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3408, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1215, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0488, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(50., device='cuda:0'), 'loss_ce_2': tensor(1.2431, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0979, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9451, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(50., device='cuda:0'), 'loss_ce_3': tensor(1.1805, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0905, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8640, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(47.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0807, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1283, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9357, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(40., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.5650, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0926, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(41.1765, device='cuda:0'), 'loss_bbox': tensor(0.2364, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4188, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(16.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1707, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1393, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1779, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(26.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3810, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1040, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0061, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48., device='cuda:0'), 'loss_ce_2': tensor(1.3389, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1100, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0206, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(53.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2298, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1020, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9754, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(56., device='cuda:0'), 'loss_ce_4': tensor(1.1772, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2220, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3885, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(40.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.3177, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4479, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(86.6667, device='cuda:0'), 'loss_bbox': tensor(0.1932, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2316, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(22.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5735, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1437, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0581, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(26.5000, device='cuda:0'), 'loss_ce_1': tensor(1.7326, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1604, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1654, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(49.5000, device='cuda:0'), 'loss_ce_2': tensor(1.7127, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1266, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9876, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(58.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6307, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1356, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0005, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(51.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6346, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1980, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2211, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(46., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.2437, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0907, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(44.8276, device='cuda:0'), 'loss_bbox': tensor(0.1311, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1042, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30., device='cuda:0'), 'loss_ce_0': tensor(1.2123, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1073, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9617, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(31.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3507, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1158, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9206, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(49., device='cuda:0'), 'loss_ce_2': tensor(1.3236, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1164, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8667, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62., device='cuda:0'), 'loss_ce_3': tensor(1.3202, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1157, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8455, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(57.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1865, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1574, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0534, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(48.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.0919, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [14]  [ 20/100]  eta: 0:00:21  lr: 0.000100  class_error: 44.83  loss: 22.1799 (22.5392)  loss_ce: 1.0906 (1.0723)  loss_bbox: 0.7123 (0.7585)  loss_giou: 2.0262 (2.0467)  loss_ce_0: 1.1707 (1.1888)  loss_bbox_0: 0.6812 (0.6789)  loss_giou_0: 1.7749 (1.8417)  loss_ce_1: 1.3073 (1.3068)  loss_bbox_1: 0.6435 (0.7064)  loss_giou_1: 1.8253 (1.8043)  loss_ce_2: 1.2106 (1.2527)  loss_bbox_2: 0.5822 (0.6308)  loss_giou_2: 1.7832 (1.7584)  loss_ce_3: 1.1805 (1.1803)  loss_bbox_3: 0.5723 (0.6520)  loss_giou_3: 1.8164 (1.8209)  loss_ce_4: 1.0904 (1.1357)  loss_bbox_4: 0.6721 (0.7554)  loss_giou_4: 1.8822 (1.9487)  loss_ce_unscaled: 1.0906 (1.0723)  class_error_unscaled: 41.1765 (46.2268)  loss_bbox_unscaled: 0.1425 (0.1517)  loss_giou_unscaled: 1.0131 (1.0233)  cardinality_error_unscaled: 23.5000 (23.9286)  loss_ce_0_unscaled: 1.1707 (1.1888)  loss_bbox_0_unscaled: 0.1362 (0.1358)  loss_giou_0_unscaled: 0.8874 (0.9208)  cardinality_error_0_unscaled: 23.0000 (19.8095)  loss_ce_1_unscaled: 1.3073 (1.3068)  loss_bbox_1_unscaled: 0.1287 (0.1413)  loss_giou_1_unscaled: 0.9127 (0.9022)  cardinality_error_1_unscaled: 48.5000 (45.0000)  loss_ce_2_unscaled: 1.2106 (1.2527)  loss_bbox_2_unscaled: 0.1164 (0.1262)  loss_giou_2_unscaled: 0.8916 (0.8792)  cardinality_error_2_unscaled: 35.5000 (35.9048)  loss_ce_3_unscaled: 1.1805 (1.1803)  loss_bbox_3_unscaled: 0.1145 (0.1304)  loss_giou_3_unscaled: 0.9082 (0.9105)  cardinality_error_3_unscaled: 39.5000 (36.2619)  loss_ce_4_unscaled: 1.0904 (1.1357)  loss_bbox_4_unscaled: 0.1344 (0.1511)  loss_giou_4_unscaled: 0.9411 (0.9743)  cardinality_error_4_unscaled: 38.0000 (32.6905)  time: 0.2627  data: 0.0092  max mem: 1509\n",
            "loss_dict {'loss_ce': tensor(1.0399, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(40.9091, device='cuda:0'), 'loss_bbox': tensor(0.1153, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9673, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30., device='cuda:0'), 'loss_ce_0': tensor(1.1487, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1169, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0884, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(22.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2794, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0932, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0067, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(49.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2322, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0947, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8769, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(58.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2276, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0935, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8738, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(59., device='cuda:0'), 'loss_ce_4': tensor(1.0933, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1121, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9864, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(42.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.1562, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.8097, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(10.8108, device='cuda:0'), 'loss_bbox': tensor(0.1367, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0526, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(29., device='cuda:0'), 'loss_ce_0': tensor(0.8947, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1050, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9525, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(26., device='cuda:0'), 'loss_ce_1': tensor(1.0411, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0775, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8248, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(46., device='cuda:0'), 'loss_ce_2': tensor(1.0617, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0795, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8436, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(65., device='cuda:0'), 'loss_ce_3': tensor(0.9578, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0748, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8143, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(55., device='cuda:0'), 'loss_ce_4': tensor(0.9082, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1387, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1107, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(43., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.3264, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6276, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(56.2500, device='cuda:0'), 'loss_bbox': tensor(0.1727, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0919, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(46.5000, device='cuda:0'), 'loss_ce_0': tensor(1.6449, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1724, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1222, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(29.5000, device='cuda:0'), 'loss_ce_1': tensor(1.7579, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1110, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9613, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58.5000, device='cuda:0'), 'loss_ce_2': tensor(1.7728, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0855, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7729, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(71.5000, device='cuda:0'), 'loss_ce_3': tensor(1.8025, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0852, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7304, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(63.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6478, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1763, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0928, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(54., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.2195, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3982, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(56.2500, device='cuda:0'), 'loss_bbox': tensor(0.1922, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0053, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(42., device='cuda:0'), 'loss_ce_0': tensor(1.3685, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1730, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9963, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(29.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4829, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1446, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9230, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(50., device='cuda:0'), 'loss_ce_2': tensor(1.4740, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1268, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8611, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(68., device='cuda:0'), 'loss_ce_3': tensor(1.5216, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1475, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8765, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62., device='cuda:0'), 'loss_ce_4': tensor(1.4326, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2186, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0531, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(46., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.5286, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2734, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(77.7778, device='cuda:0'), 'loss_bbox': tensor(0.0911, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8770, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(51., device='cuda:0'), 'loss_ce_0': tensor(1.3804, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1038, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0151, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(29.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4799, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0971, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9617, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(55.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4839, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0786, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8023, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(59.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3935, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0767, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8095, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62., device='cuda:0'), 'loss_ce_4': tensor(1.3925, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1039, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0041, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(51., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.4989, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0507, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(45.4545, device='cuda:0'), 'loss_bbox': tensor(0.1405, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8556, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(53., device='cuda:0'), 'loss_ce_0': tensor(0.9615, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1531, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9324, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(33., device='cuda:0'), 'loss_ce_1': tensor(1.0871, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1449, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8928, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(62.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0535, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1263, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8163, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(69., device='cuda:0'), 'loss_ce_3': tensor(1.1383, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1180, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7715, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(66., device='cuda:0'), 'loss_ce_4': tensor(1.0262, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1572, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8677, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(53.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.1793, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3463, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(50., device='cuda:0'), 'loss_bbox': tensor(0.1340, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8372, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(45., device='cuda:0'), 'loss_ce_0': tensor(1.2047, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1304, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9297, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(29.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3384, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1291, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0483, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(59., device='cuda:0'), 'loss_ce_2': tensor(1.2951, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1145, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8537, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(65., device='cuda:0'), 'loss_ce_3': tensor(1.4094, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1335, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9285, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(68.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2592, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1205, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9318, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(48., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.1238, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3930, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(37.1429, device='cuda:0'), 'loss_bbox': tensor(0.1043, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8495, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(42.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3757, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0978, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8459, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(31., device='cuda:0'), 'loss_ce_1': tensor(1.5259, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1198, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9156, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(53., device='cuda:0'), 'loss_ce_2': tensor(1.5068, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0909, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7295, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(55.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5758, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0968, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8038, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(58.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4316, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1073, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9016, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(43., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.3789, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9376, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(17.8571, device='cuda:0'), 'loss_bbox': tensor(0.1254, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7973, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(50., device='cuda:0'), 'loss_ce_0': tensor(0.8857, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1249, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0022, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(29.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0557, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1068, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8841, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(60., device='cuda:0'), 'loss_ce_2': tensor(0.9874, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1102, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7809, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(61.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9596, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1052, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7918, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62.5000, device='cuda:0'), 'loss_ce_4': tensor(0.8851, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1356, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9392, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(45.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.0317, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2230, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(21.4286, device='cuda:0'), 'loss_bbox': tensor(0.1091, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7724, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(42., device='cuda:0'), 'loss_ce_0': tensor(1.1846, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1411, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9456, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(34.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2705, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1280, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8888, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(62., device='cuda:0'), 'loss_ce_2': tensor(1.3124, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1067, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8356, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(51.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3186, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1060, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7801, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(57.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2314, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1187, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8126, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(38.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.5536, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [14]  [ 30/100]  eta: 0:00:18  lr: 0.000100  class_error: 21.43  loss: 22.0990 (22.3978)  loss_ce: 1.0926 (1.1167)  loss_bbox: 0.6555 (0.7269)  loss_giou: 1.9345 (1.9739)  loss_ce_0: 1.2023 (1.1940)  loss_bbox_0: 0.5845 (0.6725)  loss_giou_0: 1.9234 (1.8818)  loss_ce_1: 1.3408 (1.3149)  loss_bbox_1: 0.6045 (0.6643)  loss_giou_1: 1.8320 (1.8227)  loss_ce_2: 1.3124 (1.2737)  loss_bbox_2: 0.5499 (0.5908)  loss_giou_2: 1.7074 (1.7185)  loss_ce_3: 1.3180 (1.2287)  loss_bbox_3: 0.5258 (0.6090)  loss_giou_3: 1.6531 (1.7613)  loss_ce_4: 1.1865 (1.1664)  loss_bbox_4: 0.6781 (0.7357)  loss_giou_4: 1.9729 (1.9459)  loss_ce_unscaled: 1.0926 (1.1167)  class_error_unscaled: 41.1765 (44.6659)  loss_bbox_unscaled: 0.1311 (0.1454)  loss_giou_unscaled: 0.9673 (0.9870)  cardinality_error_unscaled: 32.5000 (30.1129)  loss_ce_0_unscaled: 1.2023 (1.1940)  loss_bbox_0_unscaled: 0.1169 (0.1345)  loss_giou_0_unscaled: 0.9617 (0.9409)  cardinality_error_0_unscaled: 29.5000 (22.9194)  loss_ce_1_unscaled: 1.3408 (1.3149)  loss_bbox_1_unscaled: 0.1209 (0.1329)  loss_giou_1_unscaled: 0.9160 (0.9114)  cardinality_error_1_unscaled: 53.0000 (48.4194)  loss_ce_2_unscaled: 1.3124 (1.2737)  loss_bbox_2_unscaled: 0.1100 (0.1182)  loss_giou_2_unscaled: 0.8537 (0.8592)  cardinality_error_2_unscaled: 58.5000 (44.4839)  loss_ce_3_unscaled: 1.3180 (1.2287)  loss_bbox_3_unscaled: 0.1052 (0.1218)  loss_giou_3_unscaled: 0.8265 (0.8806)  cardinality_error_3_unscaled: 56.0000 (44.3871)  loss_ce_4_unscaled: 1.1865 (1.1664)  loss_bbox_4_unscaled: 0.1356 (0.1471)  loss_giou_4_unscaled: 0.9864 (0.9729)  cardinality_error_4_unscaled: 45.5000 (37.1452)  time: 0.2643  data: 0.0110  max mem: 1509\n",
            "loss_dict {'loss_ce': tensor(1.2069, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(40.6250, device='cuda:0'), 'loss_bbox': tensor(0.1345, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8951, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(38., device='cuda:0'), 'loss_ce_0': tensor(1.2005, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1384, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9599, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(39.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2623, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1495, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0175, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(65., device='cuda:0'), 'loss_ce_2': tensor(1.2477, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1356, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9197, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(53., device='cuda:0'), 'loss_ce_3': tensor(1.2807, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1357, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9123, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(56.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2420, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1340, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9135, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(39.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.2130, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4900, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(46.8750, device='cuda:0'), 'loss_bbox': tensor(0.1553, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8928, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(45., device='cuda:0'), 'loss_ce_0': tensor(1.5314, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1261, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8083, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(43., device='cuda:0'), 'loss_ce_1': tensor(1.6536, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1211, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7910, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(68., device='cuda:0'), 'loss_ce_2': tensor(1.5890, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1257, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7754, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6395, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1118, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7692, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(64.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5023, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1696, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9388, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(44., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.7984, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2990, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60., device='cuda:0'), 'loss_bbox': tensor(0.1722, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9754, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(38.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3612, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1384, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8517, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(36., device='cuda:0'), 'loss_ce_1': tensor(1.5442, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1205, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8282, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(64., device='cuda:0'), 'loss_ce_2': tensor(1.4526, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1235, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8119, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(53., device='cuda:0'), 'loss_ce_3': tensor(1.4275, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1128, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8132, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(51.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3227, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1430, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9636, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(37., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.3471, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0743, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(22.8571, device='cuda:0'), 'loss_bbox': tensor(0.1560, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8473, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(46.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0720, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1332, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8142, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(42.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1572, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1542, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9397, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(65., device='cuda:0'), 'loss_ce_2': tensor(1.1054, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1256, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7564, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(55., device='cuda:0'), 'loss_ce_3': tensor(1.1608, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1128, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7141, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(64., device='cuda:0'), 'loss_ce_4': tensor(1.0822, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1354, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8369, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(45.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.9630, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9408, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(20., device='cuda:0'), 'loss_bbox': tensor(0.1839, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0262, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(41.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9428, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1863, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0533, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(41.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0222, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2011, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1227, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(70.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9473, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1818, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0196, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(42., device='cuda:0'), 'loss_ce_3': tensor(0.9856, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1799, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0294, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(55., device='cuda:0'), 'loss_ce_4': tensor(0.9380, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2008, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0630, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(41.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.4698, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0599, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(65., device='cuda:0'), 'loss_bbox': tensor(0.1229, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8387, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(54.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0807, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0750, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7985, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(54., device='cuda:0'), 'loss_ce_1': tensor(1.0925, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1544, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9954, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(79.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0211, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1021, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7696, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(55.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0885, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1315, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8483, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(68.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1200, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1002, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8347, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(53., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.4795, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3436, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(71.4286, device='cuda:0'), 'loss_bbox': tensor(0.1548, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8120, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(39.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3470, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1388, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7270, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(33., device='cuda:0'), 'loss_ce_1': tensor(1.4617, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1199, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7755, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(66., device='cuda:0'), 'loss_ce_2': tensor(1.4208, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1415, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7635, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(49., device='cuda:0'), 'loss_ce_3': tensor(1.4345, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1162, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7208, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(51.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4019, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1322, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7988, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(41., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.0078, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9133, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(22.7273, device='cuda:0'), 'loss_bbox': tensor(0.1254, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7976, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(45.5000, device='cuda:0'), 'loss_ce_0': tensor(0.8854, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1667, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8937, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(39., device='cuda:0'), 'loss_ce_1': tensor(0.9322, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1486, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8818, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(76.5000, device='cuda:0'), 'loss_ce_2': tensor(0.8891, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1363, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8742, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(55., device='cuda:0'), 'loss_ce_3': tensor(0.9752, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1340, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8164, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(66., device='cuda:0'), 'loss_ce_4': tensor(1.0018, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1313, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8363, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(46., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.3992, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9603, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(18.9655, device='cuda:0'), 'loss_bbox': tensor(0.1121, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8663, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(31.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0856, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1256, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9275, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(23., device='cuda:0'), 'loss_ce_1': tensor(1.0651, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1857, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0094, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(63.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0417, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1107, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8734, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(38.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9911, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1151, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8955, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(44., device='cuda:0'), 'loss_ce_4': tensor(1.0937, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1102, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8681, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(33., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.3188, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2781, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(62.5000, device='cuda:0'), 'loss_bbox': tensor(0.1061, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7581, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(63., device='cuda:0'), 'loss_ce_0': tensor(1.2717, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1100, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7965, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(44.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3727, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1041, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7482, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(82.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2996, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1226, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9041, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(57.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3182, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1029, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7665, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(70.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2511, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0982, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7653, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(58., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.8802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [14]  [ 40/100]  eta: 0:00:16  lr: 0.000100  class_error: 62.50  loss: 21.6219 (22.2250)  loss_ce: 1.2069 (1.1265)  loss_bbox: 0.6699 (0.7232)  loss_giou: 1.7112 (1.9173)  loss_ce_0: 1.1846 (1.1900)  loss_bbox_0: 0.6519 (0.6717)  loss_giou_0: 1.8593 (1.8438)  loss_ce_1: 1.2705 (1.3006)  loss_bbox_1: 0.6054 (0.6802)  loss_giou_1: 1.8312 (1.8225)  loss_ce_2: 1.2477 (1.2561)  loss_bbox_2: 0.5727 (0.6059)  loss_giou_2: 1.6326 (1.7124)  loss_ce_3: 1.2807 (1.2291)  loss_bbox_3: 0.5641 (0.6132)  loss_giou_3: 1.6190 (1.7359)  loss_ce_4: 1.2314 (1.1735)  loss_bbox_4: 0.6612 (0.7215)  loss_giou_4: 1.8270 (1.9015)  loss_ce_unscaled: 1.2069 (1.1265)  class_error_unscaled: 40.9091 (44.2835)  loss_bbox_unscaled: 0.1340 (0.1446)  loss_giou_unscaled: 0.8556 (0.9587)  cardinality_error_unscaled: 42.5000 (33.5854)  loss_ce_0_unscaled: 1.1846 (1.1900)  loss_bbox_0_unscaled: 0.1304 (0.1343)  loss_giou_0_unscaled: 0.9297 (0.9219)  cardinality_error_0_unscaled: 33.0000 (26.9878)  loss_ce_1_unscaled: 1.2705 (1.3006)  loss_bbox_1_unscaled: 0.1211 (0.1360)  loss_giou_1_unscaled: 0.9156 (0.9113)  cardinality_error_1_unscaled: 62.5000 (53.6951)  loss_ce_2_unscaled: 1.2477 (1.2561)  loss_bbox_2_unscaled: 0.1145 (0.1212)  loss_giou_2_unscaled: 0.8163 (0.8562)  cardinality_error_2_unscaled: 55.5000 (46.3415)  loss_ce_3_unscaled: 1.2807 (1.2291)  loss_bbox_3_unscaled: 0.1128 (0.1226)  loss_giou_3_unscaled: 0.8095 (0.8679)  cardinality_error_3_unscaled: 62.0000 (48.0000)  loss_ce_4_unscaled: 1.2314 (1.1735)  loss_bbox_4_unscaled: 0.1322 (0.1443)  loss_giou_4_unscaled: 0.9135 (0.9507)  cardinality_error_4_unscaled: 44.0000 (38.7805)  time: 0.2641  data: 0.0111  max mem: 1509\n",
            "loss_dict {'loss_ce': tensor(1.1814, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(52.9412, device='cuda:0'), 'loss_bbox': tensor(0.1146, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7452, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(43.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0906, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1161, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7113, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(32., device='cuda:0'), 'loss_ce_1': tensor(1.1346, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1499, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8184, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(70.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0969, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1076, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7297, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(44., device='cuda:0'), 'loss_ce_3': tensor(1.1056, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1024, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7131, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(54., device='cuda:0'), 'loss_ce_4': tensor(1.1982, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1108, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7292, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(43., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(20.6033, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5578, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(79.4118, device='cuda:0'), 'loss_bbox': tensor(0.1309, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8321, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(45., device='cuda:0'), 'loss_ce_0': tensor(1.4221, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1315, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8618, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(33., device='cuda:0'), 'loss_ce_1': tensor(1.5529, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1244, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8713, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(74., device='cuda:0'), 'loss_ce_2': tensor(1.5270, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1224, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8746, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(53.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5545, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1095, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8515, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(61.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5298, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1335, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8466, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(48., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.5815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6959, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(90.3846, device='cuda:0'), 'loss_bbox': tensor(0.1233, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9241, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(39., device='cuda:0'), 'loss_ce_0': tensor(1.5614, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1236, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9255, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(32., device='cuda:0'), 'loss_ce_1': tensor(1.6050, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1598, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9861, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(69.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6133, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1255, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9243, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(49.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6012, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1222, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8968, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(48.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6908, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1472, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9222, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(46.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.3166, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5509, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(70.4545, device='cuda:0'), 'loss_bbox': tensor(0.1057, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8602, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(43.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5454, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1074, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9110, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(23., device='cuda:0'), 'loss_ce_1': tensor(1.6181, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0939, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8133, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(64., device='cuda:0'), 'loss_ce_2': tensor(1.6304, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1558, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9985, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(36., device='cuda:0'), 'loss_ce_3': tensor(1.5042, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1085, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8611, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(49.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5417, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1009, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8291, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(47., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.7154, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4693, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(54.1667, device='cuda:0'), 'loss_bbox': tensor(0.1209, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8740, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(68., device='cuda:0'), 'loss_ce_0': tensor(1.2997, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1325, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8814, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(41., device='cuda:0'), 'loss_ce_1': tensor(1.2852, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1251, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8949, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(73.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3147, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1023, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8408, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(56.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3130, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1246, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8912, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(63.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4712, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1504, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9249, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(66.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.9830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1427, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(50., device='cuda:0'), 'loss_bbox': tensor(0.1121, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9448, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(69., device='cuda:0'), 'loss_ce_0': tensor(1.0199, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1397, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9619, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(37., device='cuda:0'), 'loss_ce_1': tensor(1.0889, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1252, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9953, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(70.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9983, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1571, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0957, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(45.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0964, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1357, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9936, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(58.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2204, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1677, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8932, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(68.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.9096, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4010, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(57.7778, device='cuda:0'), 'loss_bbox': tensor(0.1701, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0453, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(56., device='cuda:0'), 'loss_ce_0': tensor(1.2431, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1477, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9833, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(15., device='cuda:0'), 'loss_ce_1': tensor(1.1808, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1668, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0262, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(51.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1718, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1429, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9055, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(39., device='cuda:0'), 'loss_ce_3': tensor(1.1746, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1598, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9792, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(47.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3988, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2068, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0770, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(52.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.9593, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1188, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(38., device='cuda:0'), 'loss_bbox': tensor(0.1252, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8610, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(53.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0408, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1105, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8052, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(13.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0154, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1078, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7414, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(44.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9869, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1209, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8579, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(31., device='cuda:0'), 'loss_ce_3': tensor(1.0049, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1213, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9036, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(33.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1887, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1040, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7772, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(47.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.1023, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2649, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(62.0690, device='cuda:0'), 'loss_bbox': tensor(0.1981, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1402, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(67.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1428, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1603, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9831, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(29.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0822, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1751, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9138, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(61., device='cuda:0'), 'loss_ce_2': tensor(1.1069, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1718, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0682, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(38.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1228, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1513, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0509, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(45., device='cuda:0'), 'loss_ce_4': tensor(1.2339, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1781, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9413, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(65., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.7075, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3957, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(48.4848, device='cuda:0'), 'loss_bbox': tensor(0.0767, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9016, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(70.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2975, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0794, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7922, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(26., device='cuda:0'), 'loss_ce_1': tensor(1.2783, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0719, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8098, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(54., device='cuda:0'), 'loss_ce_2': tensor(1.2434, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0751, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8413, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(32., device='cuda:0'), 'loss_ce_3': tensor(1.2168, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0721, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8248, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(44.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4013, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0852, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8347, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(63., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.5294, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [14]  [ 50/100]  eta: 0:00:13  lr: 0.000100  class_error: 48.48  loss: 22.5232 (22.2677)  loss_ce: 1.2649 (1.1757)  loss_bbox: 0.6261 (0.7066)  loss_giou: 1.7326 (1.8994)  loss_ce_0: 1.2005 (1.2050)  loss_bbox_0: 0.6577 (0.6624)  loss_giou_0: 1.7236 (1.8280)  loss_ce_1: 1.1808 (1.2974)  loss_bbox_1: 0.6261 (0.6743)  loss_giou_1: 1.7636 (1.8130)  loss_ce_2: 1.1718 (1.2586)  loss_bbox_2: 0.6277 (0.6127)  loss_giou_2: 1.7469 (1.7349)  loss_ce_3: 1.1746 (1.2370)  loss_bbox_3: 0.5811 (0.6114)  loss_giou_3: 1.7030 (1.7471)  loss_ce_4: 1.2420 (1.2155)  loss_bbox_4: 0.6674 (0.7158)  loss_giou_4: 1.6931 (1.8728)  loss_ce_unscaled: 1.2649 (1.1757)  class_error_unscaled: 52.9412 (47.4375)  loss_bbox_unscaled: 0.1252 (0.1413)  loss_giou_unscaled: 0.8663 (0.9497)  cardinality_error_unscaled: 45.0000 (37.8922)  loss_ce_0_unscaled: 1.2005 (1.2050)  loss_bbox_0_unscaled: 0.1315 (0.1325)  loss_giou_0_unscaled: 0.8618 (0.9140)  cardinality_error_0_unscaled: 33.0000 (27.2255)  loss_ce_1_unscaled: 1.1808 (1.2974)  loss_bbox_1_unscaled: 0.1252 (0.1349)  loss_giou_1_unscaled: 0.8818 (0.9065)  cardinality_error_1_unscaled: 66.0000 (55.5784)  loss_ce_2_unscaled: 1.1718 (1.2586)  loss_bbox_2_unscaled: 0.1255 (0.1225)  loss_giou_2_unscaled: 0.8734 (0.8675)  cardinality_error_2_unscaled: 49.0000 (45.5980)  loss_ce_3_unscaled: 1.1746 (1.2370)  loss_bbox_3_unscaled: 0.1162 (0.1223)  loss_giou_3_unscaled: 0.8515 (0.8736)  cardinality_error_3_unscaled: 54.0000 (48.5098)  loss_ce_4_unscaled: 1.2420 (1.2155)  loss_bbox_4_unscaled: 0.1335 (0.1432)  loss_giou_4_unscaled: 0.8466 (0.9364)  cardinality_error_4_unscaled: 46.5000 (41.9118)  time: 0.2637  data: 0.0094  max mem: 1509\n",
            "loss_dict {'loss_ce': tensor(1.6974, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75., device='cuda:0'), 'loss_bbox': tensor(0.2467, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3682, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(57.5000, device='cuda:0'), 'loss_ce_0': tensor(1.7091, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1766, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1186, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(17.5000, device='cuda:0'), 'loss_ce_1': tensor(1.6765, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1521, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0270, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(42.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6960, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1540, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0936, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(28.5000, device='cuda:0'), 'loss_ce_3': tensor(1.7457, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1525, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0891, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(31., device='cuda:0'), 'loss_ce_4': tensor(1.6413, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2592, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2864, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(55., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(31.2623, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2476, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(43.3333, device='cuda:0'), 'loss_bbox': tensor(0.1037, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8842, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(73.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1777, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1011, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8315, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(23., device='cuda:0'), 'loss_ce_1': tensor(1.1283, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1155, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9300, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1543, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1089, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8890, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(41.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1500, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1018, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8045, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(50., device='cuda:0'), 'loss_ce_4': tensor(1.2865, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1146, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8433, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(70., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.1655, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2082, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(48.2759, device='cuda:0'), 'loss_bbox': tensor(0.1895, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1527, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(70.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1015, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1876, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0023, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(28., device='cuda:0'), 'loss_ce_1': tensor(1.0784, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1876, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9578, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(61., device='cuda:0'), 'loss_ce_2': tensor(1.1407, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1379, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8819, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(56., device='cuda:0'), 'loss_ce_3': tensor(1.2510, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1569, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8764, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(49.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3135, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2130, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1344, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(64.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.8569, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.9799, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(68., device='cuda:0'), 'loss_bbox': tensor(0.2043, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2357, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(62., device='cuda:0'), 'loss_ce_0': tensor(1.9231, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1336, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9778, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(24.5000, device='cuda:0'), 'loss_ce_1': tensor(1.8510, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1270, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9321, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(46., device='cuda:0'), 'loss_ce_2': tensor(1.8737, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1217, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9664, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(36., device='cuda:0'), 'loss_ce_3': tensor(1.8976, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1182, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9689, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(28.5000, device='cuda:0'), 'loss_ce_4': tensor(1.9355, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2399, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2059, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(56., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.1711, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3473, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(38.0952, device='cuda:0'), 'loss_bbox': tensor(0.1091, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8772, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(77., device='cuda:0'), 'loss_ce_0': tensor(1.3187, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0738, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8359, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(33., device='cuda:0'), 'loss_ce_1': tensor(1.2800, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0869, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8023, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(62., device='cuda:0'), 'loss_ce_2': tensor(1.2593, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0663, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7768, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(57., device='cuda:0'), 'loss_ce_3': tensor(1.2380, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0663, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7929, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(53.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4116, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1131, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9560, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(70.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.9428, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4957, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(77.7778, device='cuda:0'), 'loss_bbox': tensor(0.1808, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2083, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(64.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4567, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1763, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0013, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(37., device='cuda:0'), 'loss_ce_1': tensor(1.3668, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1467, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8996, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(53., device='cuda:0'), 'loss_ce_2': tensor(1.4015, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1460, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9080, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(66.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4263, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1293, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9036, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(39.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5033, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2011, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1877, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(56., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.1616, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4157, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(52.1739, device='cuda:0'), 'loss_bbox': tensor(0.1374, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0056, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(62., device='cuda:0'), 'loss_ce_0': tensor(1.3598, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1408, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9185, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(29., device='cuda:0'), 'loss_ce_1': tensor(1.3944, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1274, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8690, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3022, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1143, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8470, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(59., device='cuda:0'), 'loss_ce_3': tensor(1.4008, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1238, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9235, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(36.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4143, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1395, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9874, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(59.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.6872, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4450, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(74.1379, device='cuda:0'), 'loss_bbox': tensor(0.1032, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7983, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(46.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4136, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1137, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7665, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(28.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3610, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1486, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9216, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4185, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1108, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7923, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(54.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3890, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1148, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8203, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(30.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3883, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1019, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7596, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(51., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.9975, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3445, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(68.7500, device='cuda:0'), 'loss_bbox': tensor(0.3648, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9089, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(64.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3366, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3104, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8544, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(50., device='cuda:0'), 'loss_ce_1': tensor(1.3042, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3305, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8452, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(68.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3356, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3335, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8813, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(76., device='cuda:0'), 'loss_ce_3': tensor(1.2884, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3502, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9060, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(41.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3684, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3346, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8762, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(68., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.0339, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2799, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(61.9048, device='cuda:0'), 'loss_bbox': tensor(0.1255, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8777, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(54., device='cuda:0'), 'loss_ce_0': tensor(1.3121, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1104, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8074, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(45., device='cuda:0'), 'loss_ce_1': tensor(1.3257, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1447, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9003, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(63., device='cuda:0'), 'loss_ce_2': tensor(1.3361, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1010, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7962, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(72.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2116, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1098, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8828, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(36.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2686, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0877, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8202, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(59., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.6917, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [14]  [ 60/100]  eta: 0:00:10  lr: 0.000100  class_error: 61.90  loss: 23.1812 (22.6324)  loss_ce: 1.3957 (1.2201)  loss_bbox: 0.6261 (0.7355)  loss_giou: 1.8032 (1.9262)  loss_ce_0: 1.3121 (1.2388)  loss_bbox_0: 0.6577 (0.6788)  loss_giou_0: 1.7627 (1.8272)  loss_ce_1: 1.2852 (1.3104)  loss_bbox_1: 0.6371 (0.6922)  loss_giou_1: 1.7991 (1.8137)  loss_ce_2: 1.3022 (1.2805)  loss_bbox_2: 0.6084 (0.6266)  loss_giou_2: 1.7625 (1.7401)  loss_ce_3: 1.2510 (1.2637)  loss_bbox_3: 0.6063 (0.6278)  loss_giou_3: 1.7824 (1.7547)  loss_ce_4: 1.3988 (1.2544)  loss_bbox_4: 0.6974 (0.7463)  loss_giou_4: 1.7865 (1.8955)  loss_ce_unscaled: 1.3957 (1.2201)  class_error_unscaled: 57.7778 (49.6190)  loss_bbox_unscaled: 0.1252 (0.1471)  loss_giou_unscaled: 0.9016 (0.9631)  cardinality_error_unscaled: 62.0000 (42.0410)  loss_ce_0_unscaled: 1.3121 (1.2388)  loss_bbox_0_unscaled: 0.1315 (0.1358)  loss_giou_0_unscaled: 0.8814 (0.9136)  cardinality_error_0_unscaled: 29.0000 (27.9344)  loss_ce_1_unscaled: 1.2852 (1.3104)  loss_bbox_1_unscaled: 0.1274 (0.1384)  loss_giou_1_unscaled: 0.8996 (0.9068)  cardinality_error_1_unscaled: 61.0000 (55.5082)  loss_ce_2_unscaled: 1.3022 (1.2805)  loss_bbox_2_unscaled: 0.1217 (0.1253)  loss_giou_2_unscaled: 0.8813 (0.8700)  cardinality_error_2_unscaled: 45.5000 (47.0984)  loss_ce_3_unscaled: 1.2510 (1.2637)  loss_bbox_3_unscaled: 0.1213 (0.1256)  loss_giou_3_unscaled: 0.8912 (0.8774)  cardinality_error_3_unscaled: 45.0000 (47.0656)  loss_ce_4_unscaled: 1.3988 (1.2544)  loss_bbox_4_unscaled: 0.1395 (0.1493)  loss_giou_4_unscaled: 0.8932 (0.9477)  cardinality_error_4_unscaled: 56.0000 (45.0328)  time: 0.2646  data: 0.0099  max mem: 1509\n",
            "loss_dict {'loss_ce': tensor(1.1476, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(15.1515, device='cuda:0'), 'loss_bbox': tensor(0.1108, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8753, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(46., device='cuda:0'), 'loss_ce_0': tensor(1.3076, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0930, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.6769, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(47., device='cuda:0'), 'loss_ce_1': tensor(1.3026, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1126, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7412, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(55.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2624, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0831, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.6991, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(74., device='cuda:0'), 'loss_ce_3': tensor(1.1852, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1101, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9294, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(40., device='cuda:0'), 'loss_ce_4': tensor(1.2476, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1076, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8988, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(49., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.5755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0856, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(27.2727, device='cuda:0'), 'loss_bbox': tensor(0.0814, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7517, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(59.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2481, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0926, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7566, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(45., device='cuda:0'), 'loss_ce_1': tensor(1.1796, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0784, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.6773, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58., device='cuda:0'), 'loss_ce_2': tensor(1.2013, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0761, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7213, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(73., device='cuda:0'), 'loss_ce_3': tensor(1.1109, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0766, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7187, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(38., device='cuda:0'), 'loss_ce_4': tensor(1.1717, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0800, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7322, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(54.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(19.5686, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2520, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(47.9167, device='cuda:0'), 'loss_bbox': tensor(0.1324, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0477, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(37., device='cuda:0'), 'loss_ce_0': tensor(1.3563, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1364, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0238, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(38., device='cuda:0'), 'loss_ce_1': tensor(1.3186, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1285, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0158, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(39., device='cuda:0'), 'loss_ce_2': tensor(1.2896, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1320, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0411, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2616, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1338, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0504, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(29., device='cuda:0'), 'loss_ce_4': tensor(1.2543, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1135, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9982, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(44., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.3647, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9271, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(13.4615, device='cuda:0'), 'loss_bbox': tensor(0.1556, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0501, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(31., device='cuda:0'), 'loss_ce_0': tensor(1.1274, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1419, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0585, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(40., device='cuda:0'), 'loss_ce_1': tensor(1.1461, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1213, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9478, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(42.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0558, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1382, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9983, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9792, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1518, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1167, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(19.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0034, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1301, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0557, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(36.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.2788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3638, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(39.6226, device='cuda:0'), 'loss_bbox': tensor(0.1327, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9315, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30., device='cuda:0'), 'loss_ce_0': tensor(1.3768, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1207, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8358, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(42., device='cuda:0'), 'loss_ce_1': tensor(1.4476, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1164, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8467, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(33.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4354, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1226, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9398, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3901, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1281, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9748, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(15.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3990, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1126, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9407, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(29., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.4049, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1012, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(27.5862, device='cuda:0'), 'loss_bbox': tensor(0.1395, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9993, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(47.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2693, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1142, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9304, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(44., device='cuda:0'), 'loss_ce_1': tensor(1.2386, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1111, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9024, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(36., device='cuda:0'), 'loss_ce_2': tensor(1.2308, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1462, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0516, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(77.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0727, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1639, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1138, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(22., device='cuda:0'), 'loss_ce_4': tensor(1.0960, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1119, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9857, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(37., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.2900, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2167, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(69.2308, device='cuda:0'), 'loss_bbox': tensor(0.1401, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9049, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(51.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1380, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1476, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9058, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(61.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1924, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1596, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8857, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(53.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2384, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1415, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8833, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(85.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1686, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1439, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8373, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(37., device='cuda:0'), 'loss_ce_4': tensor(1.1851, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1480, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8755, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(50., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.5245, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9437, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(8.8235, device='cuda:0'), 'loss_bbox': tensor(0.2089, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1332, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(41.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0962, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1451, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0298, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(48., device='cuda:0'), 'loss_ce_1': tensor(1.1110, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1358, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9088, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(39.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0724, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1700, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1434, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(76.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9796, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2037, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2935, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(27.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9882, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1726, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1356, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(41.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.0599, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5100, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(61.2903, device='cuda:0'), 'loss_bbox': tensor(0.1544, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8080, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(51., device='cuda:0'), 'loss_ce_0': tensor(1.4393, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1468, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8647, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(39., device='cuda:0'), 'loss_ce_1': tensor(1.4863, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1496, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8297, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48., device='cuda:0'), 'loss_ce_2': tensor(1.4674, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1491, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8218, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(80., device='cuda:0'), 'loss_ce_3': tensor(1.4221, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1486, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8908, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(38., device='cuda:0'), 'loss_ce_4': tensor(1.4336, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1360, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8567, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(48., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.7194, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9830, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(23.0769, device='cuda:0'), 'loss_bbox': tensor(0.1607, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0457, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(47.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0346, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1435, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0140, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(39.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0953, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1601, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0043, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(41.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0420, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1483, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0569, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(69.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9825, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1732, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1252, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(33., device='cuda:0'), 'loss_ce_4': tensor(0.9830, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1336, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0382, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(45., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.6950, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [14]  [ 70/100]  eta: 0:00:08  lr: 0.000100  class_error: 23.08  loss: 23.0170 (22.6067)  loss_ce: 1.2520 (1.2106)  loss_bbox: 0.6973 (0.7316)  loss_giou: 1.8630 (1.9239)  loss_ce_0: 1.3121 (1.2388)  loss_bbox_0: 0.6820 (0.6734)  loss_giou_0: 1.8116 (1.8261)  loss_ce_1: 1.3026 (1.3021)  loss_bbox_1: 0.6425 (0.6844)  loss_giou_1: 1.8005 (1.8050)  loss_ce_2: 1.2624 (1.2733)  loss_bbox_2: 0.6602 (0.6304)  loss_giou_2: 1.7665 (1.7586)  loss_ce_3: 1.2380 (1.2484)  loss_bbox_3: 0.6463 (0.6404)  loss_giou_3: 1.8121 (1.7907)  loss_ce_4: 1.2865 (1.2434)  loss_bbox_4: 0.6505 (0.7290)  loss_giou_4: 1.9120 (1.8966)  loss_ce_unscaled: 1.2520 (1.2106)  class_error_unscaled: 47.9167 (47.3267)  loss_bbox_unscaled: 0.1395 (0.1463)  loss_giou_unscaled: 0.9315 (0.9619)  cardinality_error_unscaled: 51.5000 (42.3521)  loss_ce_0_unscaled: 1.3121 (1.2388)  loss_bbox_0_unscaled: 0.1364 (0.1347)  loss_giou_0_unscaled: 0.9058 (0.9130)  cardinality_error_0_unscaled: 39.0000 (30.2535)  loss_ce_1_unscaled: 1.3026 (1.3021)  loss_bbox_1_unscaled: 0.1285 (0.1369)  loss_giou_1_unscaled: 0.9003 (0.9025)  cardinality_error_1_unscaled: 48.5000 (53.9859)  loss_ce_2_unscaled: 1.2624 (1.2733)  loss_bbox_2_unscaled: 0.1320 (0.1261)  loss_giou_2_unscaled: 0.8833 (0.8793)  cardinality_error_2_unscaled: 62.5000 (50.6549)  loss_ce_3_unscaled: 1.2380 (1.2484)  loss_bbox_3_unscaled: 0.1293 (0.1281)  loss_giou_3_unscaled: 0.9060 (0.8954)  cardinality_error_3_unscaled: 36.5000 (44.6549)  loss_ce_4_unscaled: 1.2865 (1.2434)  loss_bbox_4_unscaled: 0.1301 (0.1458)  loss_giou_4_unscaled: 0.9560 (0.9483)  cardinality_error_4_unscaled: 51.0000 (44.8099)  time: 0.2662  data: 0.0105  max mem: 1509\n",
            "loss_dict {'loss_ce': tensor(1.4262, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(73.1707, device='cuda:0'), 'loss_bbox': tensor(0.0961, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7829, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(57.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4051, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1033, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8465, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(35., device='cuda:0'), 'loss_ce_1': tensor(1.4095, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1022, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8374, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(32.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4386, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1102, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8244, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(68., device='cuda:0'), 'loss_ce_3': tensor(1.3925, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1032, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8494, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(39., device='cuda:0'), 'loss_ce_4': tensor(1.3760, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0924, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8614, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(48., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.8698, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2641, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(46.4286, device='cuda:0'), 'loss_bbox': tensor(0.1293, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8237, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(56.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3175, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1151, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8248, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(43.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2535, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1086, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7688, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48., device='cuda:0'), 'loss_ce_2': tensor(1.2938, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1570, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9217, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(77., device='cuda:0'), 'loss_ce_3': tensor(1.2515, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1478, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8435, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(50.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2653, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1375, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8450, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(56., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2137, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(44.4444, device='cuda:0'), 'loss_bbox': tensor(0.1121, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8126, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(59.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1472, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1103, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8183, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(43.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2310, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1080, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8250, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(43., device='cuda:0'), 'loss_ce_2': tensor(1.1811, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1209, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8343, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(77., device='cuda:0'), 'loss_ce_3': tensor(1.1907, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1040, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8005, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(54., device='cuda:0'), 'loss_ce_4': tensor(1.2081, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1054, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8185, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(59., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.6815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9202, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(6.8966, device='cuda:0'), 'loss_bbox': tensor(0.1608, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7863, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(60.5000, device='cuda:0'), 'loss_ce_0': tensor(0.8683, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1861, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9743, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(41.5000, device='cuda:0'), 'loss_ce_1': tensor(0.9889, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1736, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8837, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(53., device='cuda:0'), 'loss_ce_2': tensor(0.9291, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1655, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8327, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(73.5000, device='cuda:0'), 'loss_ce_3': tensor(0.8963, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1499, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7753, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(55., device='cuda:0'), 'loss_ce_4': tensor(0.9002, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1567, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8117, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(66.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.9837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4605, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(47.8261, device='cuda:0'), 'loss_bbox': tensor(0.1326, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7574, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(59., device='cuda:0'), 'loss_ce_0': tensor(1.3807, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1309, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7566, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(38.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3894, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1095, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7182, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(52.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4314, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1464, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8340, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(72.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3943, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1198, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.6923, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(57.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4221, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1242, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7949, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(65.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.8224, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.7009, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(94.8718, device='cuda:0'), 'loss_bbox': tensor(0.1553, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9604, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(53., device='cuda:0'), 'loss_ce_0': tensor(1.7197, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1440, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9643, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(39.5000, device='cuda:0'), 'loss_ce_1': tensor(1.6703, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1378, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9011, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6288, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1588, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0469, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(61.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6469, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1290, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8809, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(48.5000, device='cuda:0'), 'loss_ce_4': tensor(1.7004, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1385, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9241, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(60.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.1305, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0166, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(15.6250, device='cuda:0'), 'loss_bbox': tensor(0.1019, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8128, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(56.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0073, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0966, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8835, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(43.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0814, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1000, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8765, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0486, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0902, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7884, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(61.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9913, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1067, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8360, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(50.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9884, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0894, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7934, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(63.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(20.4289, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0603, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(34.6154, device='cuda:0'), 'loss_bbox': tensor(0.0842, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7570, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(55., device='cuda:0'), 'loss_ce_0': tensor(1.0000, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0985, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8074, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(31., device='cuda:0'), 'loss_ce_1': tensor(1.0858, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0774, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7639, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(54., device='cuda:0'), 'loss_ce_2': tensor(1.0953, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0978, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8170, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(70.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0082, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1019, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8137, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(58.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0410, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0932, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7287, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(67.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(19.8228, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3105, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(41.1765, device='cuda:0'), 'loss_bbox': tensor(0.1942, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9204, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(56.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2974, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2000, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9436, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(35.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3819, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1985, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9495, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(59., device='cuda:0'), 'loss_ce_2': tensor(1.3562, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1697, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8818, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(73., device='cuda:0'), 'loss_ce_3': tensor(1.4033, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1690, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8412, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(63., device='cuda:0'), 'loss_ce_4': tensor(1.4046, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1775, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8895, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(74.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.9570, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2066, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(39.1304, device='cuda:0'), 'loss_bbox': tensor(0.0714, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.6728, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(63., device='cuda:0'), 'loss_ce_0': tensor(1.0855, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1051, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7858, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(34.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2073, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0681, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7506, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(59.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1549, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.6449, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(70.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2008, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0836, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7409, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(64., device='cuda:0'), 'loss_ce_4': tensor(1.1921, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0814, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.6958, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(71., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(19.4654, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [14]  [ 80/100]  eta: 0:00:05  lr: 0.000100  class_error: 39.13  loss: 21.6775 (22.4239)  loss_ce: 1.2066 (1.2165)  loss_bbox: 0.6631 (0.7177)  loss_giou: 1.6474 (1.8860)  loss_ce_0: 1.2481 (1.2369)  loss_bbox_0: 0.6033 (0.6699)  loss_giou_0: 1.7293 (1.8131)  loss_ce_1: 1.2310 (1.2981)  loss_bbox_1: 0.5629 (0.6730)  loss_giou_1: 1.6933 (1.7865)  loss_ce_2: 1.2308 (1.2711)  loss_bbox_2: 0.6910 (0.6322)  loss_giou_2: 1.6686 (1.7495)  loss_ce_3: 1.1852 (1.2471)  loss_bbox_3: 0.6451 (0.6363)  loss_giou_3: 1.6870 (1.7690)  loss_ce_4: 1.1921 (1.2442)  loss_bbox_4: 0.5676 (0.7128)  loss_giou_4: 1.7228 (1.8640)  loss_ce_unscaled: 1.2066 (1.2165)  class_error_unscaled: 39.1304 (46.9677)  loss_bbox_unscaled: 0.1326 (0.1435)  loss_giou_unscaled: 0.8237 (0.9430)  cardinality_error_unscaled: 53.0000 (44.2469)  loss_ce_0_unscaled: 1.2481 (1.2369)  loss_bbox_0_unscaled: 0.1207 (0.1340)  loss_giou_0_unscaled: 0.8647 (0.9066)  cardinality_error_0_unscaled: 40.0000 (31.2840)  loss_ce_1_unscaled: 1.2310 (1.2981)  loss_bbox_1_unscaled: 0.1126 (0.1346)  loss_giou_1_unscaled: 0.8467 (0.8932)  cardinality_error_1_unscaled: 48.0000 (53.4753)  loss_ce_2_unscaled: 1.2308 (1.2711)  loss_bbox_2_unscaled: 0.1382 (0.1264)  loss_giou_2_unscaled: 0.8343 (0.8748)  cardinality_error_2_unscaled: 72.5000 (53.1049)  loss_ce_3_unscaled: 1.1852 (1.2471)  loss_bbox_3_unscaled: 0.1290 (0.1273)  loss_giou_3_unscaled: 0.8435 (0.8845)  cardinality_error_3_unscaled: 39.0000 (45.8148)  loss_ce_4_unscaled: 1.1921 (1.2442)  loss_bbox_4_unscaled: 0.1135 (0.1426)  loss_giou_4_unscaled: 0.8614 (0.9320)  cardinality_error_4_unscaled: 50.0000 (47.0802)  time: 0.2682  data: 0.0110  max mem: 1509\n",
            "loss_dict {'loss_ce': tensor(1.5534, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(77.0492, device='cuda:0'), 'loss_bbox': tensor(0.0995, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8500, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30.5000, device='cuda:0'), 'loss_ce_0': tensor(1.6680, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1298, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0000, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(15., device='cuda:0'), 'loss_ce_1': tensor(1.6424, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1213, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9986, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(37.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6067, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1050, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9374, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(44.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5809, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1208, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9203, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(45.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6105, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1041, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8664, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(51.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.6224, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1820, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(51.0204, device='cuda:0'), 'loss_bbox': tensor(0.1082, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7557, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(34.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1584, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1068, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8039, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(23., device='cuda:0'), 'loss_ce_1': tensor(1.2292, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1085, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7947, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(43.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2192, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1000, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7359, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(44.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2652, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1145, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8495, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(46., device='cuda:0'), 'loss_ce_4': tensor(1.1943, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1084, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7414, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(54.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.2082, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0865, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(38.2353, device='cuda:0'), 'loss_bbox': tensor(0.1525, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9393, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(40., device='cuda:0'), 'loss_ce_0': tensor(1.1083, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1596, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8595, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(24., device='cuda:0'), 'loss_ce_1': tensor(1.1250, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1540, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8991, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(53.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1151, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1236, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8516, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(53.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1042, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1360, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7642, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(57.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0990, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1413, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8483, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(61.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.6960, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1478, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(40., device='cuda:0'), 'loss_bbox': tensor(0.1372, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9539, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(33., device='cuda:0'), 'loss_ce_0': tensor(1.0870, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1642, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0997, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(22., device='cuda:0'), 'loss_ce_1': tensor(1.1432, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1593, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0957, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(49., device='cuda:0'), 'loss_ce_2': tensor(1.1464, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1106, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9046, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(48.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1361, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1389, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9635, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(50., device='cuda:0'), 'loss_ce_4': tensor(1.1766, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1207, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9117, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(55.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.2296, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4204, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(44.7368, device='cuda:0'), 'loss_bbox': tensor(0.1185, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8205, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(37., device='cuda:0'), 'loss_ce_0': tensor(1.3053, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1767, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0658, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(19., device='cuda:0'), 'loss_ce_1': tensor(1.4569, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1640, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0594, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4583, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1039, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8435, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(54.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4099, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1092, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8526, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(54., device='cuda:0'), 'loss_ce_4': tensor(1.3925, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1184, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8381, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(60., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.7438, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2292, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(48.9362, device='cuda:0'), 'loss_bbox': tensor(0.1091, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7374, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(27.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0553, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1620, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9161, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(18.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1964, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1700, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9885, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(49.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1814, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0981, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7825, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(41., device='cuda:0'), 'loss_ce_3': tensor(1.2111, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1100, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8122, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(43.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2162, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1043, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7358, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(48., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.1858, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1070, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(33.3333, device='cuda:0'), 'loss_bbox': tensor(0.1175, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0444, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(39.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9505, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1829, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1394, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(31., device='cuda:0'), 'loss_ce_1': tensor(1.0293, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1816, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1894, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(65., device='cuda:0'), 'loss_ce_2': tensor(1.1159, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1165, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0583, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(55.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1207, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1291, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0468, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(57., device='cuda:0'), 'loss_ce_4': tensor(1.1038, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1283, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0406, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(61.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.1509, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4562, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(76.6667, device='cuda:0'), 'loss_bbox': tensor(0.1071, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9481, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(45.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4422, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1382, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9305, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(36.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4546, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1593, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0129, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(72.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4876, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1055, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9604, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(59., device='cuda:0'), 'loss_ce_3': tensor(1.4327, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1065, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8815, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(58.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4926, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1053, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8859, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(60.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.0143, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2063, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60.8696, device='cuda:0'), 'loss_bbox': tensor(0.1261, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8297, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(43., device='cuda:0'), 'loss_ce_0': tensor(1.1442, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1489, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8840, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(39.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2249, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1667, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9450, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(77., device='cuda:0'), 'loss_ce_2': tensor(1.2266, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1213, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7952, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(65.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1858, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1348, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8249, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(56.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1940, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1259, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8422, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(62.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.9340, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-cba15418f2a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmain_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-1465182eeffc>\u001b[0m in \u001b[0;36mmain_train\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    184\u001b[0m         train_stats = train_one_epoch(\n\u001b[1;32m    185\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             args.clip_max_norm)\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-09984071e71d>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, data_loader1, optimizer, device, epoch, max_norm)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtargets1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_domain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# print(outputs_domain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-bca958c83ebc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_tensor_from_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-cd9a4adfde29>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNestedTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m        \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m        \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNestedTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m        \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-cd9a4adfde29>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNestedTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m        \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m        \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNestedTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-cd9a4adfde29>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m        \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m        \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}