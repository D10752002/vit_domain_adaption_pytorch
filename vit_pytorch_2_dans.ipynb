{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vit_pytorch_2_dans.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6Hg1BjbuHLWTX3f9c6Sl+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86b012f0401443cebf5f350a81ce5872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f54755b7af4499fab36935b55db9344",
              "IPY_MODEL_a3069b4540454ac09383a74e8b2c6141",
              "IPY_MODEL_594fa26daaa24ff5aa163e0c5bbb3abf"
            ],
            "layout": "IPY_MODEL_08fea5461ff943cba95e6fd6a9f84d37"
          }
        },
        "5f54755b7af4499fab36935b55db9344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8e638b9d50c4457ade1e4705f08de25",
            "placeholder": "​",
            "style": "IPY_MODEL_5e112300b8ba41f69d6d27b6f9ddc006",
            "value": "100%"
          }
        },
        "a3069b4540454ac09383a74e8b2c6141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6622b8755e7f4a6990b40ffaf03cdefa",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22671a2a91714824a24eaee48e40aed0",
            "value": 102530333
          }
        },
        "594fa26daaa24ff5aa163e0c5bbb3abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e9d5159214e42f380a7d52fc3556087",
            "placeholder": "​",
            "style": "IPY_MODEL_82ffce34feff4c9384d3ea3a39205df2",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 240MB/s]"
          }
        },
        "08fea5461ff943cba95e6fd6a9f84d37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8e638b9d50c4457ade1e4705f08de25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e112300b8ba41f69d6d27b6f9ddc006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6622b8755e7f4a6990b40ffaf03cdefa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22671a2a91714824a24eaee48e40aed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e9d5159214e42f380a7d52fc3556087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82ffce34feff4c9384d3ea3a39205df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D10752002/vit_domain_adaption_pytorch/blob/main/vit_pytorch_2_dans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Eo0YwUudthWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9b4b78-e9ca-4644-a711-06b7b857bc25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jUAqjadYkOM",
        "outputId": "628b31ea-19b9-4aeb-bfc2-2d6f60564c62"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 27.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 9.5 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Collecting fsspec[http]>=2021.11.1\n",
            "  Downloading fsspec-2022.7.0-py3-none-any.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 65.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 73.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, pyyaml, fsspec, xxhash, responses, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed datasets-2.4.0 fsspec-2022.7.0 huggingface-hub-0.8.1 pyyaml-6.0 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "import datasets\n",
        "import os\n",
        "import subprocess\n",
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "import pickle\n",
        "from packaging import version\n",
        "from typing import Optional, List\n",
        "import torch.distributed as dist\n",
        "from torch import Tensor\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "from typing import Dict\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torchvision.ops.boxes import box_area\n",
        "from PIL import Image\n",
        "import io\n",
        "import copy\n",
        "import torch.utils.data\n",
        "from pycocotools import mask as coco_mask\n",
        "import torchvision.transforms as T\n",
        "import sys\n",
        "from typing import Iterable\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools.coco import COCO\n",
        "import pycocotools.mask as mask_util\n",
        "import contextlib\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "import functools\n",
        "import traceback\n",
        "if version.parse(torchvision.__version__) < version.parse('0.7'):\n",
        "  from torchvision.ops import _new_empty_tensor\n",
        "  from torchvision.ops.misc import _output_size\n",
        "import PIL"
      ],
      "metadata": {
        "id": "zYFUt0znti2H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_traceback(f):\n",
        "    @functools.wraps(f)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        try:\n",
        "            return f(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            print('Caught exception in worker thread:')\n",
        "            traceback.print_exc()\n",
        "            raise e\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "class IdGenerator():\n",
        "    '''\n",
        "    The class is designed to generate unique IDs that have meaningful RGB encoding.\n",
        "    Given semantic category unique ID will be generated and its RGB encoding will\n",
        "    have color close to the predefined semantic category color.\n",
        "    The RGB encoding used is ID = R * 256 * G + 256 * 256 + B.\n",
        "    Class constructor takes dictionary {id: category_info}, where all semantic\n",
        "    class ids are presented and category_info record is a dict with fields\n",
        "    'isthing' and 'color'\n",
        "    '''\n",
        "    def __init__(self, categories):\n",
        "        self.taken_colors = set([0, 0, 0])\n",
        "        self.categories = categories\n",
        "        for category in self.categories.values():\n",
        "            if category['isthing'] == 0:\n",
        "                self.taken_colors.add(tuple(category['color']))\n",
        "\n",
        "    def get_color(self, cat_id):\n",
        "        def random_color(base, max_dist=30):\n",
        "            new_color = base + np.random.randint(low=-max_dist,\n",
        "                                                 high=max_dist+1,\n",
        "                                                 size=3)\n",
        "            return tuple(np.maximum(0, np.minimum(255, new_color)))\n",
        "\n",
        "        category = self.categories[cat_id]\n",
        "        if category['isthing'] == 0:\n",
        "            return category['color']\n",
        "        base_color_array = category['color']\n",
        "        base_color = tuple(base_color_array)\n",
        "        if base_color not in self.taken_colors:\n",
        "            self.taken_colors.add(base_color)\n",
        "            return base_color\n",
        "        else:\n",
        "            while True:\n",
        "                color = random_color(base_color_array)\n",
        "                if color not in self.taken_colors:\n",
        "                    self.taken_colors.add(color)\n",
        "                    return color\n",
        "\n",
        "    def get_id(self, cat_id):\n",
        "        color = self.get_color(cat_id)\n",
        "        return rgb2id(color)\n",
        "\n",
        "    def get_id_and_color(self, cat_id):\n",
        "        color = self.get_color(cat_id)\n",
        "        return rgb2id(color), color\n",
        "\n",
        "\n",
        "def rgb2id(color):\n",
        "    if isinstance(color, np.ndarray) and len(color.shape) == 3:\n",
        "        if color.dtype == np.uint8:\n",
        "            color = color.astype(np.int32)\n",
        "        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n",
        "    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\n",
        "\n",
        "\n",
        "def id2rgb(id_map):\n",
        "    if isinstance(id_map, np.ndarray):\n",
        "        id_map_copy = id_map.copy()\n",
        "        rgb_shape = tuple(list(id_map.shape) + [3])\n",
        "        rgb_map = np.zeros(rgb_shape, dtype=np.uint8)\n",
        "        for i in range(3):\n",
        "            rgb_map[..., i] = id_map_copy % 256\n",
        "            id_map_copy //= 256\n",
        "        return rgb_map\n",
        "    color = []\n",
        "    for _ in range(3):\n",
        "        color.append(id_map % 256)\n",
        "        id_map //= 256\n",
        "    return color\n",
        "\n",
        "\n",
        "def save_json(d, file):\n",
        "    with open(file, 'w') as f:\n",
        "        json.dump(d, f)"
      ],
      "metadata": {
        "id": "LEYJ4qCBdBbr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/detrdatasetfinal.zip' -d '/content'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUq3ABZzXu8-",
        "outputId": "6cf22230-02a0-48d8-b256-965694255592"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/detrdatasetfinal.zip\n",
            "   creating: /content/data/annotations/\n",
            "  inflating: /content/data/annotations/instances_target.coco.json  \n",
            "  inflating: /content/data/annotations/instances_train.coco.json  \n",
            "  inflating: /content/data/annotations/instances_validation.coco.json  \n",
            "   creating: /content/data/target/\n",
            "  inflating: /content/data/target/frankfurt_000000_000576_leftImg8bit_foggy_beta_0-02_png.rf.daec2de38f1f3b583876da331ba1d5a6.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_001016_leftImg8bit_foggy_beta_0-02_png.rf.17e17e93009863128b16dc5779a837ce.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_001236_leftImg8bit_foggy_beta_0-02_png.rf.3a2878e1df3ade0417deffa6d46c6ffe.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_001751_leftImg8bit_foggy_beta_0-02_png.rf.e3a5c4c5fe5ca66ee87fd11deb70ed8c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_002963_leftImg8bit_foggy_beta_0-02_png.rf.f905fcb8331be18b80184726d8a62ac5.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_003025_leftImg8bit_foggy_beta_0-02_png.rf.cee6ac9db1021dccc8369ef517a97d4c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_003357_leftImg8bit_foggy_beta_0-02_png.rf.04884122d6f47a423554e4ae9bbe9299.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_003920_leftImg8bit_foggy_beta_0-02_png.rf.2786d62cc4a0460c46d5b908dc2c6adc.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_004617_leftImg8bit_foggy_beta_0-02_png.rf.874b9b6dee83dada71c330de7f071452.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_005543_leftImg8bit_foggy_beta_0-02_png.rf.519984da0e184c3bb5ebe1f30f42dfdd.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_005898_leftImg8bit_foggy_beta_0-02_png.rf.6210218917247c01faf63abf6f454a3b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_006589_leftImg8bit_foggy_beta_0-02_png.rf.9c023ad95349d493d2c332f7bbbdb8f0.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_007365_leftImg8bit_foggy_beta_0-02_png.rf.4daba4d2404b1d882331026306dfdd0b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_008206_leftImg8bit_foggy_beta_0-02_png.rf.b7c83b36d5a936d667cb86a7d066cbdc.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_008451_leftImg8bit_foggy_beta_0-02_png.rf.e31b7091dcb970167bf0ec05c1006c81.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_009291_leftImg8bit_foggy_beta_0-02_png.rf.8b46fb5518878a1c1cf9a10f3b49772b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_009561_leftImg8bit_foggy_beta_0-02_png.rf.43786d20a272f39e5564738d406add52.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_009688_leftImg8bit_foggy_beta_0-02_png.rf.fa851876f75fb7eaf8996a064065847c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_010351_leftImg8bit_foggy_beta_0-02_png.rf.94e6d3fa4880be57cbcdd0d8e3abf309.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_010763_leftImg8bit_foggy_beta_0-02_png.rf.41412f784705087e3ed7452f41e7aaec.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_011461_leftImg8bit_foggy_beta_0-02_png.rf.7bcae4a7ba5e784de7907e6929602804.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_011810_leftImg8bit_foggy_beta_0-02_png.rf.429538f6a910f78a687f3794228efc30.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_012009_leftImg8bit_foggy_beta_0-02_png.rf.b5b179665349d1f55fcb0b7b957e1da8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_012121_leftImg8bit_foggy_beta_0-02_png.rf.873053bb0a452930a65c6c736d12ee53.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_012868_leftImg8bit_foggy_beta_0-02_png.rf.74dcba73c598db4c27ca03a250d0f607.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_013067_leftImg8bit_foggy_beta_0-02_png.rf.054945799fa652afc10f2bc0068466c8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_013382_leftImg8bit_foggy_beta_0-02_png.rf.75716cd289416f2a4f6e3af45c615ac1.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_013942_leftImg8bit_foggy_beta_0-02_png.rf.8d61171dec498f9eb4fda53abd2b243d.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_014480_leftImg8bit_foggy_beta_0-02_png.rf.98106de09d7cf99720960df33b66a126.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_015389_leftImg8bit_foggy_beta_0-02_png.rf.078a6ee1d42ad6e37035ad1bf872b03f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_016005_leftImg8bit_foggy_beta_0-02_png.rf.05eee7b91bb54dae32d17cd9730a5891.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_016286_leftImg8bit_foggy_beta_0-02_png.rf.1dacfed62f6d58034ab7ef282ed115a7.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_017228_leftImg8bit_foggy_beta_0-02_png.rf.22bf7be9ff27f0eb7d3e92b2f25db5b6.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_017476_leftImg8bit_foggy_beta_0-02_png.rf.45b148f61450f4581429c7b3450b11a5.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_018797_leftImg8bit_foggy_beta_0-02_png.rf.d7fbeb6a2302b221e4c51fb841bf8567.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_020215_leftImg8bit_foggy_beta_0-02_png.rf.94b0a45631e49a5caf56042ff58151ae.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_020321_leftImg8bit_foggy_beta_0-02_png.rf.596eb8bdc93bfe3c2464d5eee3ef8c75.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_020880_leftImg8bit_foggy_beta_0-02_png.rf.91f83533674c60546c4116113a89cc67.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_021667_leftImg8bit_foggy_beta_0-02_png.rf.5cf7ade604da935a80537d608e7ac728.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_021879_leftImg8bit_foggy_beta_0-02_png.rf.56fa1d075392dbb51639d14251fbf8c4.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_022254_leftImg8bit_foggy_beta_0-02_png.rf.2283113944073968663f7724afcb7e74.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000000_022797_leftImg8bit_foggy_beta_0-02_png.rf.a91fd0c9990530fb2773feec87cd13e4.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_000538_leftImg8bit_foggy_beta_0-02_png.rf.eb7d1f7b29dc1c25c853dbf2d58367dc.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_001464_leftImg8bit_foggy_beta_0-02_png.rf.a57591a486a18fdeb02073b607c79031.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_002512_leftImg8bit_foggy_beta_0-02_png.rf.22e7500c22598ea591b2f0441dfdaa2d.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_002646_leftImg8bit_foggy_beta_0-02_png.rf.9166b1b51cd837bb3b4b5c592d9ab335.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_002759_leftImg8bit_foggy_beta_0-02_png.rf.fb58b2eecc705ddf15f7fa0e1658cc7c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_003056_leftImg8bit_foggy_beta_0-02_png.rf.af4633a72af7cdf4b0c4c347f4265f17.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_003588_leftImg8bit_foggy_beta_0-02_png.rf.a4c56c373980f2ff5f6a0c75fd355909.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_004327_leftImg8bit_foggy_beta_0-02_png.rf.0f9f45652da68d5493f57c3c731a5673.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_004736_leftImg8bit_foggy_beta_0-02_png.rf.0555def550938533d2136ee73f14f5bd.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_004859_leftImg8bit_foggy_beta_0-02_png.rf.0ed4b2f573dd61cdb09a1afa7d5ebc18.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_005184_leftImg8bit_foggy_beta_0-02_png.rf.89cbea0ab16363108cc195e5cbb97558.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_005410_leftImg8bit_foggy_beta_0-02_png.rf.ced9aa7bb22c70d5b35a583324025254.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_007285_leftImg8bit_foggy_beta_0-02_png.rf.c915a8e065ad5f0446a048f88ccc3347.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_007407_leftImg8bit_foggy_beta_0-02_png.rf.66d437783a5c8ee983d93d42b2747106.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_007622_leftImg8bit_foggy_beta_0-02_png.rf.3ebb0c24c3f068c83c7e2bdc42f635ad.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_007857_leftImg8bit_foggy_beta_0-02_png.rf.db0468f386ceb3905a28fab0afc15e54.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_009058_leftImg8bit_foggy_beta_0-02_png.rf.a4abde4f19b117ed329c17ffd44653d1.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_009504_leftImg8bit_foggy_beta_0-02_png.rf.59ee4e081e6c3fabb1011b30ef108a29.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_009854_leftImg8bit_foggy_beta_0-02_png.rf.c5c08d6f8175f414f86f4e9d855ee798.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_010156_leftImg8bit_foggy_beta_0-02_png.rf.76d0504e6771d9cb2ea9add2dc0b103e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_010600_leftImg8bit_foggy_beta_0-02_png.rf.f75683a923c9b1816b5cbcb675e0bcc2.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_011162_leftImg8bit_foggy_beta_0-02_png.rf.9d33df0ac1e93499d797225f29cbd637.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_011835_leftImg8bit_foggy_beta_0-02_png.rf.200ff3065ef8b0e078570ff75abb15c8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_012519_leftImg8bit_foggy_beta_0-02_png.rf.7933c834d60a7808d729eea83f1c1a88.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_012699_leftImg8bit_foggy_beta_0-02_png.rf.afaca6c142244be29c08816056512f1f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_012738_leftImg8bit_foggy_beta_0-02_png.rf.c8057f5dd17b1e87041ece81687aa744.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_012870_leftImg8bit_foggy_beta_0-02_png.rf.bfb60b70c14c77d50ceab4ca288c3d61.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_013016_leftImg8bit_foggy_beta_0-02_png.rf.0f09839d06897f79e9ed4583d13f1254.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_014221_leftImg8bit_foggy_beta_0-02_png.rf.ea582d4f40b4e1a2e30fb9a8e9d24d50.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_014406_leftImg8bit_foggy_beta_0-02_png.rf.a576e7add4dd7bde655d38330c2a1cc0.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_014741_leftImg8bit_foggy_beta_0-02_png.rf.34cbf703b5c2fffa580120cbb949d58f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_015091_leftImg8bit_foggy_beta_0-02_png.rf.0b75fdcafc58dbbd70cf27cd97f70d38.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_016462_leftImg8bit_foggy_beta_0-02_png.rf.2709ad80f84af756f6c4102dd2d76486.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_017101_leftImg8bit_foggy_beta_0-02_png.rf.e83e3e4a8ca984525d26a0e087dd9f94.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_017459_leftImg8bit_foggy_beta_0-02_png.rf.0180a65355dcf2261424eb4036ac06c9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_017842_leftImg8bit_foggy_beta_0-02_png.rf.f8c03ac0baf6df622c503c566721ef20.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_018113_leftImg8bit_foggy_beta_0-02_png.rf.6adaf79832a91f9d9f552934eca3d45e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_019969_leftImg8bit_foggy_beta_0-02_png.rf.88b9695f261f546a2e26ae61378c591a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_020046_leftImg8bit_foggy_beta_0-02_png.rf.fb334b5495e6856304ab6921041c417b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_020287_leftImg8bit_foggy_beta_0-02_png.rf.11e6a3cfb387fb21f1409afad2897850.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_020693_leftImg8bit_foggy_beta_0-02_png.rf.a595721f78b675494363637f3ba04074.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_021406_leftImg8bit_foggy_beta_0-02_png.rf.b38a76e553e86541f4104b2ef821714b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_021825_leftImg8bit_foggy_beta_0-02_png.rf.9856e3e74654c03e98f2296af8d6f0e2.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_023235_leftImg8bit_foggy_beta_0-02_png.rf.3d23c9a258e70c38a478c12889d0ddb8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_024927_leftImg8bit_foggy_beta_0-02_png.rf.cfbcfa58c1ae4f5574566c6874421f48.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_025512_leftImg8bit_foggy_beta_0-02_png.rf.5ff2bc34b22ecdb26b485ed64660ef67.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_025713_leftImg8bit_foggy_beta_0-02_png.rf.fd7ffdd5207d96b1769d8f4ad0ab2a16.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_025921_leftImg8bit_foggy_beta_0-02_png.rf.39227f7efe3ab305ac72f168952685f1.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_027325_leftImg8bit_foggy_beta_0-02_png.rf.f8374b201fe60fd4440e52ec1661341f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_028232_leftImg8bit_foggy_beta_0-02_png.rf.1db2377f03c61078cd5f9f9494b0b730.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_028335_leftImg8bit_foggy_beta_0-02_png.rf.1b390762c65ae92a4d0283fbd2d15768.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_028590_leftImg8bit_foggy_beta_0-02_png.rf.77e117489b5e50bb772991dae6c3b977.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_028854_leftImg8bit_foggy_beta_0-02_png.rf.261570402b8bcf4be6b4a91e61c462e8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_029086_leftImg8bit_foggy_beta_0-02_png.rf.1ad1279a84c6fcb39f8a54b5021ba756.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_029236_leftImg8bit_foggy_beta_0-02_png.rf.f278dc63d5c7fe24627577932f631682.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_029600_leftImg8bit_foggy_beta_0-02_png.rf.7e019490be5fb198e7ff1633079bbf2a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_030067_leftImg8bit_foggy_beta_0-02_png.rf.d0801d455ad0c1c23da4ddddeed4561e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_030669_leftImg8bit_foggy_beta_0-02_png.rf.0f05afc32333f83b1001da6e281a1311.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_031416_leftImg8bit_foggy_beta_0-02_png.rf.a9c66eb72bf7e2d9b4cf5460f09f1e75.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_032018_leftImg8bit_foggy_beta_0-02_png.rf.4061d5b9bcb60adb8f4efc283d07003a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_032556_leftImg8bit_foggy_beta_0-02_png.rf.bf28770e138c8d26d70660dd5ff66c84.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_032942_leftImg8bit_foggy_beta_0-02_png.rf.722566198855a97d120885b1f974462c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_034047_leftImg8bit_foggy_beta_0-02_png.rf.0fc5aecf07aa2bcb1e96679f881d507c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_034816_leftImg8bit_foggy_beta_0-02_png.rf.6a18160d83dbadb4671abf57d1fbb385.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_035864_leftImg8bit_foggy_beta_0-02_png.rf.d82b9e91c355387c875267c5a4bb97c4.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_037705_leftImg8bit_foggy_beta_0-02_png.rf.239947b4bd67eb4b570809f2f6010f6c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_038245_leftImg8bit_foggy_beta_0-02_png.rf.b0f15334c83c39414cd14525528875cc.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_038418_leftImg8bit_foggy_beta_0-02_png.rf.90beea57a6dc7bc4e982a92d49d3cbff.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_038645_leftImg8bit_foggy_beta_0-02_png.rf.1ad234c2944324500e1f94df4ae8a2ed.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_038844_leftImg8bit_foggy_beta_0-02_png.rf.5fb724d3abc0540336669b15d616aa31.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_039895_leftImg8bit_foggy_beta_0-02_png.rf.f1ef35ac51e2f6ef6f84dd3ac3d0bb06.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_040575_leftImg8bit_foggy_beta_0-02_png.rf.98ff6c3555fde4e598dfb0ca4c7a72df.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_040732_leftImg8bit_foggy_beta_0-02_png.rf.ae5053ce6016fda6c0a79c71a9c4b292.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_041354_leftImg8bit_foggy_beta_0-02_png.rf.8496efb7c06f60958b55b494f78e1abe.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_041664_leftImg8bit_foggy_beta_0-02_png.rf.8ab9c3d25b55491dc5408d070da77664.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_042098_leftImg8bit_foggy_beta_0-02_png.rf.fb7081da440f777731f49493be939adb.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_042384_leftImg8bit_foggy_beta_0-02_png.rf.c6e3c905ee524d26ad82b1dfa5302690.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_042733_leftImg8bit_foggy_beta_0-02_png.rf.f6caeb0cebf9667ef54e4e4befd119b0.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_043395_leftImg8bit_foggy_beta_0-02_png.rf.5e1d3305a3f1b6be9b2e7628d0087a3d.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_043564_leftImg8bit_foggy_beta_0-02_png.rf.db915ab770e35db5681bbd3e13332f14.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_044227_leftImg8bit_foggy_beta_0-02_png.rf.0193ec2b67202c035fcd92dbf0b3ef3a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_044413_leftImg8bit_foggy_beta_0-02_png.rf.67e083c622d1b4334ee362998bd973e8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_044525_leftImg8bit_foggy_beta_0-02_png.rf.3fb85909e55b752ddb4e5688f811c92a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_044787_leftImg8bit_foggy_beta_0-02_png.rf.fd8b40aeba2eeca3c164625bc7c967c9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_046126_leftImg8bit_foggy_beta_0-02_png.rf.c8140aee96a70095b2b3135a006d0447.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_046272_leftImg8bit_foggy_beta_0-02_png.rf.2a81887ad73a4b8314ddde40f7c28c5f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_046779_leftImg8bit_foggy_beta_0-02_png.rf.5e509b1cc0a93819be2eb999fabbde07.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_047178_leftImg8bit_foggy_beta_0-02_png.rf.49fb73592c28cac46c2eb1be7c0a9c4a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_048196_leftImg8bit_foggy_beta_0-02_png.rf.6983858e53586677aef4a2ae4189e750.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_048355_leftImg8bit_foggy_beta_0-02_png.rf.d3377526108486cea71e5fe2884f6b2c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_048654_leftImg8bit_foggy_beta_0-02_png.rf.30d47f637ef3d39d888a05df01d561e5.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_049078_leftImg8bit_foggy_beta_0-02_png.rf.a636e32d175d31d18fc73f1a21a56ab9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_049209_leftImg8bit_foggy_beta_0-02_png.rf.b679216abe779d5c8d2c4c4e94533d8e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_049698_leftImg8bit_foggy_beta_0-02_png.rf.2c5a55a9d5599b7c8f6eec51fdc78a1c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_049770_leftImg8bit_foggy_beta_0-02_png.rf.24d9e4287e2c769a95f9e8b126d91a2f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_050149_leftImg8bit_foggy_beta_0-02_png.rf.4224717b45dd3abae0c5cc13a9d49086.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_051516_leftImg8bit_foggy_beta_0-02_png.rf.cf6d91e3cf9124e59f3e0e0dceb242f6.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_051737_leftImg8bit_foggy_beta_0-02_png.rf.8374dfb4b6de216328dbac7541ffea4c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_051807_leftImg8bit_foggy_beta_0-02_png.rf.f60be47ffa07646a3a5bb53140b1ec48.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_052120_leftImg8bit_foggy_beta_0-02_png.rf.452e0a2b0ffd8de0e752590b6dc6fe29.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_052594_leftImg8bit_foggy_beta_0-02_png.rf.b3f0c4ba64b30e46296201c2be075d1e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_053102_leftImg8bit_foggy_beta_0-02_png.rf.7eac9f76954efb23718776a5a4bdbd4d.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_054077_leftImg8bit_foggy_beta_0-02_png.rf.0584e36a27927766d78f52c41da20660.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_054219_leftImg8bit_foggy_beta_0-02_png.rf.cad7ef276196e7a02cd114631e44006b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_054415_leftImg8bit_foggy_beta_0-02_png.rf.93c3bc9d0b36a64c4ca26b2f1681f52f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_054640_leftImg8bit_foggy_beta_0-02_png.rf.726f3ab325e86c4cf868d11e9cc2a45f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_054884_leftImg8bit_foggy_beta_0-02_png.rf.51286a1fb3809742fa9194ba6846f548.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_055172_leftImg8bit_foggy_beta_0-02_png.rf.6f52c1f3e47ee6d8eaca3251d3db430a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_055387_leftImg8bit_foggy_beta_0-02_png.rf.a1542a8607cc70e7c9b8151bd3eac76e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_055538_leftImg8bit_foggy_beta_0-02_png.rf.2ccf671c7a677d7de5df826c21359242.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_055603_leftImg8bit_foggy_beta_0-02_png.rf.495186b4059571440412a4d85aa0643a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_055709_leftImg8bit_foggy_beta_0-02_png.rf.a2cf12246b3fb117fb9a44edb5d779b2.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_056580_leftImg8bit_foggy_beta_0-02_png.rf.6cc5d55b3d58a7b6062f69db793d6a53.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_057181_leftImg8bit_foggy_beta_0-02_png.rf.618117e7aca66d00e2df496de1a97e54.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_057478_leftImg8bit_foggy_beta_0-02_png.rf.9450c22fa08ea87e60eae8fdbdc8fd34.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_057954_leftImg8bit_foggy_beta_0-02_png.rf.e25ec6ba9be71fb7418853171dd76f43.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_058057_leftImg8bit_foggy_beta_0-02_png.rf.48f7fabb454c6f6ebd2c460975eb25cb.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_058504_leftImg8bit_foggy_beta_0-02_png.rf.8fe8d5b67d22bfc3b746ed44a09160e9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_059119_leftImg8bit_foggy_beta_0-02_png.rf.102fcdbc59b1b87d4e3d812004123f43.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_059642_leftImg8bit_foggy_beta_0-02_png.rf.aee8943f778162ac72b8b8cdb9effcb8.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_059789_leftImg8bit_foggy_beta_0-02_png.rf.77be0a4dd0e563838b93a00e37bb577e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_060545_leftImg8bit_foggy_beta_0-02_png.rf.e93f8233099d5eee41912a35ecc559bb.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_061682_leftImg8bit_foggy_beta_0-02_png.rf.5a11daea6508f318c7fa4dbaa929a1f7.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_061763_leftImg8bit_foggy_beta_0-02_png.rf.57133dd710bd3aef49ee94327a9f4a78.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_062016_leftImg8bit_foggy_beta_0-02_png.rf.dac2604f53c7028f6631efbc504f8487.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_062250_leftImg8bit_foggy_beta_0-02_png.rf.83e56f1322da1bb178f27b8ea4836ec9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_062509_leftImg8bit_foggy_beta_0-02_png.rf.53dc3b28e5ea278620533d323eb0fc03.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_062653_leftImg8bit_foggy_beta_0-02_png.rf.a5ab35b72451d47aaf8451522ed923c5.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_062793_leftImg8bit_foggy_beta_0-02_png.rf.41f3259ab994f709d1f0744ae5e142fc.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_063045_leftImg8bit_foggy_beta_0-02_png.rf.f1d07e6fa7a72781cdf1f312e49dbb9a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_064130_leftImg8bit_foggy_beta_0-02_png.rf.be4873a2e0a11db55daa50bd0aaf5a1c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_064305_leftImg8bit_foggy_beta_0-02_png.rf.36d04a56e6d2179ba12dfb856ca7b216.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_064651_leftImg8bit_foggy_beta_0-02_png.rf.84f914dd1441de2bea33c6476f1f247b.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_064798_leftImg8bit_foggy_beta_0-02_png.rf.2244aea6c3c6afb9d8e5069cbac3b3fb.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_064925_leftImg8bit_foggy_beta_0-02_png.rf.8a134511a371d01232f30ff70aca1ff5.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_065850_leftImg8bit_foggy_beta_0-02_png.rf.0fb5e8031e6ec8e9b20f5f45781c8351.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_066092_leftImg8bit_foggy_beta_0-02_png.rf.298a13ab55bd9c57dc78746494bc0da5.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_066438_leftImg8bit_foggy_beta_0-02_png.rf.470885c3a7a562c3628e628b0072b9f9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_066574_leftImg8bit_foggy_beta_0-02_png.rf.a0708dcfc55946ac70a361d7323f571d.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_066832_leftImg8bit_foggy_beta_0-02_png.rf.89449858bc6e73b0a0dd9d56c17997d3.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_067092_leftImg8bit_foggy_beta_0-02_png.rf.b6e0790d1fc2862c9de00202c5862a9d.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_067178_leftImg8bit_foggy_beta_0-02_png.rf.739ca84c7e1c1d50b13f375ac3dbf458.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_067295_leftImg8bit_foggy_beta_0-02_png.rf.c9676f6994e0f6fc67f7d2278b9bbc2a.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_067474_leftImg8bit_foggy_beta_0-02_png.rf.29bfe019074e480ecb6f66b777d5dc9c.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_067735_leftImg8bit_foggy_beta_0-02_png.rf.58b641f889119067020966d7ca05776f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_068063_leftImg8bit_foggy_beta_0-02_png.rf.8430f72953df9bc7bdfd86c1182e9b42.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_068208_leftImg8bit_foggy_beta_0-02_png.rf.dcfaa17436cecad3de4482ac44d676e2.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_068682_leftImg8bit_foggy_beta_0-02_png.rf.820f51d71cebd3cce923faa5a76ee054.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_068772_leftImg8bit_foggy_beta_0-02_png.rf.68fb7cf981ab2e7fb4503a30cc7240dc.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_069633_leftImg8bit_foggy_beta_0-02_png.rf.df59f324f9f7b5e26408ba5101a6a9de.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_070099_leftImg8bit_foggy_beta_0-02_png.rf.112500572edb02464441219a17461960.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_071288_leftImg8bit_foggy_beta_0-02_png.rf.280b01b7026f8a25e5f17fa859d72d8f.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_071781_leftImg8bit_foggy_beta_0-02_png.rf.98385ba6dd6df075b08e0d5589ace87e.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_072155_leftImg8bit_foggy_beta_0-02_png.rf.54708c798b0747263b9158659d85e7d1.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_072295_leftImg8bit_foggy_beta_0-02_png.rf.4f94e06d0c134a1ab88d46c60bfcfee9.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_073088_leftImg8bit_foggy_beta_0-02_png.rf.e2932cad0c3cbd666da0868a316b1ca3.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_073243_leftImg8bit_foggy_beta_0-02_png.rf.d737628bcde1f5cc0c5b2456e7b7fc45.jpg  \n",
            "  inflating: /content/data/target/frankfurt_000001_073464_leftImg8bit_foggy_beta_0-02_png.rf.3033491eb58131cf6b072df50f8180df.jpg  \n",
            "   creating: /content/data/train/\n",
            "  inflating: /content/data/train/aachen_000000_000019_leftImg8bit_png.rf.582961c9887d9dae3c8dd1610453db5f.jpg  \n",
            "  inflating: /content/data/train/aachen_000001_000019_leftImg8bit_png.rf.f91aa5821ab0425ddcbcd47a0cac899e.jpg  \n",
            "  inflating: /content/data/train/aachen_000004_000019_leftImg8bit_png.rf.142c5891e1df86382211343550db7023.jpg  \n",
            "  inflating: /content/data/train/aachen_000005_000019_leftImg8bit_png.rf.de26ff2bd0d7e188be5c08b6d989c2a2.jpg  \n",
            "  inflating: /content/data/train/aachen_000006_000019_leftImg8bit_png.rf.4fb5925d88109cf20d3504bbe9468322.jpg  \n",
            "  inflating: /content/data/train/aachen_000008_000019_leftImg8bit_png.rf.cad9c3ec56eded4864bdd989ace140f6.jpg  \n",
            "  inflating: /content/data/train/aachen_000009_000019_leftImg8bit_png.rf.a7e527f04f1d1c5facabd83b914a95ca.jpg  \n",
            "  inflating: /content/data/train/aachen_000010_000019_leftImg8bit_png.rf.675f7c3c4a4f3cbe26c09e4b412cef5a.jpg  \n",
            "  inflating: /content/data/train/aachen_000011_000019_leftImg8bit_png.rf.dd610a26bd9e6be7f93f12f43aeb36aa.jpg  \n",
            "  inflating: /content/data/train/aachen_000013_000019_leftImg8bit_png.rf.5fc98df03ca46477aa96f62cface6fba.jpg  \n",
            "  inflating: /content/data/train/aachen_000016_000019_leftImg8bit_png.rf.2e35b5dad96ada244066499b12f2f6eb.jpg  \n",
            "  inflating: /content/data/train/aachen_000018_000019_leftImg8bit_png.rf.77d20ae3a4d0e48af549a191bd33bad9.jpg  \n",
            "  inflating: /content/data/train/aachen_000020_000019_leftImg8bit_png.rf.6a9e74e6c1a5e5c677d3ca11d3d52f69.jpg  \n",
            "  inflating: /content/data/train/aachen_000021_000019_leftImg8bit_png.rf.0b389f3e83a0d9f0ce2f0d7fb273f034.jpg  \n",
            "  inflating: /content/data/train/aachen_000022_000019_leftImg8bit_png.rf.6a16d20bca3574c8277828adf4836134.jpg  \n",
            "  inflating: /content/data/train/aachen_000023_000019_leftImg8bit_png.rf.b6b31e1fa5ea04e9cf13e9a472b6f827.jpg  \n",
            "  inflating: /content/data/train/aachen_000024_000019_leftImg8bit_png.rf.8009b3386285391e44c5a21fd3e02eaa.jpg  \n",
            "  inflating: /content/data/train/aachen_000025_000019_leftImg8bit_png.rf.e687bdc864d15d67b649837c417d9a59.jpg  \n",
            "  inflating: /content/data/train/aachen_000026_000019_leftImg8bit_png.rf.8748f8bc57ee4d152e7fa5eaab0cd5d7.jpg  \n",
            "  inflating: /content/data/train/aachen_000027_000019_leftImg8bit_png.rf.1181733e113db61e61276b22526b3fc1.jpg  \n",
            "  inflating: /content/data/train/aachen_000028_000019_leftImg8bit_png.rf.ea4a138db348d6603161fea627a5278e.jpg  \n",
            "  inflating: /content/data/train/aachen_000030_000019_leftImg8bit_png.rf.65655c6374cb57a6675ecff27c9d6f35.jpg  \n",
            "  inflating: /content/data/train/aachen_000031_000019_leftImg8bit_png.rf.e2db2694efa79a571dfa721abb9fea52.jpg  \n",
            "  inflating: /content/data/train/aachen_000033_000019_leftImg8bit_png.rf.d205d12321923fc632b7ab25098fcad2.jpg  \n",
            "  inflating: /content/data/train/aachen_000034_000019_leftImg8bit_png.rf.f8c5d9982106ca613017c72024909a67.jpg  \n",
            "  inflating: /content/data/train/aachen_000036_000019_leftImg8bit_png.rf.db334a2a24a2faee8a0d665f3871c067.jpg  \n",
            "  inflating: /content/data/train/aachen_000037_000019_leftImg8bit_png.rf.72979012452111fba093cbc6582a753b.jpg  \n",
            "  inflating: /content/data/train/aachen_000038_000019_leftImg8bit_png.rf.d1361a986c4e094e54c934a5d6e96320.jpg  \n",
            "  inflating: /content/data/train/aachen_000039_000019_leftImg8bit_png.rf.0d7e6af1d670616da2dca51e52060f88.jpg  \n",
            "  inflating: /content/data/train/aachen_000042_000019_leftImg8bit_png.rf.8d4bbff500a1a12f8c726be8e961e564.jpg  \n",
            "  inflating: /content/data/train/aachen_000043_000019_leftImg8bit_png.rf.b2cd43d90e05ce870ca9baa3096e547e.jpg  \n",
            "  inflating: /content/data/train/aachen_000044_000019_leftImg8bit_png.rf.00fa4c7ce5b1518bef7a82301cdd6a05.jpg  \n",
            "  inflating: /content/data/train/aachen_000045_000019_leftImg8bit_png.rf.1e16d81e47f37dbd285a6eb7e448abc7.jpg  \n",
            "  inflating: /content/data/train/aachen_000046_000019_leftImg8bit_png.rf.2f2ec43d41e99f166c13ea0c1e0331d8.jpg  \n",
            "  inflating: /content/data/train/aachen_000047_000019_leftImg8bit_png.rf.17ce1c4c3dbc025982fa3cda51648a16.jpg  \n",
            "  inflating: /content/data/train/aachen_000050_000019_leftImg8bit_png.rf.f4709323a3f6dde6fc747d50054cc598.jpg  \n",
            "  inflating: /content/data/train/aachen_000051_000019_leftImg8bit_png.rf.547ab162758d1e43cd969afcecc6ec34.jpg  \n",
            "  inflating: /content/data/train/aachen_000052_000019_leftImg8bit_png.rf.d02bb848e044d2b2527c62637bc43e9f.jpg  \n",
            "  inflating: /content/data/train/aachen_000054_000019_leftImg8bit_png.rf.4d8a3479da146b6bed4d6c909234eed0.jpg  \n",
            "  inflating: /content/data/train/aachen_000055_000019_leftImg8bit_png.rf.bb998259571bbcbae4f82e08e3734e12.jpg  \n",
            "  inflating: /content/data/train/aachen_000057_000019_leftImg8bit_png.rf.13cfcba5e93781422271386003839a96.jpg  \n",
            "  inflating: /content/data/train/aachen_000058_000019_leftImg8bit_png.rf.d84b7b3b2df9e69f58cae9c4d5b360fd.jpg  \n",
            "  inflating: /content/data/train/aachen_000060_000019_leftImg8bit_png.rf.eff631cc3a3c23866ed331de3b3ffb3c.jpg  \n",
            "  inflating: /content/data/train/aachen_000063_000019_leftImg8bit_png.rf.6337ee59c0f594739be7b3eec4efc5e5.jpg  \n",
            "  inflating: /content/data/train/aachen_000065_000019_leftImg8bit_png.rf.266a2b60b1c86eb35e705bbccc7d4259.jpg  \n",
            "  inflating: /content/data/train/aachen_000067_000019_leftImg8bit_png.rf.2f948caed84ba80f408cd7bd033f1b73.jpg  \n",
            "  inflating: /content/data/train/aachen_000068_000019_leftImg8bit_png.rf.9280201c560cb90cb5a88da3e1a791ff.jpg  \n",
            "  inflating: /content/data/train/aachen_000069_000019_leftImg8bit_png.rf.d874145a3fba381eb78a867c54724ac5.jpg  \n",
            "  inflating: /content/data/train/aachen_000072_000019_leftImg8bit_png.rf.1530c551e18dae775c81c0f1286a8c64.jpg  \n",
            "  inflating: /content/data/train/aachen_000073_000019_leftImg8bit_png.rf.9a071eeb371fa4e82041e1279c448a02.jpg  \n",
            "  inflating: /content/data/train/aachen_000075_000019_leftImg8bit_png.rf.73cd658733f76de10456bcc0a0947a82.jpg  \n",
            "  inflating: /content/data/train/aachen_000076_000019_leftImg8bit_png.rf.6cf14dfe9991bec232596e2bf39ef568.jpg  \n",
            "  inflating: /content/data/train/aachen_000077_000019_leftImg8bit_png.rf.0aeda9d9faa9281cda0c0a6e9fd6b97d.jpg  \n",
            "  inflating: /content/data/train/aachen_000078_000019_leftImg8bit_png.rf.cc4adfa4551ca8d0d034f89526604295.jpg  \n",
            "  inflating: /content/data/train/aachen_000079_000019_leftImg8bit_png.rf.2ac3bba14dd21775a50bd617bc7be587.jpg  \n",
            "  inflating: /content/data/train/aachen_000080_000019_leftImg8bit_png.rf.2311c995e3abeda3b7c2ba2e90dddf80.jpg  \n",
            "  inflating: /content/data/train/aachen_000081_000019_leftImg8bit_png.rf.a90eed1c7a7545c6410037bb21ef5d36.jpg  \n",
            "  inflating: /content/data/train/aachen_000082_000019_leftImg8bit_png.rf.72a023062f990cdb18baea60b19e3d79.jpg  \n",
            "  inflating: /content/data/train/aachen_000083_000019_leftImg8bit_png.rf.30dc49787becc63a284e9c2a973806c4.jpg  \n",
            "  inflating: /content/data/train/aachen_000085_000019_leftImg8bit_png.rf.122d3e8ef5ad1c8aadde930be5e4b461.jpg  \n",
            "  inflating: /content/data/train/aachen_000087_000019_leftImg8bit_png.rf.6ca99b38326512df1416d294871720eb.jpg  \n",
            "  inflating: /content/data/train/aachen_000088_000019_leftImg8bit_png.rf.c82e3c76de2fd92381659a0a82ee1405.jpg  \n",
            "  inflating: /content/data/train/aachen_000089_000019_leftImg8bit_png.rf.506c340b39d22561c43b3272e0bfe0a6.jpg  \n",
            "  inflating: /content/data/train/aachen_000090_000019_leftImg8bit_png.rf.4d935aa535df87c3733362904cc8778f.jpg  \n",
            "  inflating: /content/data/train/aachen_000092_000019_leftImg8bit_png.rf.b76779489351cd819dfa0d7c8d1484a4.jpg  \n",
            "  inflating: /content/data/train/aachen_000093_000019_leftImg8bit_png.rf.356e7e9aa28a0a91dc88d71ba98657d9.jpg  \n",
            "  inflating: /content/data/train/aachen_000094_000019_leftImg8bit_png.rf.6d24805d7ba12b81f792fdb481548448.jpg  \n",
            "  inflating: /content/data/train/aachen_000095_000019_leftImg8bit_png.rf.dc2989dcd19abb2433b9065388619310.jpg  \n",
            "  inflating: /content/data/train/aachen_000096_000019_leftImg8bit_png.rf.0255b08c88af8b9200593ced5d056a61.jpg  \n",
            "  inflating: /content/data/train/aachen_000097_000019_leftImg8bit_png.rf.aca03dc4416adfc2a64452821cda6bb1.jpg  \n",
            "  inflating: /content/data/train/aachen_000098_000019_leftImg8bit_png.rf.7fbd90ec7b2226ad8a178f7e9d9030a5.jpg  \n",
            "  inflating: /content/data/train/aachen_000099_000019_leftImg8bit_png.rf.02d4fa4ae52edebf96f044ba83ed66e0.jpg  \n",
            "  inflating: /content/data/train/aachen_000101_000019_leftImg8bit_png.rf.49c37139cf2433206154633375f28229.jpg  \n",
            "  inflating: /content/data/train/aachen_000103_000019_leftImg8bit_png.rf.1d0680f932c2ac46c90b75cf203e0bd8.jpg  \n",
            "  inflating: /content/data/train/aachen_000104_000019_leftImg8bit_png.rf.acc8cb7e50c8fbf34574646ec432f9eb.jpg  \n",
            "  inflating: /content/data/train/aachen_000105_000019_leftImg8bit_png.rf.f7dca82179bbf100cdb5e8588dda6072.jpg  \n",
            "  inflating: /content/data/train/aachen_000106_000019_leftImg8bit_png.rf.4e69cfbc700b9b93f259ea5b5154fab8.jpg  \n",
            "  inflating: /content/data/train/aachen_000107_000019_leftImg8bit_png.rf.33cc496a658ab5442d351466769b29c5.jpg  \n",
            "  inflating: /content/data/train/aachen_000108_000019_leftImg8bit_png.rf.781c474237b83bb520170754abefc440.jpg  \n",
            "  inflating: /content/data/train/aachen_000109_000019_leftImg8bit_png.rf.14683461510891e9b39c1b6e455c98e4.jpg  \n",
            "  inflating: /content/data/train/aachen_000113_000019_leftImg8bit_png.rf.d0520d659de2a15f82958165a9211091.jpg  \n",
            "  inflating: /content/data/train/aachen_000114_000019_leftImg8bit_png.rf.f458aa9a3124ba0ed37b3d770b4281da.jpg  \n",
            "  inflating: /content/data/train/aachen_000115_000019_leftImg8bit_png.rf.3eac5ba7981b51c6f2b1c76b2acb52c8.jpg  \n",
            "  inflating: /content/data/train/aachen_000117_000019_leftImg8bit_png.rf.81ae718a167a947f37ac111f8e7b50f5.jpg  \n",
            "  inflating: /content/data/train/aachen_000118_000019_leftImg8bit_png.rf.20a269ca4d5f2f4396a58ce92b472de3.jpg  \n",
            "  inflating: /content/data/train/aachen_000119_000019_leftImg8bit_png.rf.91bb1b005bf551ab15f0c17cee5d3935.jpg  \n",
            "  inflating: /content/data/train/aachen_000124_000019_leftImg8bit_png.rf.0524efd0270f1cf4e053db2417193fee.jpg  \n",
            "  inflating: /content/data/train/aachen_000125_000019_leftImg8bit_png.rf.180b75c2204b7b051ed4e1d52da910fb.jpg  \n",
            "  inflating: /content/data/train/aachen_000126_000019_leftImg8bit_png.rf.e07728e3131d3c6043cadf083e77713f.jpg  \n",
            "  inflating: /content/data/train/aachen_000127_000019_leftImg8bit_png.rf.8f706816706f0e7f20ea71d3bf7d419f.jpg  \n",
            "  inflating: /content/data/train/aachen_000128_000019_leftImg8bit_png.rf.6ae359a7fd72496cb3b0d5fcc996d434.jpg  \n",
            "  inflating: /content/data/train/aachen_000129_000019_leftImg8bit_png.rf.631ca71349d4e6baf58833f8714d7851.jpg  \n",
            "  inflating: /content/data/train/aachen_000130_000019_leftImg8bit_png.rf.c7e5ac55a71ac12880822c051f2de163.jpg  \n",
            "  inflating: /content/data/train/aachen_000131_000019_leftImg8bit_png.rf.8e488844cce658f76733654af0e2cd7d.jpg  \n",
            "  inflating: /content/data/train/aachen_000132_000019_leftImg8bit_png.rf.afc8ad56b7a29d94906caadb16ac672c.jpg  \n",
            "  inflating: /content/data/train/aachen_000133_000019_leftImg8bit_png.rf.9fb3b29d309760ff05f0a972c034beaf.jpg  \n",
            "  inflating: /content/data/train/aachen_000134_000019_leftImg8bit_png.rf.ba30a4a6e827364d7e0f407eaad5b491.jpg  \n",
            "  inflating: /content/data/train/aachen_000135_000019_leftImg8bit_png.rf.bce0c7196e52636b2a82e1374d568de1.jpg  \n",
            "  inflating: /content/data/train/aachen_000136_000019_leftImg8bit_png.rf.a7e51c45a64d183f1699f2fd993e90a0.jpg  \n",
            "  inflating: /content/data/train/aachen_000137_000019_leftImg8bit_png.rf.ca65ac2cb350587c9ddff89514878f45.jpg  \n",
            "  inflating: /content/data/train/aachen_000140_000019_leftImg8bit_png.rf.96d744a6fd6b306f019f7d8b4fa8eea6.jpg  \n",
            "  inflating: /content/data/train/aachen_000141_000019_leftImg8bit_png.rf.ee35b8d44543bbedc4045d2378147f29.jpg  \n",
            "  inflating: /content/data/train/aachen_000142_000019_leftImg8bit_png.rf.d4e871c2194bb382986237ebba91e246.jpg  \n",
            "  inflating: /content/data/train/aachen_000143_000019_leftImg8bit_png.rf.d893d65b77e35d650e9a35fad263cb76.jpg  \n",
            "  inflating: /content/data/train/aachen_000145_000019_leftImg8bit_png.rf.71fee64855f3b61477e3f2edb72c1371.jpg  \n",
            "  inflating: /content/data/train/aachen_000146_000019_leftImg8bit_png.rf.34797ab10911181da932fdda12cc1bcb.jpg  \n",
            "  inflating: /content/data/train/aachen_000147_000019_leftImg8bit_png.rf.e274d8d63ba97a51f9e2903aec4af6ca.jpg  \n",
            "  inflating: /content/data/train/aachen_000148_000019_leftImg8bit_png.rf.1623e2e6f4b5e9eb8e7d08f783c8b62a.jpg  \n",
            "  inflating: /content/data/train/aachen_000149_000019_leftImg8bit_png.rf.b2785c1ee31130aea2c1e30aa9b8ee72.jpg  \n",
            "  inflating: /content/data/train/aachen_000151_000019_leftImg8bit_png.rf.8a799370f596d09430b4ba3b4031a756.jpg  \n",
            "  inflating: /content/data/train/aachen_000152_000019_leftImg8bit_png.rf.4a46069771bc7e953460f23ee9aabb81.jpg  \n",
            "  inflating: /content/data/train/aachen_000153_000019_leftImg8bit_png.rf.f1cba172cf391b9dce19cf26e020e6a1.jpg  \n",
            "  inflating: /content/data/train/aachen_000154_000019_leftImg8bit_png.rf.882ceaaa629155c07f0e14365fc88592.jpg  \n",
            "  inflating: /content/data/train/aachen_000156_000019_leftImg8bit_png.rf.eb4161ee4b7897524001f8644c6395bc.jpg  \n",
            "  inflating: /content/data/train/aachen_000157_000019_leftImg8bit_png.rf.8f4bdf1192aed5b585586cfa3735fce6.jpg  \n",
            "  inflating: /content/data/train/aachen_000158_000019_leftImg8bit_png.rf.7e135350c7c101d65f4d416a18fb4164.jpg  \n",
            "  inflating: /content/data/train/aachen_000159_000019_leftImg8bit_png.rf.27211f1b4f231e82fa8245345ead8339.jpg  \n",
            "  inflating: /content/data/train/aachen_000161_000019_leftImg8bit_png.rf.560dc17d398c591ebbadcdbd32ca05de.jpg  \n",
            "  inflating: /content/data/train/aachen_000162_000019_leftImg8bit_png.rf.9d7e0d3df4ebbe261a35cd9f4b0d8c51.jpg  \n",
            "  inflating: /content/data/train/aachen_000166_000019_leftImg8bit_png.rf.a42129c63a156108e76d48b9d909f921.jpg  \n",
            "  inflating: /content/data/train/aachen_000170_000019_leftImg8bit_png.rf.3c7a97c13c6f6ed36fc73d977f2f5f61.jpg  \n",
            "  inflating: /content/data/train/aachen_000171_000019_leftImg8bit_png.rf.adb9a75ba618dd928a760402a18ab860.jpg  \n",
            "  inflating: /content/data/train/aachen_000172_000019_leftImg8bit_png.rf.0aa2934f298162c62156f4d873d82232.jpg  \n",
            "  inflating: /content/data/train/aachen_000173_000019_leftImg8bit_png.rf.b4845aed54386c2cc0129fcca1eae8a7.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_000313_leftImg8bit_png.rf.cdb54888334f20611ebcd00c9af1deec.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_000600_leftImg8bit_png.rf.b4c24893839f2bcd9b06afde6f849e8a.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_000885_leftImg8bit_png.rf.01a02ae08b2bfb62d0e80947c9be8872.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_001097_leftImg8bit_png.rf.c7e62a71951ab1ab5842b4ff5127325f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_001519_leftImg8bit_png.rf.0ecd8b2735e614373916e1ceb3a1d7d6.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_001828_leftImg8bit_png.rf.bbd3318dc482c8ccf7945cf65fe6d951.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_002293_leftImg8bit_png.rf.2017c0aedd3274782c88d2efb46f97df.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_002562_leftImg8bit_png.rf.848ac562b826b2c581911d69053799a3.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_003005_leftImg8bit_png.rf.23b4c88d410a7bc0748b4cd8beb4e98b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_003245_leftImg8bit_png.rf.5fe32ed5747882d0885e87ca4f5e8b21.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_003674_leftImg8bit_png.rf.06dccce91b2cfb7485ad7e703350620b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_004032_leftImg8bit_png.rf.7c84310fc88c12ddee1373ceba32cb36.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_004229_leftImg8bit_png.rf.0a200538efdf332df0ed72d7fd83bb02.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_004748_leftImg8bit_png.rf.eefb5f1a511d74a60ba60eb0b1da8cf5.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_005537_leftImg8bit_png.rf.49cc4e08f14d7d86ded0ce4665fa5210.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_005936_leftImg8bit_png.rf.56f7b0c22522c08edb995cdbb2e72c76.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_006026_leftImg8bit_png.rf.ca46678b4a0b8c454ff2707273b5326f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_006484_leftImg8bit_png.rf.a0b9382e02d60b014ca464b0b3877d57.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_006746_leftImg8bit_png.rf.2bfe8191bd8d58f2bd3c9027e82e13c3.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_007150_leftImg8bit_png.rf.5d4767e7b9f38941d4d33a9afca6f0b9.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_007651_leftImg8bit_png.rf.f3990a0ce5318f8a2543694c79dc677f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_007950_leftImg8bit_png.rf.e74ba39da412ddbc4182b18334f2b4a6.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_008162_leftImg8bit_png.rf.ffa6fba76cc50401e9320689f9fa4e7e.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_008448_leftImg8bit_png.rf.0b0d4699fed0f591b4320949ead1221f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_008804_leftImg8bit_png.rf.2b7364087aa2828dd5ce1809af738f1d.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_009554_leftImg8bit_png.rf.a53264261013eb7a73b2da3dc815b5f7.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_009951_leftImg8bit_png.rf.f5bd1ba188821fc687b5784b6bbb9db4.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_010562_leftImg8bit_png.rf.1bc55b226f79b236b65caa5d31db800b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_010700_leftImg8bit_png.rf.0d6d60bcc7834f21fa8e1b6f91f9b92a.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_011255_leftImg8bit_png.rf.ae9061f18745641e2eb326804cefe956.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_011711_leftImg8bit_png.rf.ed5d78847f6960e7550b2aa598249c15.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_013209_leftImg8bit_png.rf.ae4b5c6c11d4c9e5049e97b38fbc8879.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_013705_leftImg8bit_png.rf.8d9bab8adc8b3defa4bcaca6965d7ab7.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_014332_leftImg8bit_png.rf.550c0517305e8ff5344fd234a6c0f97b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_014658_leftImg8bit_png.rf.ab3930592d123c03c355f0eace7ff85a.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_014803_leftImg8bit_png.rf.caf45c9c363708685e260fc7b51be23b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_015038_leftImg8bit_png.rf.62e139d2f97498715ef3fe8a6b6e1c35.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_015321_leftImg8bit_png.rf.7f182a92115bb07442599a7db5ba172b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_015645_leftImg8bit_png.rf.a3651463751f17ab64b54c52fc3bfd66.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_015880_leftImg8bit_png.rf.158cbe1a9b498263d75e4f7a13219964.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_016125_leftImg8bit_png.rf.e16afcadd8849ede0789ddc05384d331.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_016260_leftImg8bit_png.rf.7f720d63a85187ae68bc78a48b497f62.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_016591_leftImg8bit_png.rf.8db9f2f9eb6b552673739c6fc74e38f5.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_016758_leftImg8bit_png.rf.3346b9076cbbde3513e9c3a752d37ea1.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_017216_leftImg8bit_png.rf.4cbea2ce893ddd2cfdaf22de929c2636.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_017453_leftImg8bit_png.rf.860c1c92b266e77ab84ef397473e95b9.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_018195_leftImg8bit_png.rf.cf6f05a76903407c272b24b22f6582f0.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_019188_leftImg8bit_png.rf.8557e31d399f843d310e76e65f0d7c80.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_020673_leftImg8bit_png.rf.dbd3a86dcbd8805d713e5c4217080444.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_020776_leftImg8bit_png.rf.369980b8627d9677e26ac8bdf83a7547.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_020899_leftImg8bit_png.rf.6ce4b719674fe57df93bb54dc5bbb998.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021070_leftImg8bit_png.rf.8325c8ae110f28f6fe237a04f2e33636.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021325_leftImg8bit_png.rf.b68052e0f3ee178253e75a0169bf49d5.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021393_leftImg8bit_png.rf.2b23e701b246cc68999af0cb35d07e8f.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021479_leftImg8bit_png.rf.d9aa04d2edc3394d394bfa35e16ae490.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_021606_leftImg8bit_png.rf.6702f2e90e4555f7f1460db2c5e5850a.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_022210_leftImg8bit_png.rf.c1177d53e1dfd1ef8bf890b09aa67aee.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_022414_leftImg8bit_png.rf.5d566b2d947f4b399ee2af9f3f5d1232.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_023040_leftImg8bit_png.rf.8f9eebc460ecdc4bff9b56f530d9aa1c.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_023174_leftImg8bit_png.rf.42032eae5d1bb3c9541b1b7bc9a02cc2.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_023435_leftImg8bit_png.rf.17cecf48854cef1e1e075a80c94e4153.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_023648_leftImg8bit_png.rf.e95f5a8364203c3f5d24ffa4ba25879d.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024196_leftImg8bit_png.rf.585d15dd2c92cab2f73b441ad4807b8b.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024343_leftImg8bit_png.rf.edaa4c6ef8c708146cdc968949a20b55.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024524_leftImg8bit_png.rf.ba7c88de97f03a740dc6dc48254f0b68.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024717_leftImg8bit_png.rf.64e781abffc3371bb926895177aeccef.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_024855_leftImg8bit_png.rf.35403fc1d9bc0a72c447e6858bac2a02.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_025746_leftImg8bit_png.rf.ec311dad296652b3ec4e81f6674af666.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_025833_leftImg8bit_png.rf.9705275256673249d132f470ce6fb911.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_026056_leftImg8bit_png.rf.344f8ddabad1025860710b809d278c20.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_026634_leftImg8bit_png.rf.5a6ad1ef572d13fa578e36c86598e1bc.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_027057_leftImg8bit_png.rf.5d389009d11e6c33bae12dc45b09c3d9.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_027699_leftImg8bit_png.rf.488e9485ee5f3ff55db89fa6522c63a4.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_027951_leftImg8bit_png.rf.283ec4d088ca18f8d32b44f9039eb5ad.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_028297_leftImg8bit_png.rf.c83bce1734ea69e0227ad541015fb28c.jpg  \n",
            "  inflating: /content/data/train/bochum_000000_028764_leftImg8bit_png.rf.a8e19726cd87755d1cc1bd4e22699652.jpg  \n",
            "   creating: /content/data/valid/\n",
            "  inflating: /content/data/valid/frankfurt_000000_000294_leftImg8bit_foggy_beta_0-02_png.rf.c2a0450b7bf7d3f39586ed9369db1249.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_002196_leftImg8bit_foggy_beta_0-02_png.rf.6f7dc8665913b1ac97d75abc32e1c6b0.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_009969_leftImg8bit_foggy_beta_0-02_png.rf.f1092c1728d6bfc2ee5a14e80101c80f.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_011007_leftImg8bit_foggy_beta_0-02_png.rf.f6e02d25a75ab42533435a05de19bb72.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_011074_leftImg8bit_foggy_beta_0-02_png.rf.56ab812c27f4bf8c262ed3f5f6222acb.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_013240_leftImg8bit_foggy_beta_0-02_png.rf.062243a5bab8b0bf8100380830699e75.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_015676_leftImg8bit_foggy_beta_0-02_png.rf.70d176088e6a4dc95368ee80d59c693c.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000000_019607_leftImg8bit_foggy_beta_0-02_png.rf.ceac590ad0fe853db0234ebc6ed99bfd.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_005703_leftImg8bit_foggy_beta_0-02_png.rf.1b0a3da88fb76254f03fe5c5df63f6b6.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_005898_leftImg8bit_foggy_beta_0-02_png.rf.508ffebb4c64389fecdf026f4f7e31fb.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_007973_leftImg8bit_foggy_beta_0-02_png.rf.5a6e038e836a96659f4c02ab8fd60395.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_008200_leftImg8bit_foggy_beta_0-02_png.rf.d67810c16fc5d31829740615a41144a3.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_008688_leftImg8bit_foggy_beta_0-02_png.rf.39e1f6e43688ae54b70a07ce427b16f4.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_010444_leftImg8bit_foggy_beta_0-02_png.rf.d371692fd3a44794f1bb906d2a277d6c.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_010830_leftImg8bit_foggy_beta_0-02_png.rf.ebb8813eb2a457153269e8e9e7797546.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_011715_leftImg8bit_foggy_beta_0-02_png.rf.ea56aabbc6ad6516cbe65ee69ff60d24.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_012038_leftImg8bit_foggy_beta_0-02_png.rf.06e9c543f930542bae200501a3daa97e.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_013496_leftImg8bit_foggy_beta_0-02_png.rf.a9fd161c65f206ed54a9a1ce2dd7cdf5.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_013710_leftImg8bit_foggy_beta_0-02_png.rf.4f3b3e249a39d329062006c5388c08f5.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_014565_leftImg8bit_foggy_beta_0-02_png.rf.29d612355e66c17cb172a6c267fe93cd.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_015328_leftImg8bit_foggy_beta_0-02_png.rf.7913c26d307ee3746497754ca5d1a8ff.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_015768_leftImg8bit_foggy_beta_0-02_png.rf.1a49279c7cd8448e4f6d7310ed9a246f.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_016029_leftImg8bit_foggy_beta_0-02_png.rf.ed14fdbf4b7476a85bb9e7950af6143c.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_016273_leftImg8bit_foggy_beta_0-02_png.rf.a36abc5c830faa632f050420bcc32dab.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_019698_leftImg8bit_foggy_beta_0-02_png.rf.b83de9b0883b3c13ce60ad1fb8ddcd1b.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_019854_leftImg8bit_foggy_beta_0-02_png.rf.ac46fa4c572a5351e7c08d35d2a15244.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_023369_leftImg8bit_foggy_beta_0-02_png.rf.4ccd47061d27da8c0ddffca61de6e1db.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_023769_leftImg8bit_foggy_beta_0-02_png.rf.37ecacee997b346c1aa1448c33bd5d68.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_030310_leftImg8bit_foggy_beta_0-02_png.rf.8abcb03fc0381f18801a4b45178b3a0e.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_031266_leftImg8bit_foggy_beta_0-02_png.rf.d1d8ca276f8ba4f28c81092313241116.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_032711_leftImg8bit_foggy_beta_0-02_png.rf.0aadc1deeabec14c5473a53656b104e0.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_033655_leftImg8bit_foggy_beta_0-02_png.rf.a5610aa28a4497523ec3cc4831bda826.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_035144_leftImg8bit_foggy_beta_0-02_png.rf.976d787f379ab714057690e2bb45a7bc.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_041074_leftImg8bit_foggy_beta_0-02_png.rf.548fb0eaa2d0ff693de37065abc9a2f9.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_041517_leftImg8bit_foggy_beta_0-02_png.rf.3f620f592f4d36d748c78f89ca085b0c.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_044658_leftImg8bit_foggy_beta_0-02_png.rf.3c7da5b5647de24fbc15e984a7315c8d.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_046504_leftImg8bit_foggy_beta_0-02_png.rf.63d41411923cb0da824977215f502527.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_047552_leftImg8bit_foggy_beta_0-02_png.rf.f74767f4e161a99ffc40d5b6353ee127.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_049298_leftImg8bit_foggy_beta_0-02_png.rf.34c6567ae5b5c87116f19b07dff776c1.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_050686_leftImg8bit_foggy_beta_0-02_png.rf.c4bb3ff3d9a67bd5a2bbda9d1b0fef35.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_055062_leftImg8bit_foggy_beta_0-02_png.rf.a198c51d4a93b8e54aaa472ff50a67da.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_055306_leftImg8bit_foggy_beta_0-02_png.rf.7ec0bdb833038aac7a41a81c9380f781.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_058176_leftImg8bit_foggy_beta_0-02_png.rf.500d9c87620c21e88b63eed037576fd9.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_058914_leftImg8bit_foggy_beta_0-02_png.rf.89e72270a2929ad7f023c35a7a974fdb.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_060135_leftImg8bit_foggy_beta_0-02_png.rf.9a752f294b6f7ca117b405254bd1f074.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_060422_leftImg8bit_foggy_beta_0-02_png.rf.a65b2fc9d1d8a2a320d7fcdb489dfd03.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_060906_leftImg8bit_foggy_beta_0-02_png.rf.abba856b74a69f8620b6e92a4d279d48.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_062396_leftImg8bit_foggy_beta_0-02_png.rf.d8f39d44f467a4b109c3fc57ecf27351.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_065160_leftImg8bit_foggy_beta_0-02_png.rf.3f0308cdce5b5cd900fc122826b489ff.jpg  \n",
            "  inflating: /content/data/valid/frankfurt_000001_065617_leftImg8bit_foggy_beta_0-02_png.rf.1809f7409f68761b1923313e8b40fea5.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                                #util misc.py\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "\n",
        "    # serialized to a Tensor\n",
        "    buffer = pickle.dumps(data)\n",
        "    storage = torch.ByteStorage.from_buffer(buffer)\n",
        "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
        "\n",
        "    # obtain Tensor size of each rank\n",
        "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
        "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
        "    dist.all_gather(size_list, local_size)\n",
        "    size_list = [int(size.item()) for size in size_list]\n",
        "    max_size = max(size_list)\n",
        "\n",
        "    # receiving Tensor from all ranks\n",
        "    # we pad the tensor because torch all_gather does not support\n",
        "    # gathering tensors of different shapes\n",
        "    tensor_list = []\n",
        "    for _ in size_list:\n",
        "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
        "    if local_size != max_size:\n",
        "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
        "        tensor = torch.cat((tensor, padding), dim=0)\n",
        "    dist.all_gather(tensor_list, tensor)\n",
        "\n",
        "    data_list = []\n",
        "    for size, tensor in zip(size_list, tensor_list):\n",
        "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
        "        data_list.append(pickle.loads(buffer))\n",
        "\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.no_grad():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "\n",
        "\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(\n",
        "                \"{}: {}\".format(name, str(meter))\n",
        "            )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}',\n",
        "                'max mem: {memory:.0f}'\n",
        "            ])\n",
        "        else:\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}'\n",
        "            ])\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
        "            header, total_time_str, total_time / len(iterable)))\n",
        "\n",
        "\n",
        "def get_sha():\n",
        "    cwd = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "    def _run(command):\n",
        "        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n",
        "    sha = 'N/A'\n",
        "    diff = \"clean\"\n",
        "    branch = 'N/A'\n",
        "    try:\n",
        "        sha = _run(['git', 'rev-parse', 'HEAD'])\n",
        "        subprocess.check_output(['git', 'diff'], cwd=cwd)\n",
        "        diff = _run(['git', 'diff-index', 'HEAD'])\n",
        "        diff = \"has uncommited changes\" if diff else \"clean\"\n",
        "        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n",
        "    except Exception:\n",
        "        pass\n",
        "    message = f\"sha: {sha}, status: {diff}, branch: {branch}\"\n",
        "    return message\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = list(zip(*batch))\n",
        "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
        "    return tuple(batch)\n",
        "\n",
        "\n",
        "def _max_by_axis(the_list):\n",
        "    # type: (List[List[int]]) -> List[int]\n",
        "    maxes = the_list[0]\n",
        "    for sublist in the_list[1:]:\n",
        "        for index, item in enumerate(sublist):\n",
        "            maxes[index] = max(maxes[index], item)\n",
        "    return maxes\n",
        "\n",
        "\n",
        "class NestedTensor(object):\n",
        "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
        "        self.tensors = tensors\n",
        "        self.mask = mask\n",
        "\n",
        "    def to(self, device):\n",
        "        # type: (Device) -> NestedTensor # noqa\n",
        "        cast_tensor = self.tensors.to(device)\n",
        "        mask = self.mask\n",
        "        if mask is not None:\n",
        "            assert mask is not None\n",
        "            cast_mask = mask.to(device)\n",
        "        else:\n",
        "            cast_mask = None\n",
        "        return NestedTensor(cast_tensor, cast_mask)\n",
        "\n",
        "    def decompose(self):\n",
        "        return self.tensors, self.mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.tensors)\n",
        "\n",
        "\n",
        "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
        "    # TODO make this more general\n",
        "    if tensor_list[0].ndim == 3:\n",
        "        if torchvision._is_tracing():\n",
        "            # nested_tensor_from_tensor_list() does not export well to ONNX\n",
        "            # call _onnx_nested_tensor_from_tensor_list() instead\n",
        "            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n",
        "\n",
        "        # TODO make it support different-sized images\n",
        "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
        "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
        "        batch_shape = [len(tensor_list)] + max_size\n",
        "        b, c, h, w = batch_shape\n",
        "        dtype = tensor_list[0].dtype\n",
        "        device = tensor_list[0].device\n",
        "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
        "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
        "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
        "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "            m[: img.shape[1], :img.shape[2]] = False\n",
        "    else:\n",
        "        raise ValueError('not supported')\n",
        "    return NestedTensor(tensor, mask)\n",
        "\n",
        "\n",
        "# _onnx_nested_tensor_from_tensor_list() is an implementation of\n",
        "# nested_tensor_from_tensor_list() that is supported by ONNX tracing.\n",
        "@torch.jit.unused\n",
        "def _onnx_nested_tensor_from_tensor_list(tensor_list: List[Tensor]) -> NestedTensor:\n",
        "    max_size = []\n",
        "    for i in range(tensor_list[0].dim()):\n",
        "        max_size_i = torch.max(torch.stack([img.shape[i] for img in tensor_list]).to(torch.float32)).to(torch.int64)\n",
        "        max_size.append(max_size_i)\n",
        "    max_size = tuple(max_size)\n",
        "\n",
        "    # work around for\n",
        "    # pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "    # m[: img.shape[1], :img.shape[2]] = False\n",
        "    # which is not yet supported in onnx\n",
        "    padded_imgs = []\n",
        "    padded_masks = []\n",
        "    for img in tensor_list:\n",
        "        padding = [(s1 - s2) for s1, s2 in zip(max_size, tuple(img.shape))]\n",
        "        padded_img = torch.nn.functional.pad(img, (0, padding[2], 0, padding[1], 0, padding[0]))\n",
        "        padded_imgs.append(padded_img)\n",
        "\n",
        "        m = torch.zeros_like(img[0], dtype=torch.int, device=img.device)\n",
        "        padded_mask = torch.nn.functional.pad(m, (0, padding[2], 0, padding[1]), \"constant\", 1)\n",
        "        padded_masks.append(padded_mask.to(torch.bool))\n",
        "\n",
        "    tensor = torch.stack(padded_imgs)\n",
        "    mask = torch.stack(padded_masks)\n",
        "\n",
        "    return NestedTensor(tensor, mask=mask)\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop('force', False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
        "        args.rank = int(os.environ[\"RANK\"])\n",
        "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
        "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
        "    elif 'SLURM_PROCID' in os.environ:\n",
        "        args.rank = int(os.environ['SLURM_PROCID'])\n",
        "        args.gpu = args.rank % torch.cuda.device_count()\n",
        "    else:\n",
        "        print('Not using distributed mode')\n",
        "        args.distributed = False\n",
        "        return\n",
        "\n",
        "    args.distributed = True\n",
        "\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    args.dist_backend = 'nccl'\n",
        "    print('| distributed init (rank {}): {}'.format(\n",
        "        args.rank, args.dist_url), flush=True)\n",
        "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                         world_size=args.world_size, rank=args.rank)\n",
        "    torch.distributed.barrier()\n",
        "    setup_for_distributed(args.rank == 0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    if target.numel() == 0:\n",
        "        return [torch.zeros([], device=output.device)]\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
        "    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n",
        "    \"\"\"\n",
        "    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n",
        "    This will eventually be supported natively by PyTorch, and this\n",
        "    class can go away.\n",
        "    \"\"\"\n",
        "    if version.parse(torchvision.__version__) < version.parse('0.7'):\n",
        "        if input.numel() > 0:\n",
        "            return torch.nn.functional.interpolate(\n",
        "                input, size, scale_factor, mode, align_corners\n",
        "            )\n",
        "\n",
        "        output_shape = _output_size(2, input, size, scale_factor)\n",
        "        output_shape = list(input.shape[:-2]) + list(output_shape)\n",
        "        return _new_empty_tensor(input, output_shape)\n",
        "    else:\n",
        "        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)"
      ],
      "metadata": {
        "id": "_sXoHHZpupKf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEmbeddingSine(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a more standard version of the position embedding, very similar to the one\n",
        "    used by the Attention is all you need paper, generalized to work on images.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
        "        super().__init__()\n",
        "        self.num_pos_feats = num_pos_feats\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        if scale is not None and normalize is False:\n",
        "            raise ValueError(\"normalize should be True if scale is passed\")\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        x = tensor_list.tensors\n",
        "        mask = tensor_list.mask\n",
        "        assert mask is not None\n",
        "        not_mask = ~mask\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "        if self.normalize:\n",
        "            eps = 1e-6\n",
        "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos\n",
        "\n",
        "\n",
        "class PositionEmbeddingLearned(nn.Module):\n",
        "    \"\"\"\n",
        "    Absolute pos embedding, learned.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_pos_feats=256):\n",
        "        super().__init__()\n",
        "        self.row_embed = nn.Embedding(50, num_pos_feats)\n",
        "        self.col_embed = nn.Embedding(50, num_pos_feats)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.uniform_(self.row_embed.weight)\n",
        "        nn.init.uniform_(self.col_embed.weight)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        x = tensor_list.tensors\n",
        "        h, w = x.shape[-2:]\n",
        "        i = torch.arange(w, device=x.device)\n",
        "        j = torch.arange(h, device=x.device)\n",
        "        x_emb = self.col_embed(i)\n",
        "        y_emb = self.row_embed(j)\n",
        "        pos = torch.cat([\n",
        "            x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
        "            y_emb.unsqueeze(1).repeat(1, w, 1),\n",
        "        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
        "        return pos\n",
        "\n",
        "\n",
        "def build_position_encoding(args):\n",
        "    N_steps = args.hidden_dim // 2\n",
        "    if args.position_embedding in ('v2', 'sine'):\n",
        "        # TODO find a better way of exposing other arguments\n",
        "        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "    elif args.position_embedding in ('v3', 'learned'):\n",
        "        position_embedding = PositionEmbeddingLearned(N_steps)\n",
        "    else:\n",
        "        raise ValueError(f\"not supported {args.position_embedding}\")\n",
        "\n",
        "    return position_embedding"
      ],
      "metadata": {
        "id": "z7tUSid5d-aL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FrozenBatchNorm2d(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
        "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
        "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
        "    produce nans.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n):\n",
        "        super(FrozenBatchNorm2d, self).__init__()\n",
        "        self.register_buffer(\"weight\", torch.ones(n))\n",
        "        self.register_buffer(\"bias\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_var\", torch.ones(n))\n",
        "\n",
        "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
        "                              missing_keys, unexpected_keys, error_msgs):\n",
        "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
        "        if num_batches_tracked_key in state_dict:\n",
        "            del state_dict[num_batches_tracked_key]\n",
        "\n",
        "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
        "            state_dict, prefix, local_metadata, strict,\n",
        "            missing_keys, unexpected_keys, error_msgs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # move reshapes to the beginning\n",
        "        # to make it fuser-friendly\n",
        "        w = self.weight.reshape(1, -1, 1, 1)\n",
        "        b = self.bias.reshape(1, -1, 1, 1)\n",
        "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
        "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
        "        eps = 1e-5\n",
        "        scale = w * (rv + eps).rsqrt()\n",
        "        bias = b - rm * scale\n",
        "        return x * scale + bias\n",
        "\n",
        "\n",
        "class BackboneBase(nn.Module):\n",
        "\n",
        "    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):\n",
        "        super().__init__()\n",
        "        for name, parameter in backbone.named_parameters():\n",
        "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
        "                parameter.requires_grad_(False)\n",
        "        if return_interm_layers:\n",
        "            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n",
        "        else:\n",
        "            return_layers = {'layer4': \"0\", 'layer3': \"1\"}\n",
        "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        xs = self.body(tensor_list.tensors)\n",
        "        out: Dict[str, NestedTensor] = {}\n",
        "        for name, x in xs.items():\n",
        "            m = tensor_list.mask\n",
        "            assert m is not None\n",
        "            mask = torch.nn.functional.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
        "            out[name] = NestedTensor(x, mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Backbone(BackboneBase):\n",
        "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
        "    def __init__(self, name: str,\n",
        "                 train_backbone: bool,\n",
        "                 return_interm_layers: bool,\n",
        "                 dilation: bool):\n",
        "        backbone = getattr(torchvision.models, name)(\n",
        "            replace_stride_with_dilation=[False, False, dilation],\n",
        "            pretrained=is_main_process(), norm_layer=FrozenBatchNorm2d)\n",
        "        num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n",
        "        super().__init__(backbone, train_backbone, num_channels, return_interm_layers)\n",
        "\n",
        "\n",
        "class Joiner(nn.Sequential):\n",
        "    def __init__(self, backbone, position_embedding):\n",
        "        super().__init__(backbone, position_embedding)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        xs = self[0](tensor_list)\n",
        "        out: List[NestedTensor] = []\n",
        "        pos = []\n",
        "        for name, x in xs.items():\n",
        "            out.append(x)\n",
        "            # position encoding\n",
        "            pos.append(self[1](x).to(x.tensors.dtype))\n",
        "\n",
        "        return out, pos\n",
        "\n",
        "\n",
        "def build_backbone(args):\n",
        "    position_embedding = build_position_encoding(args)\n",
        "    train_backbone = args.lr_backbone > 0\n",
        "    return_interm_layers = args.masks\n",
        "    backbone = Backbone(args.backbone, train_backbone, return_interm_layers, args.dilation)\n",
        "    model = Joiner(backbone, position_embedding)\n",
        "    model.num_channels = backbone.num_channels\n",
        "    return model"
      ],
      "metadata": {
        "id": "iZbz9oRtdxbW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(-1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "\n",
        "def box_xyxy_to_cxcywh(x):\n",
        "    x0, y0, x1, y1 = x.unbind(-1)\n",
        "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
        "         (x1 - x0), (y1 - y0)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "\n",
        "# modified from torchvision to also return the union\n",
        "def box_iou(boxes1, boxes2):\n",
        "    area1 = box_area(boxes1)\n",
        "    area2 = box_area(boxes2)\n",
        "\n",
        "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
        "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "\n",
        "    union = area1[:, None] + area2 - inter\n",
        "\n",
        "    iou = inter / union\n",
        "    return iou, union\n",
        "\n",
        "\n",
        "def generalized_box_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    Generalized IoU from https://giou.stanford.edu/\n",
        "    The boxes should be in [x0, y0, x1, y1] format\n",
        "    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
        "    and M = len(boxes2)\n",
        "    \"\"\"\n",
        "    # degenerate boxes gives inf / nan results\n",
        "    # so do an early check\n",
        "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
        "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
        "    iou, union = box_iou(boxes1, boxes2)\n",
        "\n",
        "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    area = wh[:, :, 0] * wh[:, :, 1]\n",
        "\n",
        "    return iou - (area - union) / area\n",
        "\n",
        "\n",
        "def masks_to_boxes(masks):\n",
        "    \"\"\"Compute the bounding boxes around the provided masks\n",
        "    The masks should be in format [N, H, W] where N is the number of masks, (H, W) are the spatial dimensions.\n",
        "    Returns a [N, 4] tensors, with the boxes in xyxy format\n",
        "    \"\"\"\n",
        "    if masks.numel() == 0:\n",
        "        return torch.zeros((0, 4), device=masks.device)\n",
        "\n",
        "    h, w = masks.shape[-2:]\n",
        "\n",
        "    y = torch.arange(0, h, dtype=torch.float)\n",
        "    x = torch.arange(0, w, dtype=torch.float)\n",
        "    y, x = torch.meshgrid(y, x)\n",
        "\n",
        "    x_mask = (masks * x.unsqueeze(0))\n",
        "    x_max = x_mask.flatten(1).max(-1)[0]\n",
        "    x_min = x_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
        "\n",
        "    y_mask = (masks * y.unsqueeze(0))\n",
        "    y_max = y_mask.flatten(1).max(-1)[0]\n",
        "    y_min = y_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]\n",
        "\n",
        "    return torch.stack([x_min, y_min, x_max, y_max], 1)"
      ],
      "metadata": {
        "id": "_9wRuXfUebzm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
        "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
        "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
        "    while the others are un-matched (and thus treated as non-objects).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n",
        "        \"\"\"Creates the matcher\n",
        "        Params:\n",
        "            cost_class: This is the relative weight of the classification error in the matching cost\n",
        "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
        "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" Performs the matching\n",
        "        Params:\n",
        "            outputs: This is a dict that contains at least these entries:\n",
        "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
        "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
        "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
        "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
        "                           objects in the target) containing the class labels\n",
        "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
        "        Returns:\n",
        "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
        "                - index_i is the indices of the selected predictions (in order)\n",
        "                - index_j is the indices of the corresponding selected targets (in order)\n",
        "            For each batch element, it holds:\n",
        "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
        "        \"\"\"\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "\n",
        "        # We flatten to compute the cost matrices in a batch\n",
        "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
        "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
        "\n",
        "        # Also concat the target labels and boxes\n",
        "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
        "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
        "\n",
        "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
        "        # but approximate it in 1 - proba[target class].\n",
        "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
        "        cost_class = -out_prob[:, tgt_ids]\n",
        "\n",
        "        # Compute the L1 cost between boxes\n",
        "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
        "\n",
        "        # Compute the giou cost betwen boxes\n",
        "        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))\n",
        "\n",
        "        # Final cost matrix\n",
        "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "\n",
        "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
        "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "\n",
        "\n",
        "def build_matcher(args):\n",
        "    return HungarianMatcher(cost_class=args.set_cost_class, cost_bbox=args.set_cost_bbox, cost_giou=args.set_cost_giou)"
      ],
      "metadata": {
        "id": "r40lsWnJeNEc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from panopticapi.utils import id2rgb, rgb2id\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "\n",
        "class DETRsegm(nn.Module):\n",
        "    def __init__(self, detr, freeze_detr=False):\n",
        "        super().__init__()\n",
        "        self.detr = detr\n",
        "\n",
        "        if freeze_detr:\n",
        "            for p in self.parameters():\n",
        "                p.requires_grad_(False)\n",
        "\n",
        "        hidden_dim, nheads = detr.transformer.d_model, detr.transformer.nhead\n",
        "        self.bbox_attention = MHAttentionMap(hidden_dim, hidden_dim, nheads, dropout=0.0)\n",
        "        self.mask_head = MaskHeadSmallConv(hidden_dim + nheads, [1024, 512, 256], hidden_dim)\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        if isinstance(samples, (list, torch.Tensor)):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        features, pos = self.detr.backbone(samples)\n",
        "\n",
        "        bs = features[-1].tensors.shape[0]\n",
        "\n",
        "        src, mask = features[-1].decompose()\n",
        "        assert mask is not None\n",
        "        src_proj = self.detr.input_proj(src)\n",
        "        hs, memory = self.detr.transformer(src_proj, mask, self.detr.query_embed.weight, pos[-1])\n",
        "\n",
        "        outputs_class = self.detr.class_embed(hs)\n",
        "        outputs_coord = self.detr.bbox_embed(hs).sigmoid()\n",
        "        out = {\"pred_logits\": outputs_class[-1], \"pred_boxes\": outputs_coord[-1]}\n",
        "        if self.detr.aux_loss:\n",
        "            out['aux_outputs'] = self.detr._set_aux_loss(outputs_class, outputs_coord)\n",
        "\n",
        "        # FIXME h_boxes takes the last one computed, keep this in mind\n",
        "        bbox_mask = self.bbox_attention(hs[-1], memory, mask=mask)\n",
        "\n",
        "        seg_masks = self.mask_head(src_proj, bbox_mask, [features[2].tensors, features[1].tensors, features[0].tensors])\n",
        "        outputs_seg_masks = seg_masks.view(bs, self.detr.num_queries, seg_masks.shape[-2], seg_masks.shape[-1])\n",
        "\n",
        "        out[\"pred_masks\"] = outputs_seg_masks\n",
        "        return out\n",
        "\n",
        "\n",
        "def _expand(tensor, length: int):\n",
        "    return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)\n",
        "\n",
        "\n",
        "class MaskHeadSmallConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple convolutional head, using group norm.\n",
        "    Upsampling is done using a FPN approach\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, fpn_dims, context_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]\n",
        "        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
        "        self.gn1 = torch.nn.GroupNorm(8, dim)\n",
        "        self.lay2 = torch.nn.Conv2d(dim, inter_dims[1], 3, padding=1)\n",
        "        self.gn2 = torch.nn.GroupNorm(8, inter_dims[1])\n",
        "        self.lay3 = torch.nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)\n",
        "        self.gn3 = torch.nn.GroupNorm(8, inter_dims[2])\n",
        "        self.lay4 = torch.nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)\n",
        "        self.gn4 = torch.nn.GroupNorm(8, inter_dims[3])\n",
        "        self.lay5 = torch.nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)\n",
        "        self.gn5 = torch.nn.GroupNorm(8, inter_dims[4])\n",
        "        self.out_lay = torch.nn.Conv2d(inter_dims[4], 1, 3, padding=1)\n",
        "\n",
        "        self.dim = dim\n",
        "\n",
        "        self.adapter1 = torch.nn.Conv2d(fpn_dims[0], inter_dims[1], 1)\n",
        "        self.adapter2 = torch.nn.Conv2d(fpn_dims[1], inter_dims[2], 1)\n",
        "        self.adapter3 = torch.nn.Conv2d(fpn_dims[2], inter_dims[3], 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor, bbox_mask: Tensor, fpns: List[Tensor]):\n",
        "        x = torch.cat([_expand(x, bbox_mask.shape[1]), bbox_mask.flatten(0, 1)], 1)\n",
        "\n",
        "        x = self.lay1(x)\n",
        "        x = self.gn1(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.lay2(x)\n",
        "        x = self.gn2(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        cur_fpn = self.adapter1(fpns[0])\n",
        "        if cur_fpn.size(0) != x.size(0):\n",
        "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
        "        x = cur_fpn + torch.nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "        x = self.lay3(x)\n",
        "        x = self.gn3(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        cur_fpn = self.adapter2(fpns[1])\n",
        "        if cur_fpn.size(0) != x.size(0):\n",
        "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
        "        x = cur_fpn + torch.nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "        x = self.lay4(x)\n",
        "        x = self.gn4(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        cur_fpn = self.adapter3(fpns[2])\n",
        "        if cur_fpn.size(0) != x.size(0):\n",
        "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
        "        x = cur_fpn + torch.nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
        "        x = self.lay5(x)\n",
        "        x = self.gn5(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        x = self.out_lay(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MHAttentionMap(nn.Module):\n",
        "    \"\"\"This is a 2D attention module, which only returns the attention softmax (no multiplication by value)\"\"\"\n",
        "\n",
        "    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0.0, bias=True):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
        "        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
        "\n",
        "        nn.init.zeros_(self.k_linear.bias)\n",
        "        nn.init.zeros_(self.q_linear.bias)\n",
        "        nn.init.xavier_uniform_(self.k_linear.weight)\n",
        "        nn.init.xavier_uniform_(self.q_linear.weight)\n",
        "        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5\n",
        "\n",
        "    def forward(self, q, k, mask: Optional[Tensor] = None):\n",
        "        q = self.q_linear(q)\n",
        "        k = torch.nn.functional.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)\n",
        "        qh = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)\n",
        "        kh = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])\n",
        "        weights = torch.einsum(\"bqnc,bnchw->bqnhw\", qh * self.normalize_fact, kh)\n",
        "\n",
        "        if mask is not None:\n",
        "            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
        "        weights = torch.nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())\n",
        "        weights = self.dropout(weights)\n",
        "        return weights\n",
        "\n",
        "\n",
        "def dice_loss(inputs, targets, num_boxes):\n",
        "    \"\"\"\n",
        "    Compute the DICE loss, similar to generalized IOU for masks\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "    \"\"\"\n",
        "    inputs = inputs.sigmoid()\n",
        "    inputs = inputs.flatten(1)\n",
        "    numerator = 2 * (inputs * targets).sum(1)\n",
        "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
        "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
        "    return loss.sum() / num_boxes\n",
        "\n",
        "\n",
        "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n",
        "    \"\"\"\n",
        "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
        "                positive vs negative examples. Default = -1 (no weighting).\n",
        "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
        "               balance easy vs hard examples.\n",
        "    Returns:\n",
        "        Loss tensor\n",
        "    \"\"\"\n",
        "    prob = inputs.sigmoid()\n",
        "    ce_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
        "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    return loss.mean(1).sum() / num_boxes\n",
        "\n",
        "\n",
        "class PostProcessSegm(nn.Module):\n",
        "    def __init__(self, threshold=0.5):\n",
        "        super().__init__()\n",
        "        self.threshold = threshold\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, results, outputs, orig_target_sizes, max_target_sizes):\n",
        "        assert len(orig_target_sizes) == len(max_target_sizes)\n",
        "        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n",
        "        outputs_masks = outputs[\"pred_masks\"].squeeze(2)\n",
        "        outputs_masks = torch.nn.functional.interpolate(outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False)\n",
        "        outputs_masks = (outputs_masks.sigmoid() > self.threshold).cpu()\n",
        "\n",
        "        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n",
        "            img_h, img_w = t[0], t[1]\n",
        "            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n",
        "            results[i][\"masks\"] = torch.nn.functional.interpolate(\n",
        "                results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\"\n",
        "            ).byte()\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class PostProcessPanoptic(nn.Module):\n",
        "    \"\"\"This class converts the output of the model to the final panoptic result, in the format expected by the\n",
        "    coco panoptic API \"\"\"\n",
        "\n",
        "    def __init__(self, is_thing_map, threshold=0.85):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "           is_thing_map: This is a whose keys are the class ids, and the values a boolean indicating whether\n",
        "                          the class is  a thing (True) or a stuff (False) class\n",
        "           threshold: confidence threshold: segments with confidence lower than this will be deleted\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.threshold = threshold\n",
        "        self.is_thing_map = is_thing_map\n",
        "\n",
        "    def forward(self, outputs, processed_sizes, target_sizes=None):\n",
        "        \"\"\" This function computes the panoptic prediction from the model's predictions.\n",
        "        Parameters:\n",
        "            outputs: This is a dict coming directly from the model. See the model doc for the content.\n",
        "            processed_sizes: This is a list of tuples (or torch tensors) of sizes of the images that were passed to the\n",
        "                             model, ie the size after data augmentation but before batching.\n",
        "            target_sizes: This is a list of tuples (or torch tensors) corresponding to the requested final size\n",
        "                          of each prediction. If left to None, it will default to the processed_sizes\n",
        "            \"\"\"\n",
        "        if target_sizes is None:\n",
        "            target_sizes = processed_sizes\n",
        "        assert len(processed_sizes) == len(target_sizes)\n",
        "        out_logits, raw_masks, raw_boxes = outputs[\"pred_logits\"], outputs[\"pred_masks\"], outputs[\"pred_boxes\"]\n",
        "        assert len(out_logits) == len(raw_masks) == len(target_sizes)\n",
        "        preds = []\n",
        "\n",
        "        def to_tuple(tup):\n",
        "            if isinstance(tup, tuple):\n",
        "                return tup\n",
        "            return tuple(tup.cpu().tolist())\n",
        "\n",
        "        for cur_logits, cur_masks, cur_boxes, size, target_size in zip(\n",
        "            out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes\n",
        "        ):\n",
        "            # we filter empty queries and detection below threshold\n",
        "            scores, labels = cur_logits.softmax(-1).max(-1)\n",
        "            keep = labels.ne(outputs[\"pred_logits\"].shape[-1] - 1) & (scores > self.threshold)\n",
        "            cur_scores, cur_classes = cur_logits.softmax(-1).max(-1)\n",
        "            cur_scores = cur_scores[keep]\n",
        "            cur_classes = cur_classes[keep]\n",
        "            cur_masks = cur_masks[keep]\n",
        "            cur_masks = interpolate(cur_masks[:, None], to_tuple(size), mode=\"bilinear\").squeeze(1)\n",
        "            cur_boxes = box_cxcywh_to_xyxy(cur_boxes[keep])\n",
        "\n",
        "            h, w = cur_masks.shape[-2:]\n",
        "            assert len(cur_boxes) == len(cur_classes)\n",
        "\n",
        "            # It may be that we have several predicted masks for the same stuff class.\n",
        "            # In the following, we track the list of masks ids for each stuff class (they are merged later on)\n",
        "            cur_masks = cur_masks.flatten(1)\n",
        "            stuff_equiv_classes = defaultdict(lambda: [])\n",
        "            for k, label in enumerate(cur_classes):\n",
        "                if not self.is_thing_map[label.item()]:\n",
        "                    stuff_equiv_classes[label.item()].append(k)\n",
        "\n",
        "            def get_ids_area(masks, scores, dedup=False):\n",
        "                # This helper function creates the final panoptic segmentation image\n",
        "                # It also returns the area of the masks that appears on the image\n",
        "\n",
        "                m_id = masks.transpose(0, 1).softmax(-1)\n",
        "\n",
        "                if m_id.shape[-1] == 0:\n",
        "                    # We didn't detect any mask :(\n",
        "                    m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)\n",
        "                else:\n",
        "                    m_id = m_id.argmax(-1).view(h, w)\n",
        "\n",
        "                if dedup:\n",
        "                    # Merge the masks corresponding to the same stuff class\n",
        "                    for equiv in stuff_equiv_classes.values():\n",
        "                        if len(equiv) > 1:\n",
        "                            for eq_id in equiv:\n",
        "                                m_id.masked_fill_(m_id.eq(eq_id), equiv[0])\n",
        "\n",
        "                final_h, final_w = to_tuple(target_size)\n",
        "\n",
        "                seg_img = Image.fromarray(id2rgb(m_id.view(h, w).cpu().numpy()))\n",
        "                seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)\n",
        "\n",
        "                np_seg_img = (\n",
        "                    torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes())).view(final_h, final_w, 3).numpy()\n",
        "                )\n",
        "                m_id = torch.from_numpy(rgb2id(np_seg_img))\n",
        "\n",
        "                area = []\n",
        "                for i in range(len(scores)):\n",
        "                    area.append(m_id.eq(i).sum().item())\n",
        "                return area, seg_img\n",
        "\n",
        "            area, seg_img = get_ids_area(cur_masks, cur_scores, dedup=True)\n",
        "            if cur_classes.numel() > 0:\n",
        "                # We know filter empty masks as long as we find some\n",
        "                while True:\n",
        "                    filtered_small = torch.as_tensor(\n",
        "                        [area[i] <= 4 for i, c in enumerate(cur_classes)], dtype=torch.bool, device=keep.device\n",
        "                    )\n",
        "                    if filtered_small.any().item():\n",
        "                        cur_scores = cur_scores[~filtered_small]\n",
        "                        cur_classes = cur_classes[~filtered_small]\n",
        "                        cur_masks = cur_masks[~filtered_small]\n",
        "                        area, seg_img = get_ids_area(cur_masks, cur_scores)\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "            else:\n",
        "                cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)\n",
        "\n",
        "            segments_info = []\n",
        "            for i, a in enumerate(area):\n",
        "                cat = cur_classes[i].item()\n",
        "                segments_info.append({\"id\": i, \"isthing\": self.is_thing_map[cat], \"category_id\": cat, \"area\": a})\n",
        "            del cur_classes\n",
        "\n",
        "            with io.BytesIO() as out:\n",
        "                seg_img.save(out, format=\"PNG\")\n",
        "                predictions = {\"png_string\": out.getvalue(), \"segments_info\": segments_info}\n",
        "            preds.append(predictions)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "s35pEGzafkaI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n",
        "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False,\n",
        "                 return_intermediate_dec=False):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
        "                                                dropout, activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
        "                                                dropout, activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n",
        "                                          return_intermediate=return_intermediate_dec)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, mask, query_embed, pos_embed):\n",
        "        # flatten NxCxHxW to HWxNxC\n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "        mask = mask.flatten(1)\n",
        "\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n",
        "                          pos=pos_embed, query_pos=query_embed)\n",
        "        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, src,\n",
        "                mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        output = src\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, src_mask=mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self.return_intermediate = return_intermediate\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        output = tgt\n",
        "\n",
        "        intermediate = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, memory, tgt_mask=tgt_mask,\n",
        "                           memory_mask=memory_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask,\n",
        "                           pos=pos, query_pos=query_pos)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.append(self.norm(output))\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.pop()\n",
        "                intermediate.append(output)\n",
        "\n",
        "        if self.return_intermediate:\n",
        "            return torch.stack(intermediate)\n",
        "\n",
        "        return output.unsqueeze(0)\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self,\n",
        "                     src,\n",
        "                     src_mask: Optional[Tensor] = None,\n",
        "                     src_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(self, src,\n",
        "                    src_mask: Optional[Tensor] = None,\n",
        "                    src_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None):\n",
        "        src2 = self.norm1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src2 = self.norm2(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(self, src,\n",
        "                src_mask: Optional[Tensor] = None,\n",
        "                src_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
        "                 activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(self, tgt, memory,\n",
        "                     tgt_mask: Optional[Tensor] = None,\n",
        "                     memory_mask: Optional[Tensor] = None,\n",
        "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                     pos: Optional[Tensor] = None,\n",
        "                     query_pos: Optional[Tensor] = None):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(self, tgt, memory,\n",
        "                    tgt_mask: Optional[Tensor] = None,\n",
        "                    memory_mask: Optional[Tensor] = None,\n",
        "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                    pos: Optional[Tensor] = None,\n",
        "                    query_pos: Optional[Tensor] = None):\n",
        "        tgt2 = self.norm1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt2 = self.norm2(tgt)\n",
        "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
        "                                   key=self.with_pos_embed(memory, pos),\n",
        "                                   value=memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt2 = self.norm3(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(self, tgt, memory,\n",
        "                tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[Tensor] = None,\n",
        "                pos: Optional[Tensor] = None,\n",
        "                query_pos: Optional[Tensor] = None):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
        "                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
        "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
        "                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def build_transformer(args):\n",
        "    return Transformer(\n",
        "        d_model=args.hidden_dim,\n",
        "        dropout=args.dropout,\n",
        "        nhead=args.nheads,\n",
        "        dim_feedforward=args.dim_feedforward,\n",
        "        num_encoder_layers=args.enc_layers,\n",
        "        num_decoder_layers=args.dec_layers,\n",
        "        normalize_before=args.pre_norm,\n",
        "        return_intermediate_dec=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"relu\":\n",
        "        return torch.nn.functional.relu\n",
        "    if activation == \"gelu\":\n",
        "        return torch.nn.functional.gelu\n",
        "    if activation == \"glu\":\n",
        "        return torch.nn.functional.glu\n",
        "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")"
      ],
      "metadata": {
        "id": "iBVuKFedfv5a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "# Autograd Function objects are what record operation history on tensors,\n",
        "# and define formulas for the forward and backprop.\n",
        "\n",
        "class GradientReversalFn(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        # Store context for backprop\n",
        "        ctx.alpha = alpha\n",
        "        \n",
        "        # Forward pass is a no-op\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Backward pass is just to -alpha the gradient\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        # Must return same number as inputs to forward()\n",
        "        return output, None"
      ],
      "metadata": {
        "id": "GgySY6tH3uGa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DETR(nn.Module):\n",
        "    \"\"\" This is the DETR module that performs object detection \"\"\"\n",
        "    def __init__(self, backbone, transformer, num_classes, num_queries, aux_loss=False):\n",
        "        \"\"\" Initializes the model.\n",
        "        Parameters:\n",
        "            backbone: torch module of the backbone to be used. See backbone.py\n",
        "            transformer: torch module of the transformer architecture. See transformer.py\n",
        "            num_classes: number of object classes\n",
        "            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
        "                         DETR can detect in a single image. For COCO, we recommend 100 queries.\n",
        "            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.d_model\n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n",
        "        self.backbone = backbone\n",
        "        self.aux_loss = aux_loss\n",
        "        self.num_channels_backbone = backbone.num_channels\n",
        "        self.domain_classifier1 = nn.Sequential(\n",
        "            nn.Conv2d(2048, 1024, kernel_size=1, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(19*19, 100), nn.BatchNorm1d(100),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(100, 2),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "        self.domain_classifier2 = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
        "            nn.Conv2d(256, 1, kernel_size=1, stride=1, padding=1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32*32, 100), nn.BatchNorm1d(100),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(100, 2),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        \"\"\" The forward expects a NestedTensor, which consists of:\n",
        "               - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
        "               - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n",
        "            It returns a dict with the following elements:\n",
        "               - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
        "                                Shape= [batch_size x num_queries x (num_classes + 1)]\n",
        "               - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n",
        "                               (center_x, center_y, height, width). These values are normalized in [0, 1],\n",
        "                               relative to the size of each individual image (disregarding possible padding).\n",
        "                               See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
        "               - \"aux_outputs\": Optional, only returned when auxilary losses are activated. It is a list of\n",
        "                                dictionnaries containing the two above keys for each decoder layer.\n",
        "        \"\"\"\n",
        "        if isinstance(samples, (list, torch.Tensor)):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        features, pos = self.backbone(samples)\n",
        "\n",
        "        src, mask = features[-1].decompose()\n",
        "        assert mask is not None\n",
        "        src1=src\n",
        "        # print(\"dim after resnet \", src.size())\n",
        "        rev_features1 = GradientReversalFn.apply(src1, 0.1)\n",
        "\n",
        "        src2, mask2 = features[0].decompose()\n",
        "        assert mask2 is not None\n",
        "        # print(\"dim after resnet \", src2.size())                         #h/16+6\n",
        "        rev_features2 = GradientReversalFn.apply(src2, 0.1)\n",
        "\n",
        "        domain_pred1 = self.domain_classifier1(rev_features1)\n",
        "        domain_pred2 = self.domain_classifier2(rev_features2)\n",
        "        # print(\"domain_pred1\", domain_pred1)\n",
        "        # print(\"domain_pred2\", domain_pred2)\n",
        "\n",
        "\n",
        "        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n",
        "\n",
        "        outputs_class = self.class_embed(hs)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n",
        "        if self.aux_loss:\n",
        "            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n",
        "\n",
        "        return out, domain_pred1, domain_pred2\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def _set_aux_loss(self, outputs_class, outputs_coord):\n",
        "        # this is a workaround to make torchscript happy, as torchscript\n",
        "        # doesn't support dictionary with non-homogeneous values, such\n",
        "        # as a dict having both a Tensor and a list.\n",
        "        return [{'pred_logits': a, 'pred_boxes': b}\n",
        "                for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n",
        "\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\" This class computes the loss for DETR.\n",
        "    The process happens in two steps:\n",
        "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
        "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
        "        \"\"\" Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of object categories, omitting the special no-object category\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
        "            eos_coef: relative classification weight applied to the no-object category\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer('empty_weight', empty_weight)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
        "        \"\"\"\n",
        "        assert 'pred_logits' in outputs\n",
        "        src_logits = outputs['pred_logits']\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                    dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        loss_ce = torch.nn.functional.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
        "        losses = {'loss_ce': loss_ce}\n",
        "\n",
        "        if log:\n",
        "            # TODO this should probably be a separate loss, not hacked in this one here\n",
        "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
        "        return losses\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
        "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
        "        \"\"\"\n",
        "        pred_logits = outputs['pred_logits']\n",
        "        device = pred_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
        "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
        "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = torch.nn.functional.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {'cardinality_error': card_err}\n",
        "        return losses\n",
        "\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
        "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
        "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
        "        \"\"\"\n",
        "        assert 'pred_boxes' in outputs\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_bbox = torch.nn.functional.l1_loss(src_boxes, target_boxes, reduction='none')\n",
        "\n",
        "        losses = {}\n",
        "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
        "\n",
        "        loss_giou = 1 - torch.diag(generalized_box_iou(\n",
        "            box_cxcywh_to_xyxy(src_boxes),\n",
        "            box_cxcywh_to_xyxy(target_boxes)))\n",
        "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
        "        return losses\n",
        "\n",
        "    def loss_masks(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
        "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
        "        \"\"\"\n",
        "        assert \"pred_masks\" in outputs\n",
        "\n",
        "        src_idx = self._get_src_permutation_idx(indices)\n",
        "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
        "        src_masks = outputs[\"pred_masks\"]\n",
        "        src_masks = src_masks[src_idx]\n",
        "        masks = [t[\"masks\"] for t in targets]\n",
        "        # TODO use valid to mask invalid areas due to padding in loss\n",
        "        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
        "        target_masks = target_masks.to(src_masks)\n",
        "        target_masks = target_masks[tgt_idx]\n",
        "\n",
        "        # upsample predictions to the target size\n",
        "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
        "                                mode=\"bilinear\", align_corners=False)\n",
        "        src_masks = src_masks[:, 0].flatten(1)\n",
        "\n",
        "        target_masks = target_masks.flatten(1)\n",
        "        target_masks = target_masks.view(src_masks.shape)\n",
        "        losses = {\n",
        "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
        "            \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
        "        }\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            'labels': self.loss_labels,\n",
        "            'cardinality': self.loss_cardinality,\n",
        "            'boxes': self.loss_boxes,\n",
        "            'masks': self.loss_masks\n",
        "        }\n",
        "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
        "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
        "\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs_without_aux, targets)\n",
        "\n",
        "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "        if is_dist_avail_and_initialized():\n",
        "            torch.distributed.all_reduce(num_boxes)\n",
        "        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()\n",
        "\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
        "        if 'aux_outputs' in outputs:\n",
        "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
        "                indices = self.matcher(aux_outputs, targets)\n",
        "                for loss in self.losses:\n",
        "                    if loss == 'masks':\n",
        "                        # Intermediate masks losses are too costly to compute, we ignore them.\n",
        "                        continue\n",
        "                    kwargs = {}\n",
        "                    if loss == 'labels':\n",
        "                        # Logging is enabled only for the last layer\n",
        "                        kwargs = {'log': False}\n",
        "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)\n",
        "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
        "                    losses.update(l_dict)\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "class PostProcess(nn.Module):\n",
        "    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, target_sizes):\n",
        "        \"\"\" Perform the computation\n",
        "        Parameters:\n",
        "            outputs: raw outputs of the model\n",
        "            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n",
        "                          For evaluation, this must be the original image size (before any data augmentation)\n",
        "                          For visualization, this should be the image size after data augment, but before padding\n",
        "        \"\"\"\n",
        "        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']\n",
        "\n",
        "        assert len(out_logits) == len(target_sizes)\n",
        "        assert target_sizes.shape[1] == 2\n",
        "\n",
        "        prob = torch.nn.functional.softmax(out_logits, -1)\n",
        "        scores, labels = prob[..., :-1].max(-1)\n",
        "\n",
        "        # convert to [x0, y0, x1, y1] format\n",
        "        boxes = box_cxcywh_to_xyxy(out_bbox)\n",
        "        # and from relative [0, 1] to absolute [0, height] coordinates\n",
        "        img_h, img_w = target_sizes.unbind(1)\n",
        "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
        "        boxes = boxes * scale_fct[:, None, :]\n",
        "\n",
        "        results = [{'scores': s, 'labels': l, 'boxes': b} for s, l, b in zip(scores, labels, boxes)]\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = torch.nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def build(args):\n",
        "    # the `num_classes` naming here is somewhat misleading.\n",
        "    # it indeed corresponds to `max_obj_id + 1`, where max_obj_id\n",
        "    # is the maximum id for a class in your dataset. For example,\n",
        "    # COCO has a max_obj_id of 90, so we pass `num_classes` to be 91.\n",
        "    # As another example, for a dataset that has a single class with id 1,\n",
        "    # you should pass `num_classes` to be 2 (max_obj_id + 1).\n",
        "    # For more details on this, check the following discussion\n",
        "    # https://github.com/facebookresearch/detr/issues/108#issuecomment-650269223\n",
        "    num_classes = 20 if args.dataset_file != 'coco' else 9\n",
        "    if args.dataset_file == \"coco_panoptic\":\n",
        "        # for panoptic, we just add a num_classes that is large enough to hold\n",
        "        # max_obj_id + 1, but the exact value doesn't really matter\n",
        "        num_classes = 150\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    backbone = build_backbone(args)\n",
        "\n",
        "    transformer = build_transformer(args)\n",
        "\n",
        "    model = DETR(\n",
        "        backbone,\n",
        "        transformer,\n",
        "        num_classes=num_classes,\n",
        "        num_queries=args.num_queries,\n",
        "        aux_loss=args.aux_loss,\n",
        "    )\n",
        "    if args.masks:\n",
        "        model = DETRsegm(model, freeze_detr=(args.frozen_weights is not None))\n",
        "    matcher = build_matcher(args)\n",
        "    weight_dict = {'loss_ce': 1, 'loss_bbox': args.bbox_loss_coef}\n",
        "    weight_dict['loss_giou'] = args.giou_loss_coef\n",
        "    if args.masks:\n",
        "        weight_dict[\"loss_mask\"] = args.mask_loss_coef\n",
        "        weight_dict[\"loss_dice\"] = args.dice_loss_coef\n",
        "    # TODO this is a hack\n",
        "    if args.aux_loss:\n",
        "        aux_weight_dict = {}\n",
        "        for i in range(args.dec_layers - 1):\n",
        "            aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})\n",
        "        weight_dict.update(aux_weight_dict)\n",
        "\n",
        "    losses = ['labels', 'boxes', 'cardinality']\n",
        "    if args.masks:\n",
        "        losses += [\"masks\"]\n",
        "    criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict,\n",
        "                             eos_coef=args.eos_coef, losses=losses)\n",
        "    criterion.to(device)\n",
        "    postprocessors = {'bbox': PostProcess()}\n",
        "    if args.masks:\n",
        "        postprocessors['segm'] = PostProcessSegm()\n",
        "        if args.dataset_file == \"coco_panoptic\":\n",
        "            is_thing_map = {i: i <= 90 for i in range(201)}\n",
        "            postprocessors[\"panoptic\"] = PostProcessPanoptic(is_thing_map, threshold=0.85)\n",
        "\n",
        "    return model, criterion, postprocessors"
      ],
      "metadata": {
        "id": "4krKVB3edRMJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(args):\n",
        "    return build(args)"
      ],
      "metadata": {
        "id": "JJbCcgswvAJx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.functional as F\n",
        "\n",
        "def crop(image, target, region):\n",
        "    cropped_image = F.crop(image, *region)\n",
        "\n",
        "    target = target.copy()\n",
        "    i, j, h, w = region\n",
        "\n",
        "    # should we do something wrt the original size?\n",
        "    target[\"size\"] = torch.tensor([h, w])\n",
        "\n",
        "    fields = [\"labels\", \"area\", \"iscrowd\"]\n",
        "\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        max_size = torch.as_tensor([w, h], dtype=torch.float32)\n",
        "        cropped_boxes = boxes - torch.as_tensor([j, i, j, i])\n",
        "        cropped_boxes = torch.min(cropped_boxes.reshape(-1, 2, 2), max_size)\n",
        "        cropped_boxes = cropped_boxes.clamp(min=0)\n",
        "        area = (cropped_boxes[:, 1, :] - cropped_boxes[:, 0, :]).prod(dim=1)\n",
        "        target[\"boxes\"] = cropped_boxes.reshape(-1, 4)\n",
        "        target[\"area\"] = area\n",
        "        fields.append(\"boxes\")\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        # FIXME should we update the area here if there are no boxes?\n",
        "        target['masks'] = target['masks'][:, i:i + h, j:j + w]\n",
        "        fields.append(\"masks\")\n",
        "\n",
        "    # remove elements for which the boxes or masks that have zero area\n",
        "    if \"boxes\" in target or \"masks\" in target:\n",
        "        # favor boxes selection when defining which elements to keep\n",
        "        # this is compatible with previous implementation\n",
        "        if \"boxes\" in target:\n",
        "            cropped_boxes = target['boxes'].reshape(-1, 2, 2)\n",
        "            keep = torch.all(cropped_boxes[:, 1, :] > cropped_boxes[:, 0, :], dim=1)\n",
        "        else:\n",
        "            keep = target['masks'].flatten(1).any(1)\n",
        "\n",
        "        for field in fields:\n",
        "            target[field] = target[field][keep]\n",
        "\n",
        "    return cropped_image, target\n",
        "\n",
        "\n",
        "def hflip(image, target):\n",
        "    flipped_image = F.hflip(image)\n",
        "\n",
        "    w, h = image.size\n",
        "\n",
        "    target = target.copy()\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n",
        "        target[\"boxes\"] = boxes\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = target['masks'].flip(-1)\n",
        "\n",
        "    return flipped_image, target\n",
        "\n",
        "\n",
        "def resize(image, target, size, max_size=None):\n",
        "    # size can be min_size (scalar) or (w, h) tuple\n",
        "\n",
        "    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n",
        "        w, h = image_size\n",
        "        if max_size is not None:\n",
        "            min_original_size = float(min((w, h)))\n",
        "            max_original_size = float(max((w, h)))\n",
        "            if max_original_size / min_original_size * size > max_size:\n",
        "                size = int(round(max_size * min_original_size / max_original_size))\n",
        "\n",
        "        if (w <= h and w == size) or (h <= w and h == size):\n",
        "            return (h, w)\n",
        "\n",
        "        if w < h:\n",
        "            ow = size\n",
        "            oh = int(size * h / w)\n",
        "        else:\n",
        "            oh = size\n",
        "            ow = int(size * w / h)\n",
        "\n",
        "        return (oh, ow)\n",
        "\n",
        "    def get_size(image_size, size, max_size=None):\n",
        "        if isinstance(size, (list, tuple)):\n",
        "            return size[::-1]\n",
        "        else:\n",
        "            return get_size_with_aspect_ratio(image_size, size, max_size)\n",
        "\n",
        "    size = get_size(image.size, size, max_size)\n",
        "    rescaled_image = F.resize(image, size)\n",
        "\n",
        "    if target is None:\n",
        "        return rescaled_image, None\n",
        "\n",
        "    ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(rescaled_image.size, image.size))\n",
        "    ratio_width, ratio_height = ratios\n",
        "\n",
        "    target = target.copy()\n",
        "    if \"boxes\" in target:\n",
        "        boxes = target[\"boxes\"]\n",
        "        scaled_boxes = boxes * torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height])\n",
        "        target[\"boxes\"] = scaled_boxes\n",
        "\n",
        "    if \"area\" in target:\n",
        "        area = target[\"area\"]\n",
        "        scaled_area = area * (ratio_width * ratio_height)\n",
        "        target[\"area\"] = scaled_area\n",
        "\n",
        "    h, w = size\n",
        "    target[\"size\"] = torch.tensor([h, w])\n",
        "\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = interpolate(\n",
        "            target['masks'][:, None].float(), size, mode=\"nearest\")[:, 0] > 0.5\n",
        "\n",
        "    return rescaled_image, target\n",
        "\n",
        "\n",
        "def pad(image, target, padding):\n",
        "    # assumes that we only pad on the bottom right corners\n",
        "    padded_image = F.pad(image, (0, 0, padding[0], padding[1]))\n",
        "    if target is None:\n",
        "        return padded_image, None\n",
        "    target = target.copy()\n",
        "    # should we do something wrt the original size?\n",
        "    target[\"size\"] = torch.tensor(padded_image.size[::-1])\n",
        "    if \"masks\" in target:\n",
        "        target['masks'] = torch.nn.functional.pad(target['masks'], (0, padding[0], 0, padding[1]))\n",
        "    return padded_image, target\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        region = T.RandomCrop.get_params(img, self.size)\n",
        "        return crop(img, target, region)\n",
        "\n",
        "\n",
        "class RandomSizeCrop(object):\n",
        "    def __init__(self, min_size: int, max_size: int):\n",
        "        self.min_size = min_size\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, img: PIL.Image.Image, target: dict):\n",
        "        w = random.randint(self.min_size, min(img.width, self.max_size))\n",
        "        h = random.randint(self.min_size, min(img.height, self.max_size))\n",
        "        region = T.RandomCrop.get_params(img, [h, w])\n",
        "        return crop(img, target, region)\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        image_width, image_height = img.size\n",
        "        crop_height, crop_width = self.size\n",
        "        crop_top = int(round((image_height - crop_height) / 2.))\n",
        "        crop_left = int(round((image_width - crop_width) / 2.))\n",
        "        return crop(img, target, (crop_top, crop_left, crop_height, crop_width))\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random() < self.p:\n",
        "            return hflip(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class RandomResize(object):\n",
        "    def __init__(self, sizes, max_size=None):\n",
        "        assert isinstance(sizes, (list, tuple))\n",
        "        self.sizes = sizes\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, img, target=None):\n",
        "        size = random.choice(self.sizes)\n",
        "        return resize(img, target, size, self.max_size)\n",
        "\n",
        "\n",
        "class RandomPad(object):\n",
        "    def __init__(self, max_pad):\n",
        "        self.max_pad = max_pad\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        pad_x = random.randint(0, self.max_pad)\n",
        "        pad_y = random.randint(0, self.max_pad)\n",
        "        return pad(img, target, (pad_x, pad_y))\n",
        "\n",
        "\n",
        "class RandomSelect(object):\n",
        "    \"\"\"\n",
        "    Randomly selects between transforms1 and transforms2,\n",
        "    with probability p for transforms1 and (1 - p) for transforms2\n",
        "    \"\"\"\n",
        "    def __init__(self, transforms1, transforms2, p=0.5):\n",
        "        self.transforms1 = transforms1\n",
        "        self.transforms2 = transforms2\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random() < self.p:\n",
        "            return self.transforms1(img, target)\n",
        "        return self.transforms2(img, target)\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, img, target):\n",
        "        return F.to_tensor(img), target\n",
        "\n",
        "\n",
        "class RandomErasing(object):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.eraser = T.RandomErasing(*args, **kwargs)\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        return self.eraser(img), target\n",
        "\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target=None):\n",
        "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "        if target is None:\n",
        "            return image, None\n",
        "        target = target.copy()\n",
        "        h, w = image.shape[-2:]\n",
        "        if \"boxes\" in target:\n",
        "            boxes = target[\"boxes\"]\n",
        "            boxes = box_xyxy_to_cxcywh(boxes)\n",
        "            boxes = boxes / torch.tensor([w, h, w, h], dtype=torch.float32)\n",
        "            target[\"boxes\"] = boxes\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + \"(\"\n",
        "        for t in self.transforms:\n",
        "            format_string += \"\\n\"\n",
        "            format_string += \"    {0}\".format(t)\n",
        "        format_string += \"\\n)\"\n",
        "        return format_string"
      ],
      "metadata": {
        "id": "pGFF3M8cdneg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class CocoPanoptic:\n",
        "    def __init__(self, img_folder, ann_folder, ann_file, transforms=None, return_masks=True):\n",
        "        with open(ann_file, 'r') as f:\n",
        "            self.coco = json.load(f)\n",
        "\n",
        "        # sort 'images' field so that they are aligned with 'annotations'\n",
        "        # i.e., in alphabetical order\n",
        "        self.coco['images'] = sorted(self.coco['images'], key=lambda x: x['id'])\n",
        "        # sanity check\n",
        "        if \"annotations\" in self.coco:\n",
        "            for img, ann in zip(self.coco['images'], self.coco['annotations']):\n",
        "                assert img['file_name'][:-4] == ann['file_name'][:-4]\n",
        "\n",
        "        self.img_folder = img_folder\n",
        "        self.ann_folder = ann_folder\n",
        "        self.ann_file = ann_file\n",
        "        self.transforms = transforms\n",
        "        self.return_masks = return_masks\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ann_info = self.coco['annotations'][idx] if \"annotations\" in self.coco else self.coco['images'][idx]\n",
        "        img_path = Path(self.img_folder) / ann_info['file_name'].replace('.png', '.jpg')\n",
        "        ann_path = Path(self.ann_folder) / ann_info['file_name']\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        w, h = img.size\n",
        "        if \"segments_info\" in ann_info:\n",
        "            masks = np.asarray(Image.open(ann_path), dtype=np.uint32)\n",
        "            masks = rgb2id(masks)\n",
        "\n",
        "            ids = np.array([ann['id'] for ann in ann_info['segments_info']])\n",
        "            masks = masks == ids[:, None, None]\n",
        "\n",
        "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "            labels = torch.tensor([ann['category_id'] for ann in ann_info['segments_info']], dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target['image_id'] = torch.tensor([ann_info['image_id'] if \"image_id\" in ann_info else ann_info[\"id\"]])\n",
        "        if self.return_masks:\n",
        "            target['masks'] = masks\n",
        "        target['labels'] = labels\n",
        "\n",
        "        target[\"boxes\"] = masks_to_boxes(masks)\n",
        "\n",
        "        target['size'] = torch.as_tensor([int(h), int(w)])\n",
        "        target['orig_size'] = torch.as_tensor([int(h), int(w)])\n",
        "        if \"segments_info\" in ann_info:\n",
        "            for name in ['iscrowd', 'area']:\n",
        "                target[name] = torch.tensor([ann[name] for ann in ann_info['segments_info']])\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.coco['images'])\n",
        "\n",
        "    def get_height_and_width(self, idx):\n",
        "        img_info = self.coco['images'][idx]\n",
        "        height = img_info['height']\n",
        "        width = img_info['width']\n",
        "        return height, width\n",
        "\n",
        "\n",
        "def build_coco_panoptic(image_set, args):\n",
        "    img_folder_root = Path(args.coco_path)\n",
        "    ann_folder_root = Path(args.coco_panoptic_path)\n",
        "    assert img_folder_root.exists(), f'provided COCO path {img_folder_root} does not exist'\n",
        "    assert ann_folder_root.exists(), f'provided COCO path {ann_folder_root} does not exist'\n",
        "    mode = 'panoptic'\n",
        "    PATHS = {\n",
        "        \"train\": (\"train\", Path(\"annotations\") / f'{mode}_train.coco.json'),\n",
        "        \"val\": (\"valid\", Path(\"annotations\") / f'{mode}_validation.coco.json'),\n",
        "    }\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    img_folder_path = img_folder_root / img_folder\n",
        "    ann_folder = ann_folder_root / f'{mode}_{img_folder}'\n",
        "    ann_file = ann_folder_root / ann_file\n",
        "\n",
        "    dataset = CocoPanoptic(img_folder_path, ann_folder, ann_file,\n",
        "                           transforms=make_coco_transforms(image_set), return_masks=args.masks)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "sIpvAUtOl2fx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms, return_masks):\n",
        "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
        "        self._transforms = transforms\n",
        "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = {'image_id': image_id, 'annotations': target}\n",
        "        img, target = self.prepare(img, target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def convert_coco_poly_to_mask(segmentations, height, width):\n",
        "    masks = []\n",
        "    for polygons in segmentations:\n",
        "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
        "        mask = coco_mask.decode(rles)\n",
        "        if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "        mask = mask.any(dim=2)\n",
        "        masks.append(mask)\n",
        "    if masks:\n",
        "        masks = torch.stack(masks, dim=0)\n",
        "    else:\n",
        "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "    return masks\n",
        "\n",
        "\n",
        "class ConvertCocoPolysToMask(object):\n",
        "    def __init__(self, return_masks=False):\n",
        "        self.return_masks = return_masks\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "        image_id = torch.tensor([image_id])\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
        "\n",
        "        boxes = [obj[\"bbox\"] for obj in anno]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64)\n",
        "\n",
        "        if self.return_masks:\n",
        "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
        "            masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
        "\n",
        "        keypoints = None\n",
        "        if anno and \"keypoints\" in anno[0]:\n",
        "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
        "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
        "            num_keypoints = keypoints.shape[0]\n",
        "            if num_keypoints:\n",
        "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        if self.return_masks:\n",
        "            masks = masks[keep]\n",
        "        if keypoints is not None:\n",
        "            keypoints = keypoints[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        if self.return_masks:\n",
        "            target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        if keypoints is not None:\n",
        "            target[\"keypoints\"] = keypoints\n",
        "\n",
        "        # for conversion to coco api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
        "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
        "        target[\"area\"] = area[keep]\n",
        "        target[\"iscrowd\"] = iscrowd[keep]\n",
        "\n",
        "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
        "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def make_coco_transforms(image_set):\n",
        "\n",
        "    normalize = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
        "\n",
        "    if image_set == 'train':\n",
        "        return Compose([\n",
        "            RandomHorizontalFlip(),\n",
        "            # RandomSelect(\n",
        "            #     RandomResize(scales, max_size=1333),\n",
        "            #     Compose([\n",
        "            #         RandomResize([400, 500, 600]),\n",
        "            #         RandomResize(scales, max_size=1333),\n",
        "            #     ])\n",
        "            # ),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    if image_set == 'target':\n",
        "        return Compose([\n",
        "            RandomHorizontalFlip(),\n",
        "            # RandomSelect(\n",
        "            #     RandomResize(scales, max_size=1333),\n",
        "            #     Compose([\n",
        "            #         RandomResize([400, 500, 600]),\n",
        "            #         RandomResize(scales, max_size=1333),\n",
        "            #     ])\n",
        "            # ),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    if image_set == 'val':\n",
        "        return Compose([\n",
        "            # RandomResize([800], max_size=1333),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    raise ValueError(f'unknown {image_set}')\n",
        "\n",
        "\n",
        "def build_coco(image_set, args):\n",
        "    root = Path(args.coco_path)\n",
        "    assert root.exists(), f'provided COCO path {root} does not exist'\n",
        "    mode = 'instances'\n",
        "    PATHS = {\n",
        "        \"train\": (root / \"train\", root / \"annotations\" / f'{mode}_train.coco.json'),\n",
        "        \"target\": (root / \"target\", root / \"annotations\" / f'{mode}_target.coco.json'),\n",
        "        \"val\": (root / \"valid\", root / \"annotations\" / f'{mode}_validation.coco.json'),\n",
        "    }\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), return_masks=args.masks)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "9EDouCpOtaKZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_coco_api_from_dataset(dataset):\n",
        "    for _ in range(10):\n",
        "        # if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        #     break\n",
        "        if isinstance(dataset, torch.utils.data.Subset):\n",
        "            dataset = dataset.dataset\n",
        "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        return dataset.coco\n",
        "\n",
        "\n",
        "def build_dataset(image_set, args):\n",
        "    if args.dataset_file == 'coco':\n",
        "        return build_coco(image_set, args)\n",
        "    if args.dataset_file == 'coco_panoptic':\n",
        "        # to avoid making panopticapi required for coco\n",
        "        # from .coco_panoptic import build as build_coco_panoptic\n",
        "        return build_coco_panoptic(image_set, args)\n",
        "    raise ValueError(f'dataset {args.dataset_file} not supported')"
      ],
      "metadata": {
        "id": "v02HZoMxkgib"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoEvaluator(object):\n",
        "    def __init__(self, coco_gt, iou_types):\n",
        "        assert isinstance(iou_types, (list, tuple))\n",
        "        coco_gt = copy.deepcopy(coco_gt)\n",
        "        self.coco_gt = coco_gt\n",
        "\n",
        "        self.iou_types = iou_types\n",
        "        self.coco_eval = {}\n",
        "        for iou_type in iou_types:\n",
        "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
        "\n",
        "        self.img_ids = []\n",
        "        self.eval_imgs = {k: [] for k in iou_types}\n",
        "\n",
        "    def update(self, predictions):\n",
        "        img_ids = list(np.unique(list(predictions.keys())))\n",
        "        self.img_ids.extend(img_ids)\n",
        "\n",
        "        for iou_type in self.iou_types:\n",
        "            results = self.prepare(predictions, iou_type)\n",
        "\n",
        "            # suppress pycocotools prints\n",
        "            with open(os.devnull, 'w') as devnull:\n",
        "                with contextlib.redirect_stdout(devnull):\n",
        "                    coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()\n",
        "            coco_eval = self.coco_eval[iou_type]\n",
        "\n",
        "            coco_eval.cocoDt = coco_dt\n",
        "            coco_eval.params.imgIds = list(img_ids)\n",
        "            img_ids, eval_imgs = _evaluate(coco_eval)\n",
        "\n",
        "            self.eval_imgs[iou_type].append(eval_imgs)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for iou_type in self.iou_types:\n",
        "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
        "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
        "\n",
        "    def accumulate(self):\n",
        "        for coco_eval in self.coco_eval.values():\n",
        "            coco_eval.accumulate()\n",
        "\n",
        "    def summarize(self):\n",
        "        for iou_type, coco_eval in self.coco_eval.items():\n",
        "            print(\"IoU metric: {}\".format(iou_type))\n",
        "            coco_eval.summarize()\n",
        "\n",
        "    def prepare(self, predictions, iou_type):\n",
        "        if iou_type == \"bbox\":\n",
        "            return self.prepare_for_coco_detection(predictions)\n",
        "        elif iou_type == \"segm\":\n",
        "            return self.prepare_for_coco_segmentation(predictions)\n",
        "        elif iou_type == \"keypoints\":\n",
        "            return self.prepare_for_coco_keypoint(predictions)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n",
        "\n",
        "    def prepare_for_coco_detection(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"bbox\": box,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, box in enumerate(boxes)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_segmentation(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            scores = prediction[\"scores\"]\n",
        "            labels = prediction[\"labels\"]\n",
        "            masks = prediction[\"masks\"]\n",
        "\n",
        "            masks = masks > 0.5\n",
        "\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            rles = [\n",
        "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0]\n",
        "                for mask in masks\n",
        "            ]\n",
        "            for rle in rles:\n",
        "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"segmentation\": rle,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, rle in enumerate(rles)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_keypoint(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "            keypoints = prediction[\"keypoints\"]\n",
        "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        'keypoints': keypoint,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, keypoint in enumerate(keypoints)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "\n",
        "def merge(img_ids, eval_imgs):\n",
        "    all_img_ids = all_gather(img_ids)\n",
        "    all_eval_imgs = all_gather(eval_imgs)\n",
        "\n",
        "    merged_img_ids = []\n",
        "    for p in all_img_ids:\n",
        "        merged_img_ids.extend(p)\n",
        "\n",
        "    merged_eval_imgs = []\n",
        "    for p in all_eval_imgs:\n",
        "        merged_eval_imgs.append(p)\n",
        "\n",
        "    merged_img_ids = np.array(merged_img_ids)\n",
        "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
        "\n",
        "    # keep only unique (and in sorted order) images\n",
        "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
        "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
        "\n",
        "    return merged_img_ids, merged_eval_imgs\n",
        "\n",
        "\n",
        "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
        "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
        "    img_ids = list(img_ids)\n",
        "    eval_imgs = list(eval_imgs.flatten())\n",
        "\n",
        "    coco_eval.evalImgs = eval_imgs\n",
        "    coco_eval.params.imgIds = img_ids\n",
        "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
        "\n",
        "\n",
        "#################################################################\n",
        "# From pycocotools, just removed the prints and fixed\n",
        "# a Python3 bug about unicode not defined\n",
        "#################################################################\n",
        "\n",
        "\n",
        "def _evaluate(self):\n",
        "    '''\n",
        "    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
        "    :return: None\n",
        "    '''\n",
        "    # tic = time.time()\n",
        "    # print('Running per image evaluation...')\n",
        "    p = self.params\n",
        "    # add backward compatibility if useSegm is specified in params\n",
        "    if p.useSegm is not None:\n",
        "        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
        "        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
        "    # print('Evaluate annotation type *{}*'.format(p.iouType))\n",
        "    p.imgIds = list(np.unique(p.imgIds))\n",
        "    if p.useCats:\n",
        "        p.catIds = list(np.unique(p.catIds))\n",
        "    p.maxDets = sorted(p.maxDets)\n",
        "    self.params = p\n",
        "\n",
        "    self._prepare()\n",
        "    # loop through images, area range, max detection number\n",
        "    catIds = p.catIds if p.useCats else [-1]\n",
        "\n",
        "    if p.iouType == 'segm' or p.iouType == 'bbox':\n",
        "        computeIoU = self.computeIoU\n",
        "    elif p.iouType == 'keypoints':\n",
        "        computeIoU = self.computeOks\n",
        "    self.ious = {\n",
        "        (imgId, catId): computeIoU(imgId, catId)\n",
        "        for imgId in p.imgIds\n",
        "        for catId in catIds}\n",
        "\n",
        "    evaluateImg = self.evaluateImg\n",
        "    maxDet = p.maxDets[-1]\n",
        "    evalImgs = [\n",
        "        evaluateImg(imgId, catId, areaRng, maxDet)\n",
        "        for catId in catIds\n",
        "        for areaRng in p.areaRng\n",
        "        for imgId in p.imgIds\n",
        "    ]\n",
        "    # this is NOT in the pycocotools code, but could be done outside\n",
        "    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n",
        "    self._paramsEval = copy.deepcopy(self.params)\n",
        "    # toc = time.time()\n",
        "    # print('DONE (t={:0.2f}s).'.format(toc-tic))\n",
        "    return p.imgIds, evalImgs\n",
        "\n",
        "#################################################################\n",
        "# end of straight copy from pycocotools, just removing the prints\n",
        "#################################################################"
      ],
      "metadata": {
        "id": "a2K6mtEoohni"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from evaluation import pq_compute\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "\n",
        "class PanopticEvaluator(object):\n",
        "    def __init__(self, ann_file, ann_folder, output_dir=\"panoptic_eval\"):\n",
        "        self.gt_json = ann_file\n",
        "        self.gt_folder = ann_folder\n",
        "        if is_main_process():\n",
        "            if not os.path.exists(output_dir):\n",
        "                os.mkdir(output_dir)\n",
        "        self.output_dir = output_dir\n",
        "        self.predictions = []\n",
        "\n",
        "    def update(self, predictions):\n",
        "        for p in predictions:\n",
        "            with open(os.path.join(self.output_dir, p[\"file_name\"]), \"wb\") as f:\n",
        "                f.write(p.pop(\"png_string\"))\n",
        "\n",
        "        self.predictions += predictions\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        all_predictions = all_gather(self.predictions)\n",
        "        merged_predictions = []\n",
        "        for p in all_predictions:\n",
        "            merged_predictions += p\n",
        "        self.predictions = merged_predictions\n",
        "\n",
        "    def summarize(self):\n",
        "        if is_main_process():\n",
        "            json_data = {\"annotations\": self.predictions}\n",
        "            predictions_json = os.path.join(self.output_dir, \"predictions.json\")\n",
        "            with open(predictions_json, \"w\") as f:\n",
        "                f.write(json.dumps(json_data))\n",
        "            return pq_compute(self.gt_json, predictions_json, gt_folder=self.gt_folder, pred_folder=self.output_dir)\n",
        "        return None"
      ],
      "metadata": {
        "id": "XnnXPGQno-wp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module, data_loader: Iterable,\n",
        "                    data_loader1: Iterable, optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device, epoch: int, max_norm: float = 0):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 10\n",
        "    j=0\n",
        "    loss_fn_domain = torch.nn.NLLLoss()\n",
        "\n",
        "    target_dataloader_list=[]\n",
        "\n",
        "    for samples1, targets1 in data_loader1:\n",
        "        target_dataloader_list.append(samples1)\n",
        "        target_dataloader_list.append(targets1)\n",
        "\n",
        "    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        samples = samples.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        samples1 = target_dataloader_list[2*j]\n",
        "        targets1 = target_dataloader_list[2*j+1]\n",
        "        samples1 = samples1.to(device)\n",
        "        targets1 = [{k: v.to(device) for k, v in t.items()} for t in targets1]\n",
        "\n",
        "        outputs, outputs_domain01,  outputs_domain02= model(samples)\n",
        "        # print(outputs_domain)\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        print(\"loss_dict\", loss_dict)\n",
        "        weight_dict = criterion.weight_dict\n",
        "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "        \n",
        "        # for k in loss_dict.keys():\n",
        "        #     if k in weight_dict and k == 'domain_loss':\n",
        "        domain_predictions01 = outputs_domain01\n",
        "        domain_predictions01 = domain_predictions01.to(device)\n",
        "        domain_predictions02 = outputs_domain02\n",
        "        domain_predictions02 = domain_predictions02.to(device)\n",
        "\n",
        "\n",
        "        outputs1, outputs_domain11, outputs_domain12 = model(samples1)\n",
        "        # weight_dict1 = criterion.weight_dict\n",
        "        # losses1 = sum(loss_dict1[k] * weight_dict1[k] for k in loss_dict1.keys() if k in weight_dict1 and k != 'domain_loss')\n",
        "        \n",
        "        # for k in loss_dict.keys():\n",
        "        #     if k in weight_dict and k == 'domain_loss':\n",
        "        domain_predictions11 = outputs_domain11\n",
        "        domain_predictions11 = domain_predictions11.to(device)\n",
        "        domain_predictions12 = outputs_domain12\n",
        "        domain_predictions12 = domain_predictions12.to(device)\n",
        "\n",
        "        y_s_domain01 = y_s_domain02 = torch.zeros(2, dtype=torch.long) # generate source domain labels\n",
        "        y_t_domain11 = y_t_domain12 = torch.ones(2, dtype=torch.long) # generate target domain labels\n",
        "\n",
        "\n",
        "        y_s_domain01=y_s_domain01.to(device)\n",
        "        y_s_domain02=y_s_domain02.to(device)\n",
        "        y_t_domain11=y_t_domain11.to(device)\n",
        "        y_t_domain12=y_t_domain12.to(device)\n",
        "\n",
        "        loss_s_domain01 = loss_fn_domain(domain_predictions01, y_s_domain01)\n",
        "        loss_s_domain02 = loss_fn_domain(domain_predictions02, y_s_domain02)\n",
        "        loss_t_domain11 = loss_fn_domain(domain_predictions11, y_t_domain11)\n",
        "        loss_t_domain12 = loss_fn_domain(domain_predictions12, y_t_domain12)\n",
        "\n",
        "\n",
        "        losses = losses + loss_s_domain01 + loss_s_domain02 + loss_t_domain11 + loss_t_domain12\n",
        "\n",
        "        print(\"total loss with DAN is: \", losses)\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
        "\n",
        "        loss_value = losses_reduced_scaled.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        if max_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
        "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "        j+=1\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, output_dir):\n",
        "    model.eval()\n",
        "    criterion.eval()\n",
        "\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Test:'\n",
        "\n",
        "    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
        "    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n",
        "    # coco_evaluator.coco_eval[iou_types[0]].params.iouThrs = [0, 0.1, 0.5, 0.75]\n",
        "\n",
        "    panoptic_evaluator = None\n",
        "    if 'panoptic' in postprocessors.keys():\n",
        "        panoptic_evaluator = PanopticEvaluator(\n",
        "            data_loader.dataset.ann_file,\n",
        "            data_loader.dataset.ann_folder,\n",
        "            output_dir=os.path.join(output_dir, \"panoptic_eval\"),\n",
        "        )\n",
        "\n",
        "    for samples, targets in metric_logger.log_every(data_loader, 10, header):\n",
        "        samples = samples.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        outputs, _, _ = model(samples)\n",
        "        \n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
        "                             **loss_dict_reduced_scaled,\n",
        "                             **loss_dict_reduced_unscaled)\n",
        "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "\n",
        "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
        "        results = postprocessors['bbox'](outputs, orig_target_sizes)\n",
        "        if 'segm' in postprocessors.keys():\n",
        "            target_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n",
        "            results = postprocessors['segm'](results, outputs, orig_target_sizes, target_sizes)\n",
        "        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
        "        if coco_evaluator is not None:\n",
        "            coco_evaluator.update(res)\n",
        "\n",
        "        if panoptic_evaluator is not None:\n",
        "            res_pano = postprocessors[\"panoptic\"](outputs, target_sizes, orig_target_sizes)\n",
        "            for i, target in enumerate(targets):\n",
        "                image_id = target[\"image_id\"].item()\n",
        "                file_name = f\"{image_id:012d}.png\"\n",
        "                res_pano[i][\"image_id\"] = image_id\n",
        "                res_pano[i][\"file_name\"] = file_name\n",
        "\n",
        "            panoptic_evaluator.update(res_pano)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    if coco_evaluator is not None:\n",
        "        coco_evaluator.synchronize_between_processes()\n",
        "    if panoptic_evaluator is not None:\n",
        "        panoptic_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    if coco_evaluator is not None:\n",
        "        coco_evaluator.accumulate()\n",
        "        coco_evaluator.summarize()\n",
        "    panoptic_res = None\n",
        "    if panoptic_evaluator is not None:\n",
        "        panoptic_res = panoptic_evaluator.summarize()\n",
        "    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "    if coco_evaluator is not None:\n",
        "        if 'bbox' in postprocessors.keys():\n",
        "            stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n",
        "        if 'segm' in postprocessors.keys():\n",
        "            stats['coco_eval_masks'] = coco_evaluator.coco_eval['segm'].stats.tolist()\n",
        "    if panoptic_res is not None:\n",
        "        stats['PQ_all'] = panoptic_res[\"All\"]\n",
        "        stats['PQ_th'] = panoptic_res[\"Things\"]\n",
        "        stats['PQ_st'] = panoptic_res[\"Stuff\"]\n",
        "    return stats, coco_evaluator"
      ],
      "metadata": {
        "id": "bYNMLy8jpUIX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_args_parser():\n",
        "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
        "    parser.add_argument('-f')\n",
        "    parser.add_argument('--lr', default=1e-4, type=float)\n",
        "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
        "    parser.add_argument('--batch_size', default=2, type=int)\n",
        "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
        "    parser.add_argument('--epochs', default=300, type=int)\n",
        "    parser.add_argument('--lr_drop', default=200, type=int)\n",
        "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
        "                        help='gradient clipping max norm')\n",
        "\n",
        "    # Model parameters\n",
        "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
        "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
        "    # * Backbone\n",
        "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
        "                        help=\"Name of the convolutional backbone to use\")\n",
        "    parser.add_argument('--dilation', action='store_true',\n",
        "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
        "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
        "                        help=\"Type of positional embedding to use on top of the image features\")\n",
        "\n",
        "    # * Transformer\n",
        "    parser.add_argument('--enc_layers', default=6, type=int,\n",
        "                        help=\"Number of encoding layers in the transformer\")\n",
        "    parser.add_argument('--dec_layers', default=6, type=int,\n",
        "                        help=\"Number of decoding layers in the transformer\")\n",
        "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
        "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
        "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
        "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
        "    parser.add_argument('--dropout', default=0.1, type=float,\n",
        "                        help=\"Dropout applied in the transformer\")\n",
        "    parser.add_argument('--nheads', default=8, type=int,\n",
        "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
        "    parser.add_argument('--num_queries', default=100, type=int,\n",
        "                        help=\"Number of query slots\")\n",
        "    parser.add_argument('--pre_norm', action='store_true')\n",
        "\n",
        "    # * Segmentation\n",
        "    parser.add_argument('--masks', action='store_true',\n",
        "                        help=\"Train segmentation head if the flag is provided\")\n",
        "\n",
        "    # Loss\n",
        "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
        "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
        "    # * Matcher\n",
        "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
        "                        help=\"Class coefficient in the matching cost\")\n",
        "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
        "                        help=\"L1 box coefficient in the matching cost\")\n",
        "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
        "                        help=\"giou box coefficient in the matching cost\")\n",
        "    # * Loss coefficients\n",
        "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
        "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
        "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
        "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
        "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
        "                        help=\"Relative classification weight of the no-object class\")\n",
        "\n",
        "    # dataset parameters\n",
        "    parser.add_argument('--dataset_file', default='coco')\n",
        "    parser.add_argument('--coco_path', default='/content/data', type=str)\n",
        "    parser.add_argument('--coco_panoptic_path', type=str)\n",
        "    parser.add_argument('--remove_difficult', action='store_true')\n",
        "\n",
        "    parser.add_argument('--output_dir', default='',\n",
        "                        help='path where to save, empty for no saving')\n",
        "    parser.add_argument('--device', default='cuda',\n",
        "                        help='device to use for training / testing')\n",
        "    parser.add_argument('--seed', default=42, type=int)\n",
        "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
        "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
        "                        help='start epoch')\n",
        "    parser.add_argument('--eval', action='store_true')\n",
        "    parser.add_argument('--num_workers', default=2, type=int)\n",
        "\n",
        "    # distributed training parameters\n",
        "    parser.add_argument('--world_size', default=1, type=int,\n",
        "                        help='number of distributed processes')\n",
        "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
        "    return parser\n",
        "\n",
        "def main_train(args):\n",
        "    init_distributed_mode(args)\n",
        "    # print(\"git:\\n  {}\\n\".format(get_sha()))\n",
        "\n",
        "    if args.frozen_weights is not None:\n",
        "        assert args.masks, \"Frozen training is meant for segmentation only\"\n",
        "    print(args)\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    # fix the seed for reproducibility\n",
        "    seed = args.seed + get_rank()\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    model, criterion, postprocessors = build_model(args)\n",
        "    model.to(device)\n",
        "\n",
        "    model_without_ddp = model\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "        model_without_ddp = model.module\n",
        "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('number of params:', n_parameters)\n",
        "\n",
        "    param_dicts = [\n",
        "        {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "        {\n",
        "            \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "            \"lr\": args.lr_backbone,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
        "                                  weight_decay=args.weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
        "\n",
        "    dataset_train = build_dataset(image_set='train', args=args)\n",
        "    dataset_target = build_dataset(image_set='target', args=args)\n",
        "    dataset_val = build_dataset(image_set='val', args=args)\n",
        "\n",
        "    if args.distributed:\n",
        "        sampler_train = DistributedSampler(dataset_train)\n",
        "        sampler_target= DistributedSampler(dataset_target)\n",
        "        sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
        "    else:\n",
        "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
        "        sampler_target = torch.utils.data.RandomSampler(dataset_target)\n",
        "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
        "\n",
        "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
        "        sampler_train, args.batch_size, drop_last=True) \n",
        "    batch_sampler_target = torch.utils.data.BatchSampler(\n",
        "        sampler_target, args.batch_size, drop_last=True) \n",
        "\n",
        "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
        "                                   collate_fn=collate_fn, num_workers=args.num_workers)\n",
        "    data_loader_target = DataLoader(dataset_target, batch_sampler=batch_sampler_target,\n",
        "                                   collate_fn=collate_fn, num_workers=args.num_workers)\n",
        "    data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
        "                                 drop_last=False, collate_fn=collate_fn, num_workers=args.num_workers)\n",
        "\n",
        "    if args.dataset_file == \"coco_panoptic\":\n",
        "        # We also evaluate AP during panoptic training, on original coco DS\n",
        "        coco_val = build_coco(\"val\", args)\n",
        "        base_ds = get_coco_api_from_dataset(coco_val)\n",
        "    else:\n",
        "        base_ds = get_coco_api_from_dataset(dataset_val)\n",
        "\n",
        "    if args.frozen_weights is not None:\n",
        "        checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
        "        model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
        "\n",
        "    output_dir = Path(args.output_dir)\n",
        "    if args.resume:\n",
        "        if args.resume.startswith('https'):\n",
        "            checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                args.resume, map_location='cpu', check_hash=True)\n",
        "        else:\n",
        "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "        if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "            args.start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    if args.eval:\n",
        "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
        "                                              data_loader_val, base_ds, device, args.output_dir)\n",
        "        if args.output_dir:\n",
        "            save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
        "        return\n",
        "\n",
        "    print(\"Start training\")\n",
        "    start_time = time.time()\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        if args.distributed:\n",
        "            sampler_train.set_epoch(epoch)\n",
        "        train_stats = train_one_epoch(\n",
        "            model, criterion, data_loader_train, data_loader_target, optimizer, device, epoch,\n",
        "            args.clip_max_norm)\n",
        "        lr_scheduler.step()\n",
        "        if args.output_dir:\n",
        "            checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
        "            # extra checkpoint before LR drop and every 100 epochs\n",
        "            if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 100 == 0:\n",
        "                checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
        "            for checkpoint_path in checkpoint_paths:\n",
        "                save_on_master({\n",
        "                    'model': model_without_ddp.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'args': args,\n",
        "                }, checkpoint_path)\n",
        "\n",
        "        test_stats, coco_evaluator = evaluate(\n",
        "            model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
        "        )\n",
        "\n",
        "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                     **{f'test_{k}': v for k, v in test_stats.items()},\n",
        "                     'epoch': epoch,\n",
        "                     'n_parameters': n_parameters}\n",
        "\n",
        "        if args.output_dir and is_main_process():\n",
        "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
        "                f.write(json.dumps(log_stats) + \"\\n\")\n",
        "\n",
        "            # for evaluation logs\n",
        "            if coco_evaluator is not None:\n",
        "                (output_dir / 'eval').mkdir(exist_ok=True)\n",
        "                if \"bbox\" in coco_evaluator.coco_eval:\n",
        "                    filenames = ['latest.pth']\n",
        "                    if epoch % 50 == 0:\n",
        "                        filenames.append(f'{epoch:03}.pth')\n",
        "                    for name in filenames:\n",
        "                        torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
        "                                   output_dir / \"eval\" / name)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Training time {}'.format(total_time_str))"
      ],
      "metadata": {
        "id": "_B1ZqWzSubXV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])\n",
        "args = parser.parse_args()\n",
        "if args.output_dir:\n",
        "  Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "main_train(args)"
      ],
      "metadata": {
        "id": "fFbD4iyPufcF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "86b012f0401443cebf5f350a81ce5872",
            "5f54755b7af4499fab36935b55db9344",
            "a3069b4540454ac09383a74e8b2c6141",
            "594fa26daaa24ff5aa163e0c5bbb3abf",
            "08fea5461ff943cba95e6fd6a9f84d37",
            "d8e638b9d50c4457ade1e4705f08de25",
            "5e112300b8ba41f69d6d27b6f9ddc006",
            "6622b8755e7f4a6990b40ffaf03cdefa",
            "22671a2a91714824a24eaee48e40aed0",
            "3e9d5159214e42f380a7d52fc3556087",
            "82ffce34feff4c9384d3ea3a39205df2"
          ]
        },
        "outputId": "50f5dff9-08c6-4fc1-ffab-59db02884838"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Namespace(aux_loss=True, backbone='resnet50', batch_size=2, bbox_loss_coef=5, clip_max_norm=0.1, coco_panoptic_path=None, coco_path='/content/data', dataset_file='coco', dec_layers=6, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dropout=0.1, enc_layers=6, eos_coef=0.1, epochs=300, eval=False, f='/root/.local/share/jupyter/runtime/kernel-39d24351-f5c5-447a-bc7c-454cb3cf2758.json', frozen_weights=None, giou_loss_coef=2, hidden_dim=256, lr=0.0001, lr_backbone=1e-05, lr_drop=200, mask_loss_coef=1, masks=False, nheads=8, num_queries=100, num_workers=2, output_dir='', position_embedding='sine', pre_norm=False, remove_difficult=False, resume='', seed=42, set_cost_bbox=5, set_cost_class=1, set_cost_giou=2, start_epoch=0, weight_decay=0.0001, world_size=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86b012f0401443cebf5f350a81ce5872"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of params: 44705280\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss_dict {'loss_ce': tensor(2.1581, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(91.4894, device='cuda:0'), 'loss_bbox': tensor(1.0987, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0295, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(16., device='cuda:0'), 'loss_ce_0': tensor(2.2534, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(1.1073, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0413, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(36., device='cuda:0'), 'loss_ce_1': tensor(2.3502, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(1.1261, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0358, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(76.5000, device='cuda:0'), 'loss_ce_2': tensor(2.2619, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(1.1194, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0339, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(76.5000, device='cuda:0'), 'loss_ce_3': tensor(2.2380, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(1.1265, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0387, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(76.5000, device='cuda:0'), 'loss_ce_4': tensor(2.0868, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(1.1062, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0370, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(63., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(62.1896, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [0]  [  0/100]  eta: 0:16:36  lr: 0.000100  class_error: 91.49  loss: 59.2023 (59.2023)  loss_ce: 2.1581 (2.1581)  loss_bbox: 5.4937 (5.4937)  loss_giou: 2.0590 (2.0590)  loss_ce_0: 2.2534 (2.2534)  loss_bbox_0: 5.5366 (5.5366)  loss_giou_0: 2.0825 (2.0825)  loss_ce_1: 2.3502 (2.3502)  loss_bbox_1: 5.6305 (5.6305)  loss_giou_1: 2.0715 (2.0715)  loss_ce_2: 2.2619 (2.2619)  loss_bbox_2: 5.5971 (5.5971)  loss_giou_2: 2.0678 (2.0678)  loss_ce_3: 2.2380 (2.2380)  loss_bbox_3: 5.6324 (5.6324)  loss_giou_3: 2.0774 (2.0774)  loss_ce_4: 2.0868 (2.0868)  loss_bbox_4: 5.5311 (5.5311)  loss_giou_4: 2.0739 (2.0739)  loss_ce_unscaled: 2.1581 (2.1581)  class_error_unscaled: 91.4894 (91.4894)  loss_bbox_unscaled: 1.0987 (1.0987)  loss_giou_unscaled: 1.0295 (1.0295)  cardinality_error_unscaled: 16.0000 (16.0000)  loss_ce_0_unscaled: 2.2534 (2.2534)  loss_bbox_0_unscaled: 1.1073 (1.1073)  loss_giou_0_unscaled: 1.0413 (1.0413)  cardinality_error_0_unscaled: 36.0000 (36.0000)  loss_ce_1_unscaled: 2.3502 (2.3502)  loss_bbox_1_unscaled: 1.1261 (1.1261)  loss_giou_1_unscaled: 1.0358 (1.0358)  cardinality_error_1_unscaled: 76.5000 (76.5000)  loss_ce_2_unscaled: 2.2619 (2.2619)  loss_bbox_2_unscaled: 1.1194 (1.1194)  loss_giou_2_unscaled: 1.0339 (1.0339)  cardinality_error_2_unscaled: 76.5000 (76.5000)  loss_ce_3_unscaled: 2.2380 (2.2380)  loss_bbox_3_unscaled: 1.1265 (1.1265)  loss_giou_3_unscaled: 1.0387 (1.0387)  cardinality_error_3_unscaled: 76.5000 (76.5000)  loss_ce_4_unscaled: 2.0868 (2.0868)  loss_bbox_4_unscaled: 1.1062 (1.1062)  loss_giou_4_unscaled: 1.0370 (1.0370)  cardinality_error_4_unscaled: 63.0000 (63.0000)  time: 9.9610  data: 0.1873  max mem: 920\n",
            "loss_dict {'loss_ce': tensor(2.1124, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(68.5714, device='cuda:0'), 'loss_bbox': tensor(1.2040, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0452, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(65., device='cuda:0'), 'loss_ce_0': tensor(1.9243, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(1.1125, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0606, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(65., device='cuda:0'), 'loss_ce_1': tensor(1.8606, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(1.1222, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0594, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(65., device='cuda:0'), 'loss_ce_2': tensor(1.5960, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(1.1452, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0630, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(65., device='cuda:0'), 'loss_ce_3': tensor(1.8846, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(1.1305, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0636, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(65., device='cuda:0'), 'loss_ce_4': tensor(1.8798, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(1.1178, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0653, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(64.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(61.0621, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4888, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(33.3333, device='cuda:0'), 'loss_bbox': tensor(1.0930, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0356, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(75., device='cuda:0'), 'loss_ce_0': tensor(1.5421, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(1.0427, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0547, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(79., device='cuda:0'), 'loss_ce_1': tensor(1.4307, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(1.0306, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0600, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(79., device='cuda:0'), 'loss_ce_2': tensor(1.5567, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(1.0616, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0627, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(79., device='cuda:0'), 'loss_ce_3': tensor(1.5541, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(1.0597, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0607, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(79., device='cuda:0'), 'loss_ce_4': tensor(1.3924, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(1.0365, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0588, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(75., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(56.1700, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5399, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(88., device='cuda:0'), 'loss_bbox': tensor(0.9667, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0096, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(71.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5934, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.9256, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0080, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(73., device='cuda:0'), 'loss_ce_1': tensor(1.6616, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.9007, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0095, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(70.5000, device='cuda:0'), 'loss_ce_2': tensor(1.7678, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.9007, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0099, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(75., device='cuda:0'), 'loss_ce_3': tensor(1.8735, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.9144, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0129, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(74.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6583, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.9109, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0152, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(75., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(52.8449, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3300, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(12.5000, device='cuda:0'), 'loss_bbox': tensor(1.0244, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0587, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(69., device='cuda:0'), 'loss_ce_0': tensor(1.3851, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(1.0027, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0612, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(22.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4247, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.9782, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0644, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(14., device='cuda:0'), 'loss_ce_2': tensor(1.3438, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.9828, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0685, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(37.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4168, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.9912, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0731, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(53.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3697, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.9795, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0787, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(68., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(53.7829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2106, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(12.5000, device='cuda:0'), 'loss_bbox': tensor(0.8690, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0326, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(88., device='cuda:0'), 'loss_ce_0': tensor(1.0645, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.8678, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0347, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(83.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0746, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.8340, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0395, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(85.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0886, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.8369, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0407, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(88., device='cuda:0'), 'loss_ce_3': tensor(1.2169, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.8486, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0398, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(88., device='cuda:0'), 'loss_ce_4': tensor(1.1900, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.8299, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0449, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(88., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(47.7051, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(2.1006, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(76.4706, device='cuda:0'), 'loss_bbox': tensor(0.8637, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0539, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(83., device='cuda:0'), 'loss_ce_0': tensor(1.9337, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.8683, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0544, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(83., device='cuda:0'), 'loss_ce_1': tensor(1.9661, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.8385, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0589, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(83., device='cuda:0'), 'loss_ce_2': tensor(2.0086, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.8403, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0595, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(83., device='cuda:0'), 'loss_ce_3': tensor(2.0710, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.8497, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0647, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(83., device='cuda:0'), 'loss_ce_4': tensor(2.0484, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.8368, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0688, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(83., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(53.3352, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3688, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(45.1613, device='cuda:0'), 'loss_bbox': tensor(0.8356, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0546, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(80.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4537, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.8577, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0461, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(82., device='cuda:0'), 'loss_ce_1': tensor(1.4327, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.8182, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0566, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(83., device='cuda:0'), 'loss_ce_2': tensor(1.4477, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.8075, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0655, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(78.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4798, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.8258, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0667, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(84., device='cuda:0'), 'loss_ce_4': tensor(1.4338, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.8283, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0703, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(84.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(49.1242, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1520, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(52.9412, device='cuda:0'), 'loss_bbox': tensor(0.7784, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0552, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(4., device='cuda:0'), 'loss_ce_0': tensor(1.0125, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.7984, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0483, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(65., device='cuda:0'), 'loss_ce_1': tensor(1.0640, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.7635, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0585, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(60., device='cuda:0'), 'loss_ce_2': tensor(1.1335, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.7557, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0617, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(3., device='cuda:0'), 'loss_ce_3': tensor(1.1470, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.7592, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0663, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(12., device='cuda:0'), 'loss_ce_4': tensor(1.1043, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.7643, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0629, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(19.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(45.3781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3979, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(69.2308, device='cuda:0'), 'loss_bbox': tensor(0.6805, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0560, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(18.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5837, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.7117, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0411, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(76.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4622, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.6712, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0510, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(79.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3793, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.6665, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0581, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(28., device='cuda:0'), 'loss_ce_3': tensor(1.4760, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.6770, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0634, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(26.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4570, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.6873, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0617, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(35., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(44.8491, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2681, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(47.3684, device='cuda:0'), 'loss_bbox': tensor(0.7715, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0359, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(25., device='cuda:0'), 'loss_ce_0': tensor(1.2527, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.7750, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0240, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(77., device='cuda:0'), 'loss_ce_1': tensor(1.3176, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.7490, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0320, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(81.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2771, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.7297, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0448, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(26., device='cuda:0'), 'loss_ce_3': tensor(1.3110, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.7454, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0459, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(19., device='cuda:0'), 'loss_ce_4': tensor(1.2541, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.7495, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0402, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(29.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(45.6168, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [0]  [ 10/100]  eta: 0:01:44  lr: 0.000100  class_error: 47.37  loss: 49.8193 (49.0536)  loss_ce: 1.3979 (1.5570)  loss_bbox: 4.3451 (4.6298)  loss_giou: 2.0905 (2.0849)  loss_ce_0: 1.5421 (1.5454)  loss_bbox_0: 4.3414 (4.5771)  loss_giou_0: 2.0922 (2.0863)  loss_ce_1: 1.4327 (1.5496)  loss_bbox_1: 4.1925 (4.4692)  loss_giou_1: 2.1131 (2.0955)  loss_ce_2: 1.4477 (1.5328)  loss_bbox_2: 4.2016 (4.4756)  loss_giou_2: 2.1190 (2.1033)  loss_ce_3: 1.4798 (1.6063)  loss_bbox_3: 4.2486 (4.5128)  loss_giou_3: 2.1269 (2.1083)  loss_ce_4: 1.4338 (1.5341)  loss_bbox_4: 4.1840 (4.4759)  loss_giou_4: 2.1234 (2.1098)  loss_ce_unscaled: 1.3979 (1.5570)  class_error_unscaled: 52.9412 (54.3242)  loss_bbox_unscaled: 0.8690 (0.9260)  loss_giou_unscaled: 1.0452 (1.0424)  cardinality_error_unscaled: 69.0000 (54.1364)  loss_ce_0_unscaled: 1.5421 (1.5454)  loss_bbox_0_unscaled: 0.8683 (0.9154)  loss_giou_0_unscaled: 1.0461 (1.0431)  cardinality_error_0_unscaled: 76.5000 (67.5000)  loss_ce_1_unscaled: 1.4327 (1.5496)  loss_bbox_1_unscaled: 0.8385 (0.8938)  loss_giou_1_unscaled: 1.0566 (1.0478)  cardinality_error_1_unscaled: 79.0000 (70.6818)  loss_ce_2_unscaled: 1.4477 (1.5328)  loss_bbox_2_unscaled: 0.8403 (0.8951)  loss_giou_2_unscaled: 1.0595 (1.0517)  cardinality_error_2_unscaled: 75.0000 (58.1364)  loss_ce_3_unscaled: 1.4798 (1.6063)  loss_bbox_3_unscaled: 0.8497 (0.9026)  loss_giou_3_unscaled: 1.0634 (1.0542)  cardinality_error_3_unscaled: 74.5000 (60.0909)  loss_ce_4_unscaled: 1.4338 (1.5341)  loss_bbox_4_unscaled: 0.8368 (0.8952)  loss_giou_4_unscaled: 1.0617 (1.0549)  cardinality_error_4_unscaled: 68.0000 (62.2727)  time: 1.1576  data: 0.0262  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.3681, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(56.8182, device='cuda:0'), 'loss_bbox': tensor(0.7650, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1518, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(1., device='cuda:0'), 'loss_ce_0': tensor(1.3876, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.7922, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1397, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(46., device='cuda:0'), 'loss_ce_1': tensor(1.3614, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.7530, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1586, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(45.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3789, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.7443, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1730, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(2., device='cuda:0'), 'loss_ce_3': tensor(1.4447, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.7558, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1694, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(8., device='cuda:0'), 'loss_ce_4': tensor(1.3893, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.7503, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1515, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(7., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(47.8799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4303, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(57.7778, device='cuda:0'), 'loss_bbox': tensor(0.6649, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0455, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(9., device='cuda:0'), 'loss_ce_0': tensor(1.4329, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.6838, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0423, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(23., device='cuda:0'), 'loss_ce_1': tensor(1.4517, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.6506, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0455, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(29.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4659, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.6395, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0555, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(4.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4756, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.6549, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0544, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(10., device='cuda:0'), 'loss_ce_4': tensor(1.4506, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.6497, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0463, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(21., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(43.9070, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5430, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(85.1852, device='cuda:0'), 'loss_bbox': tensor(0.6981, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1940, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(65., device='cuda:0'), 'loss_ce_0': tensor(1.5733, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.7395, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1966, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(37.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5347, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.6936, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2024, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(43.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4722, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.6929, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2227, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(47., device='cuda:0'), 'loss_ce_3': tensor(1.5935, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.7015, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2232, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(36., device='cuda:0'), 'loss_ce_4': tensor(1.6157, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.6877, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2003, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(69.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(47.7717, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1873, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(16.6667, device='cuda:0'), 'loss_bbox': tensor(0.5984, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1156, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(72., device='cuda:0'), 'loss_ce_0': tensor(1.0719, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.6314, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1078, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(23.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1917, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.6002, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1147, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(38.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2657, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.5995, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1255, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62., device='cuda:0'), 'loss_ce_3': tensor(1.2317, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.5965, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1214, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(34.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1675, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.5946, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1166, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(75.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(41.5150, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3156, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(44.4444, device='cuda:0'), 'loss_bbox': tensor(0.6975, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2871, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(46., device='cuda:0'), 'loss_ce_0': tensor(1.2521, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.7202, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2560, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(10., device='cuda:0'), 'loss_ce_1': tensor(1.3367, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.6818, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2712, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(12., device='cuda:0'), 'loss_ce_2': tensor(1.3475, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.6998, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2875, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(39., device='cuda:0'), 'loss_ce_3': tensor(1.3370, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.6961, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2991, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(25., device='cuda:0'), 'loss_ce_4': tensor(1.2912, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.6966, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2887, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(59.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(47.2770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3335, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(94.7368, device='cuda:0'), 'loss_bbox': tensor(0.7077, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2758, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(7., device='cuda:0'), 'loss_ce_0': tensor(1.2479, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.7375, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2613, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(7., device='cuda:0'), 'loss_ce_1': tensor(1.2267, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.7061, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2692, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(8., device='cuda:0'), 'loss_ce_2': tensor(1.3168, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.7148, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2743, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(2., device='cuda:0'), 'loss_ce_3': tensor(1.3332, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.7082, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2806, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(4.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3633, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.7071, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2744, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(3.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(47.4255, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2366, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(100., device='cuda:0'), 'loss_bbox': tensor(0.6132, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2248, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(14., device='cuda:0'), 'loss_ce_0': tensor(1.2604, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.6531, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2191, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(14., device='cuda:0'), 'loss_ce_1': tensor(1.3013, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.6034, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2101, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(13.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2227, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.6232, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2164, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(13., device='cuda:0'), 'loss_ce_3': tensor(1.2274, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.6127, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2301, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(13.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2229, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.6066, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2225, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(13.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(43.5947, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3218, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(100., device='cuda:0'), 'loss_bbox': tensor(0.4796, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1053, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(12., device='cuda:0'), 'loss_ce_0': tensor(1.3756, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4984, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0953, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(12., device='cuda:0'), 'loss_ce_1': tensor(1.3932, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4788, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0989, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(12., device='cuda:0'), 'loss_ce_2': tensor(1.3503, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4927, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1025, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(11.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3101, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4829, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1082, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(12., device='cuda:0'), 'loss_ce_4': tensor(1.3447, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4791, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1144, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(12., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.7663, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.9454, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(100., device='cuda:0'), 'loss_bbox': tensor(0.5672, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2574, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(18.5000, device='cuda:0'), 'loss_ce_0': tensor(1.9105, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.5981, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2629, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(18.5000, device='cuda:0'), 'loss_ce_1': tensor(1.9534, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.5659, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2586, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(18.5000, device='cuda:0'), 'loss_ce_2': tensor(1.9411, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.5804, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2537, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(18.5000, device='cuda:0'), 'loss_ce_3': tensor(2.0258, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.5698, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2631, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(17.5000, device='cuda:0'), 'loss_ce_4': tensor(1.9215, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.5635, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2622, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(18.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(46.9685, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2562, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(100., device='cuda:0'), 'loss_bbox': tensor(0.5461, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2081, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(21., device='cuda:0'), 'loss_ce_0': tensor(1.2252, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.5567, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1871, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(21., device='cuda:0'), 'loss_ce_1': tensor(1.2233, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.5496, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2073, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(21., device='cuda:0'), 'loss_ce_2': tensor(1.1633, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.5566, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2036, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(20.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1250, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.5482, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2104, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(19., device='cuda:0'), 'loss_ce_4': tensor(1.2440, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.5492, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2152, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(21., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(41.0302, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [0]  [ 20/100]  eta: 0:00:58  lr: 0.000100  class_error: 100.00  loss: 44.2196 (45.5563)  loss_ce: 1.3335 (1.4793)  loss_bbox: 3.5385 (3.9341)  loss_giou: 2.1119 (2.2221)  loss_ce_0: 1.3851 (1.4636)  loss_bbox_0: 3.6975 (3.9715)  loss_giou_0: 2.1212 (2.2136)  loss_ce_1: 1.3932 (1.4771)  loss_bbox_1: 3.5305 (3.8370)  loss_giou_1: 2.1199 (2.2250)  loss_ce_2: 1.3503 (1.4660)  loss_bbox_2: 3.5742 (3.8548)  loss_giou_2: 2.1310 (2.2365)  loss_ce_3: 1.4168 (1.5130)  loss_bbox_3: 3.5411 (3.8702)  loss_giou_3: 2.1334 (2.2434)  loss_ce_4: 1.3697 (1.4707)  loss_bbox_4: 3.5353 (3.8408)  loss_giou_4: 2.1406 (2.2377)  loss_ce_unscaled: 1.3335 (1.4793)  class_error_unscaled: 57.7778 (64.4379)  loss_bbox_unscaled: 0.7077 (0.7868)  loss_giou_unscaled: 1.0560 (1.1110)  cardinality_error_unscaled: 25.0000 (41.0000)  loss_ce_0_unscaled: 1.3851 (1.4636)  loss_bbox_0_unscaled: 0.7395 (0.7943)  loss_giou_0_unscaled: 1.0606 (1.1068)  cardinality_error_0_unscaled: 37.5000 (45.4762)  loss_ce_1_unscaled: 1.3932 (1.4771)  loss_bbox_1_unscaled: 0.7061 (0.7674)  loss_giou_1_unscaled: 1.0600 (1.1125)  cardinality_error_1_unscaled: 43.5000 (48.5476)  loss_ce_2_unscaled: 1.3503 (1.4660)  loss_bbox_2_unscaled: 0.7148 (0.7710)  loss_giou_2_unscaled: 1.0655 (1.1182)  cardinality_error_2_unscaled: 28.0000 (40.9286)  loss_ce_3_unscaled: 1.4168 (1.5130)  loss_bbox_3_unscaled: 0.7082 (0.7740)  loss_giou_3_unscaled: 1.0667 (1.1217)  cardinality_error_3_unscaled: 25.0000 (40.0476)  loss_ce_4_unscaled: 1.3697 (1.4707)  loss_bbox_4_unscaled: 0.7071 (0.7682)  loss_giou_4_unscaled: 1.0703 (1.1188)  cardinality_error_4_unscaled: 35.0000 (46.9524)  time: 0.2720  data: 0.0099  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.2961, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(88., device='cuda:0'), 'loss_bbox': tensor(0.4978, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1607, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(6.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3054, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4938, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1200, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(12., device='cuda:0'), 'loss_ce_1': tensor(1.2762, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4991, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1596, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(11., device='cuda:0'), 'loss_ce_2': tensor(1.2593, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.5056, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1584, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(9.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2713, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4940, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1693, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(17.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3067, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4991, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1757, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(11., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.4727, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2393, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(41.0256, device='cuda:0'), 'loss_bbox': tensor(0.4666, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1541, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(45., device='cuda:0'), 'loss_ce_0': tensor(1.2373, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4844, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1079, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(7.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2021, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4794, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1637, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(11., device='cuda:0'), 'loss_ce_2': tensor(1.1928, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4778, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1507, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(39.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1674, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4732, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1619, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(48., device='cuda:0'), 'loss_ce_4': tensor(1.2528, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4762, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1682, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(11., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.2085, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1324, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(4.7619, device='cuda:0'), 'loss_bbox': tensor(0.4475, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1598, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(87.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1018, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4491, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1399, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(62., device='cuda:0'), 'loss_ce_1': tensor(1.1107, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4471, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1659, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(74.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0932, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4539, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1667, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(82., device='cuda:0'), 'loss_ce_3': tensor(1.1027, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4440, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1652, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(87., device='cuda:0'), 'loss_ce_4': tensor(1.1528, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4473, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1690, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(67.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.1229, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5867, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(68.7500, device='cuda:0'), 'loss_bbox': tensor(0.4207, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1904, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(92., device='cuda:0'), 'loss_ce_0': tensor(1.4812, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4380, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1729, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(86., device='cuda:0'), 'loss_ce_1': tensor(1.5459, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4187, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1864, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(90.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5373, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4260, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2113, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(91., device='cuda:0'), 'loss_ce_3': tensor(1.5149, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4192, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2048, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(92., device='cuda:0'), 'loss_ce_4': tensor(1.4881, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4187, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2087, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(88.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.1733, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3939, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(40., device='cuda:0'), 'loss_bbox': tensor(0.4282, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2897, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(79.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3851, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4436, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2672, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(75., device='cuda:0'), 'loss_ce_1': tensor(1.3980, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4288, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2832, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(78., device='cuda:0'), 'loss_ce_2': tensor(1.3873, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4356, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2960, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(77., device='cuda:0'), 'loss_ce_3': tensor(1.3965, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4247, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2963, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(76., device='cuda:0'), 'loss_ce_4': tensor(1.3759, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4301, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2947, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(73., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.7142, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2047, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(24.4444, device='cuda:0'), 'loss_bbox': tensor(0.4259, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2290, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(76., device='cuda:0'), 'loss_ce_0': tensor(1.1935, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4434, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2197, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(71., device='cuda:0'), 'loss_ce_1': tensor(1.2308, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4288, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2282, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(72., device='cuda:0'), 'loss_ce_2': tensor(1.2128, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4287, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2264, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(65., device='cuda:0'), 'loss_ce_3': tensor(1.2425, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4251, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2348, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(63., device='cuda:0'), 'loss_ce_4': tensor(1.2389, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4275, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2322, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(69., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.8151, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2981, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(42.4242, device='cuda:0'), 'loss_bbox': tensor(0.4140, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2847, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(80., device='cuda:0'), 'loss_ce_0': tensor(1.3045, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4304, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2586, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(75., device='cuda:0'), 'loss_ce_1': tensor(1.3412, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4221, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2869, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(70., device='cuda:0'), 'loss_ce_2': tensor(1.3107, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4164, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2871, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(72.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3281, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4104, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2816, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3189, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4180, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2861, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(76.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.6412, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9664, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(9.6774, device='cuda:0'), 'loss_bbox': tensor(0.3445, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1725, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(74., device='cuda:0'), 'loss_ce_0': tensor(0.9722, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3635, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1514, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(81., device='cuda:0'), 'loss_ce_1': tensor(0.9788, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3573, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1782, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(72., device='cuda:0'), 'loss_ce_2': tensor(0.9761, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3477, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1760, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(63.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9937, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3459, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1745, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(49.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9867, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3525, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1743, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(74.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.3831, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2231, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(36., device='cuda:0'), 'loss_bbox': tensor(0.4002, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3147, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(75.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2525, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4143, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2815, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(85., device='cuda:0'), 'loss_ce_1': tensor(1.2517, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4039, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2965, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(75.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1970, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4044, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3210, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(67.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2031, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4019, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3185, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(51., device='cuda:0'), 'loss_ce_4': tensor(1.1981, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4032, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3051, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(75.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.0664, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1581, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(30., device='cuda:0'), 'loss_bbox': tensor(0.3198, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1088, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(56.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1597, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3363, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1119, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(79., device='cuda:0'), 'loss_ce_1': tensor(1.1775, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3245, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1081, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(69., device='cuda:0'), 'loss_ce_2': tensor(1.1630, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3193, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1141, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(48.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1421, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3200, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1136, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(31.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1718, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3227, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1099, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(72.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.9750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [0]  [ 30/100]  eta: 0:00:40  lr: 0.000100  class_error: 30.00  loss: 36.5488 (42.0027)  loss_ce: 1.2961 (1.4053)  loss_bbox: 2.3978 (3.3368)  loss_giou: 2.3808 (2.2836)  loss_ce_0: 1.2525 (1.3913)  loss_bbox_0: 2.4689 (3.3834)  loss_giou_0: 2.3459 (2.2628)  loss_ce_1: 1.2762 (1.4043)  loss_bbox_1: 2.3972 (3.2782)  loss_giou_1: 2.3728 (2.2851)  loss_ce_2: 1.2657 (1.3908)  loss_bbox_2: 2.4636 (3.2912)  loss_giou_2: 2.4071 (2.2962)  loss_ce_3: 1.2713 (1.4237)  loss_bbox_3: 2.4147 (3.2924)  loss_giou_3: 2.4096 (2.3017)  loss_ce_4: 1.2912 (1.3992)  loss_bbox_4: 2.3956 (3.2785)  loss_giou_4: 2.4007 (2.2981)  loss_ce_unscaled: 1.2961 (1.4053)  class_error_unscaled: 44.4444 (56.0735)  loss_bbox_unscaled: 0.4796 (0.6674)  loss_giou_unscaled: 1.1904 (1.1418)  cardinality_error_unscaled: 46.0000 (49.4677)  loss_ce_0_unscaled: 1.2525 (1.3913)  loss_bbox_0_unscaled: 0.4938 (0.6767)  loss_giou_0_unscaled: 1.1729 (1.1314)  cardinality_error_0_unscaled: 23.5000 (51.2419)  loss_ce_1_unscaled: 1.2762 (1.4043)  loss_bbox_1_unscaled: 0.4794 (0.6556)  loss_giou_1_unscaled: 1.1864 (1.1425)  cardinality_error_1_unscaled: 38.5000 (53.0000)  loss_ce_2_unscaled: 1.2657 (1.3908)  loss_bbox_2_unscaled: 0.4927 (0.6582)  loss_giou_2_unscaled: 1.2036 (1.1481)  cardinality_error_2_unscaled: 39.5000 (47.5968)  loss_ce_3_unscaled: 1.2713 (1.4237)  loss_bbox_3_unscaled: 0.4829 (0.6585)  loss_giou_3_unscaled: 1.2048 (1.1508)  cardinality_error_3_unscaled: 31.5000 (45.7742)  loss_ce_4_unscaled: 1.2912 (1.3992)  loss_bbox_4_unscaled: 0.4791 (0.6557)  loss_giou_4_unscaled: 1.2003 (1.1490)  cardinality_error_4_unscaled: 59.5000 (51.7742)  time: 0.2683  data: 0.0117  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.0645, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(39.2857, device='cuda:0'), 'loss_bbox': tensor(0.4368, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3979, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(26.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0536, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4539, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3806, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(75., device='cuda:0'), 'loss_ce_1': tensor(1.0812, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4393, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3848, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(54.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0792, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4242, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3786, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(35., device='cuda:0'), 'loss_ce_3': tensor(1.0737, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4331, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4004, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(22., device='cuda:0'), 'loss_ce_4': tensor(1.0728, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4362, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3940, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(55., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.0834, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1960, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60., device='cuda:0'), 'loss_bbox': tensor(0.3300, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2665, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(20., device='cuda:0'), 'loss_ce_0': tensor(1.2622, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3265, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2332, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(81.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2285, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3266, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2701, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(61.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2077, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3257, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2798, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(35.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2034, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3262, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2694, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(30.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2585, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3258, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2588, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(62., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.2400, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6546, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(80.5556, device='cuda:0'), 'loss_bbox': tensor(0.4046, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4703, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(10.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5634, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4070, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4274, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(40.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5460, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4037, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4604, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(14., device='cuda:0'), 'loss_ce_2': tensor(1.5910, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4032, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4801, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(5.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5984, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4038, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4747, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(3., device='cuda:0'), 'loss_ce_4': tensor(1.6077, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4015, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4643, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(23., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(42.1129, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3937, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(92.8571, device='cuda:0'), 'loss_bbox': tensor(0.3764, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4197, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(19., device='cuda:0'), 'loss_ce_0': tensor(1.3645, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3815, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4059, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(11.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3558, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3796, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4205, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(12.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3409, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3798, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4236, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(15.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3611, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3759, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4210, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(12., device='cuda:0'), 'loss_ce_4': tensor(1.3872, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3728, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4130, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(8., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.4164, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6514, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(98.0769, device='cuda:0'), 'loss_bbox': tensor(0.3402, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3469, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(25.5000, device='cuda:0'), 'loss_ce_0': tensor(1.6175, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3492, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3309, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(10.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5810, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3482, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3589, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(19.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6174, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3505, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3596, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(21.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6233, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3434, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3432, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(19.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6335, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3408, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3355, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(15., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.0920, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6263, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(95.6522, device='cuda:0'), 'loss_bbox': tensor(0.3634, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4519, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(33., device='cuda:0'), 'loss_ce_0': tensor(1.6118, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3776, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4498, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(27., device='cuda:0'), 'loss_ce_1': tensor(1.5761, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3723, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4647, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(30.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5967, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3735, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4695, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(29., device='cuda:0'), 'loss_ce_3': tensor(1.5680, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3678, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4557, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(20., device='cuda:0'), 'loss_ce_4': tensor(1.5892, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3685, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4604, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(19., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(41.0003, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0243, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(86.3636, device='cuda:0'), 'loss_bbox': tensor(0.3318, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3114, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(7.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0290, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3460, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2943, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(5., device='cuda:0'), 'loss_ce_1': tensor(1.0417, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3414, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3106, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(2.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9922, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3445, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3193, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(5.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0278, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3314, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2998, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(18., device='cuda:0'), 'loss_ce_4': tensor(0.9749, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3369, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3039, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(7.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(34.8346, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3245, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(78.4314, device='cuda:0'), 'loss_bbox': tensor(0.3344, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4257, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(12.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3761, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3260, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3762, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(18.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3724, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3271, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3916, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(2.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3427, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3291, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3993, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(18.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3516, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3313, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4016, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(38.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3604, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3282, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3938, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(18.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.6011, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2089, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(64., device='cuda:0'), 'loss_bbox': tensor(0.3869, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.6088, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(50., device='cuda:0'), 'loss_ce_0': tensor(1.2000, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3740, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.5164, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(8., device='cuda:0'), 'loss_ce_1': tensor(1.2083, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3790, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.5693, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(45.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2297, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3806, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.5589, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(75.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2519, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3888, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.5792, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(82., device='cuda:0'), 'loss_ce_4': tensor(1.2413, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3803, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.5606, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(72., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(40.4352, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1294, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(0., device='cuda:0'), 'loss_bbox': tensor(0.4685, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.7438, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(83., device='cuda:0'), 'loss_ce_0': tensor(1.0852, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4644, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.6955, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(24.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1012, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4730, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.7313, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(75., device='cuda:0'), 'loss_ce_2': tensor(1.1149, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4690, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.7287, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(86.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1264, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4619, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.7201, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(88.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0989, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4586, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.7151, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(82., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(44.0963, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [0]  [ 40/100]  eta: 0:00:30  lr: 0.000100  class_error: 0.00  loss: 35.8277 (40.6440)  loss_ce: 1.2231 (1.3863)  loss_bbox: 2.0010 (2.9831)  loss_giou: 2.5793 (2.4312)  loss_ce_0: 1.2525 (1.3730)  loss_bbox_0: 2.0350 (3.0224)  loss_giou_0: 2.5344 (2.3992)  loss_ce_1: 1.2308 (1.3811)  loss_bbox_1: 2.0184 (2.9409)  loss_giou_1: 2.5738 (2.4283)  loss_ce_2: 1.2128 (1.3714)  loss_bbox_2: 2.0159 (2.9495)  loss_giou_2: 2.5919 (2.4385)  loss_ce_3: 1.2425 (1.3981)  loss_bbox_3: 2.0096 (2.9484)  loss_giou_3: 2.5927 (2.4410)  loss_ce_4: 1.2528 (1.3805)  loss_bbox_4: 2.0074 (2.9362)  loss_giou_4: 2.5894 (2.4351)  loss_ce_unscaled: 1.2231 (1.3863)  class_error_unscaled: 42.4242 (59.3537)  loss_bbox_unscaled: 0.4002 (0.5966)  loss_giou_unscaled: 1.2897 (1.2156)  cardinality_error_unscaled: 45.0000 (44.4146)  loss_ce_0_unscaled: 1.2525 (1.3730)  loss_bbox_0_unscaled: 0.4070 (0.6045)  loss_giou_0_unscaled: 1.2672 (1.1996)  cardinality_error_0_unscaled: 40.5000 (46.1098)  loss_ce_1_unscaled: 1.2308 (1.3811)  loss_bbox_1_unscaled: 0.4037 (0.5882)  loss_giou_1_unscaled: 1.2869 (1.2142)  cardinality_error_1_unscaled: 54.5000 (47.8293)  loss_ce_2_unscaled: 1.2128 (1.3714)  loss_bbox_2_unscaled: 0.4032 (0.5899)  loss_giou_2_unscaled: 1.2960 (1.2192)  cardinality_error_2_unscaled: 39.5000 (43.9878)  loss_ce_3_unscaled: 1.2425 (1.3981)  loss_bbox_3_unscaled: 0.4019 (0.5897)  loss_giou_3_unscaled: 1.2963 (1.2205)  cardinality_error_3_unscaled: 38.5000 (42.7561)  loss_ce_4_unscaled: 1.2528 (1.3805)  loss_bbox_4_unscaled: 0.4015 (0.5872)  loss_giou_4_unscaled: 1.2947 (1.2175)  cardinality_error_4_unscaled: 62.0000 (47.9756)  time: 0.2705  data: 0.0118  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.0908, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(7.8947, device='cuda:0'), 'loss_bbox': tensor(0.3375, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4621, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(80.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0763, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3391, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4218, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(28.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0711, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3431, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4633, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(73., device='cuda:0'), 'loss_ce_2': tensor(1.0304, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3417, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4500, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(80., device='cuda:0'), 'loss_ce_3': tensor(1.0210, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3395, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4606, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(80.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0270, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3340, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4389, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(79., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(36.7067, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3191, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(40., device='cuda:0'), 'loss_bbox': tensor(0.3469, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4075, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(87., device='cuda:0'), 'loss_ce_0': tensor(1.2505, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3498, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3929, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(63., device='cuda:0'), 'loss_ce_1': tensor(1.2920, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3550, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4249, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(86.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3059, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3498, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4070, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(87.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3161, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3543, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4246, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(87.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2724, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3434, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3871, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(87.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.9708, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1180, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(9.0909, device='cuda:0'), 'loss_bbox': tensor(0.3214, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3033, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(89., device='cuda:0'), 'loss_ce_0': tensor(1.0625, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3192, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2615, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(72.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0794, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3249, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2979, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(88., device='cuda:0'), 'loss_ce_2': tensor(1.0988, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3245, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3011, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(89., device='cuda:0'), 'loss_ce_3': tensor(1.1199, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3270, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3079, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(89., device='cuda:0'), 'loss_ce_4': tensor(1.0536, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3233, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2956, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(89., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(34.6003, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4506, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(67.5676, device='cuda:0'), 'loss_bbox': tensor(0.3424, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.5306, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(81.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3785, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3370, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4928, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(72.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4079, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3388, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4955, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(81.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3864, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3410, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.5141, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(81.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4343, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3482, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.5269, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(81.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4433, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3446, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.5366, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(81.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.7962, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0975, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(27.0833, device='cuda:0'), 'loss_bbox': tensor(0.3397, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4674, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(76., device='cuda:0'), 'loss_ce_0': tensor(1.1413, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3384, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4401, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(63.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1438, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3380, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4475, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(75., device='cuda:0'), 'loss_ce_2': tensor(1.1372, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3397, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4571, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(75.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1417, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3416, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4645, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(76., device='cuda:0'), 'loss_ce_4': tensor(1.1392, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3401, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4697, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(76., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.3235, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1316, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(19.5652, device='cuda:0'), 'loss_bbox': tensor(0.3861, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4162, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(77., device='cuda:0'), 'loss_ce_0': tensor(1.1693, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3894, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4237, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(66., device='cuda:0'), 'loss_ce_1': tensor(1.1748, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3909, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4243, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(76., device='cuda:0'), 'loss_ce_2': tensor(1.1547, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3896, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4225, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(77., device='cuda:0'), 'loss_ce_3': tensor(1.1597, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3860, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4123, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(77., device='cuda:0'), 'loss_ce_4': tensor(1.1510, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3838, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4009, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(76., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.4024, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1751, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(25., device='cuda:0'), 'loss_bbox': tensor(0.2707, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1590, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(93.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1326, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2694, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1359, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(90., device='cuda:0'), 'loss_ce_1': tensor(1.1181, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2720, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1571, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(92., device='cuda:0'), 'loss_ce_2': tensor(1.1340, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2771, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1705, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(93.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1639, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2765, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1768, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(93., device='cuda:0'), 'loss_ce_4': tensor(1.1031, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2711, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1760, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(91.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(31.7833, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9418, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(10., device='cuda:0'), 'loss_bbox': tensor(0.2634, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3239, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(84., device='cuda:0'), 'loss_ce_0': tensor(0.9666, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2668, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3411, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(76.5000, device='cuda:0'), 'loss_ce_1': tensor(0.9555, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2624, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3267, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(81.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9459, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2700, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3311, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(83., device='cuda:0'), 'loss_ce_3': tensor(0.9805, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2613, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3152, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(80.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9701, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2607, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3199, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(76.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.4395, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2696, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(22.5806, device='cuda:0'), 'loss_bbox': tensor(0.3006, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4632, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(83., device='cuda:0'), 'loss_ce_0': tensor(1.2826, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2870, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4062, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(78., device='cuda:0'), 'loss_ce_1': tensor(1.2636, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3030, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4458, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(80., device='cuda:0'), 'loss_ce_2': tensor(1.2519, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3056, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4569, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(78., device='cuda:0'), 'loss_ce_3': tensor(1.2619, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3111, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4699, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(74., device='cuda:0'), 'loss_ce_4': tensor(1.2548, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3043, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4692, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(66., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(36.9008, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9846, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(17.2414, device='cuda:0'), 'loss_bbox': tensor(0.3635, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4739, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(76., device='cuda:0'), 'loss_ce_0': tensor(1.0420, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3591, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4551, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(82., device='cuda:0'), 'loss_ce_1': tensor(1.0061, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3653, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4699, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(78., device='cuda:0'), 'loss_ce_2': tensor(0.9610, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3653, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4691, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(68.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0022, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3684, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4676, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(68., device='cuda:0'), 'loss_ce_4': tensor(1.0026, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3658, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4727, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(57., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.4185, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [0]  [ 50/100]  eta: 0:00:22  lr: 0.000100  class_error: 17.24  loss: 34.7846 (39.2427)  loss_ce: 1.1751 (1.3415)  loss_bbox: 1.7010 (2.7190)  loss_giou: 2.8395 (2.5038)  loss_ce_0: 1.1693 (1.3293)  loss_bbox_0: 1.7302 (2.7489)  loss_giou_0: 2.8119 (2.4688)  loss_ce_1: 1.1748 (1.3360)  loss_bbox_1: 1.7155 (2.6871)  loss_giou_1: 2.8485 (2.4994)  loss_ce_2: 1.1547 (1.3261)  loss_bbox_2: 1.7227 (2.6951)  loss_giou_2: 2.8450 (2.5085)  loss_ce_3: 1.1639 (1.3514)  loss_bbox_3: 1.7170 (2.6952)  loss_giou_3: 2.8420 (2.5125)  loss_ce_4: 1.1510 (1.3337)  loss_bbox_4: 1.7041 (2.6811)  loss_giou_4: 2.8019 (2.5053)  loss_ce_unscaled: 1.1751 (1.3415)  class_error_unscaled: 39.2857 (52.5397)  loss_bbox_unscaled: 0.3402 (0.5438)  loss_giou_unscaled: 1.4197 (1.2519)  cardinality_error_unscaled: 76.0000 (51.9314)  loss_ce_0_unscaled: 1.1693 (1.3293)  loss_bbox_0_unscaled: 0.3460 (0.5498)  loss_giou_0_unscaled: 1.4059 (1.2344)  cardinality_error_0_unscaled: 63.0000 (50.6471)  loss_ce_1_unscaled: 1.1748 (1.3360)  loss_bbox_1_unscaled: 0.3431 (0.5374)  loss_giou_1_unscaled: 1.4243 (1.2497)  cardinality_error_1_unscaled: 73.0000 (54.3627)  loss_ce_2_unscaled: 1.1547 (1.3261)  loss_bbox_2_unscaled: 0.3445 (0.5390)  loss_giou_2_unscaled: 1.4225 (1.2543)  cardinality_error_2_unscaled: 75.5000 (51.3137)  loss_ce_3_unscaled: 1.1639 (1.3514)  loss_bbox_3_unscaled: 0.3434 (0.5390)  loss_giou_3_unscaled: 1.4210 (1.2562)  cardinality_error_3_unscaled: 74.0000 (50.1961)  loss_ce_4_unscaled: 1.1510 (1.3337)  loss_bbox_4_unscaled: 0.3408 (0.5362)  loss_giou_4_unscaled: 1.4009 (1.2527)  cardinality_error_4_unscaled: 66.0000 (53.8627)  time: 0.2678  data: 0.0102  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.6573, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(87.0968, device='cuda:0'), 'loss_bbox': tensor(0.3216, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4572, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(69., device='cuda:0'), 'loss_ce_0': tensor(1.5801, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3241, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4600, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(77., device='cuda:0'), 'loss_ce_1': tensor(1.6720, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3245, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4431, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(72., device='cuda:0'), 'loss_ce_2': tensor(1.6648, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3257, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4460, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(63.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5901, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3255, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4408, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(58.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5826, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3224, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4529, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(48., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.6627, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1188, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(52., device='cuda:0'), 'loss_bbox': tensor(0.2648, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3497, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(31.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1330, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2462, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3083, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(72., device='cuda:0'), 'loss_ce_1': tensor(1.1307, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2643, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3303, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(41.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1261, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2670, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3354, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(26., device='cuda:0'), 'loss_ce_3': tensor(1.1073, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2736, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3532, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(23.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1034, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2679, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3598, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(15., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.5360, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2621, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(81.3559, device='cuda:0'), 'loss_bbox': tensor(0.3350, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4411, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(17.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2058, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3324, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4418, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(28.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2628, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3384, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4422, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(9.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3156, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3432, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4372, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(21., device='cuda:0'), 'loss_ce_3': tensor(1.2611, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3441, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4425, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(20., device='cuda:0'), 'loss_ce_4': tensor(1.2719, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3385, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4484, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(23.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.8993, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5587, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(97.7273, device='cuda:0'), 'loss_bbox': tensor(0.3591, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4964, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(14.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5218, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3538, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4930, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(25.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5841, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3657, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4940, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(11.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6279, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3665, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4853, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(12.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5606, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3722, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4931, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(17., device='cuda:0'), 'loss_ce_4': tensor(1.5474, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3622, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4969, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(17.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(41.0392, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1573, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(90.4762, device='cuda:0'), 'loss_bbox': tensor(0.3739, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4817, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(7., device='cuda:0'), 'loss_ce_0': tensor(1.0736, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3581, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4760, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(31.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0937, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3812, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.5009, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(3., device='cuda:0'), 'loss_ce_2': tensor(1.0728, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3849, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4751, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(3., device='cuda:0'), 'loss_ce_3': tensor(1.0985, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3883, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4737, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(6., device='cuda:0'), 'loss_ce_4': tensor(1.1081, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3789, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4870, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(6.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.5372, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2776, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(97.5610, device='cuda:0'), 'loss_bbox': tensor(0.2881, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3803, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(19., device='cuda:0'), 'loss_ce_0': tensor(1.2831, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2801, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3649, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(8., device='cuda:0'), 'loss_ce_1': tensor(1.3164, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2941, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3845, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(17., device='cuda:0'), 'loss_ce_2': tensor(1.3226, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3007, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3712, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(16.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2964, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3035, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3921, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(18., device='cuda:0'), 'loss_ce_4': tensor(1.2730, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2910, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3788, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(15.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.9491, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1860, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(89.1892, device='cuda:0'), 'loss_bbox': tensor(0.3181, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3249, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(8.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0488, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3175, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3444, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(4.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0835, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3217, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3236, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(12.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0716, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3292, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3227, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(13.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0594, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3321, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3384, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(15., device='cuda:0'), 'loss_ce_4': tensor(1.1278, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3275, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3304, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(8.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.0986, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3015, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(81.8182, device='cuda:0'), 'loss_bbox': tensor(0.3727, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4891, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(24., device='cuda:0'), 'loss_ce_0': tensor(1.2281, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3626, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4943, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(4.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2591, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3794, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4792, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(9.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2399, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3924, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4778, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(10., device='cuda:0'), 'loss_ce_3': tensor(1.2539, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4035, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.5099, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(4., device='cuda:0'), 'loss_ce_4': tensor(1.2545, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3917, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.5026, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(23., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.7723, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2662, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(64.7059, device='cuda:0'), 'loss_bbox': tensor(0.3589, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3622, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(68., device='cuda:0'), 'loss_ce_0': tensor(1.2225, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3672, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3878, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(43.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2249, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3648, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3652, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(37.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2301, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3659, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3479, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(57., device='cuda:0'), 'loss_ce_3': tensor(1.2267, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3571, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3486, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(25.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2449, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3603, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3583, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(69., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.4195, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0866, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(41.9355, device='cuda:0'), 'loss_bbox': tensor(0.3380, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3664, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(77., device='cuda:0'), 'loss_ce_0': tensor(1.0797, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3442, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4259, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(64.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0849, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3425, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3642, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(66.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0786, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3465, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3445, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(76., device='cuda:0'), 'loss_ce_3': tensor(1.0650, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3456, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3371, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(66.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0763, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3423, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3534, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(80., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.9815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [0]  [ 60/100]  eta: 0:00:17  lr: 0.000100  class_error: 41.94  loss: 34.4803 (38.4925)  loss_ce: 1.1751 (1.3326)  loss_bbox: 1.6876 (2.5462)  loss_giou: 2.8323 (2.5572)  loss_ce_0: 1.1413 (1.3143)  loss_bbox_0: 1.6851 (2.5676)  loss_giou_0: 2.8436 (2.5296)  loss_ce_1: 1.1438 (1.3254)  loss_bbox_1: 1.6921 (2.5234)  loss_giou_1: 2.8497 (2.5528)  loss_ce_2: 1.1372 (1.3178)  loss_bbox_2: 1.7051 (2.5337)  loss_giou_2: 2.8450 (2.5577)  loss_ce_3: 1.1597 (1.3351)  loss_bbox_3: 1.7078 (2.5357)  loss_giou_3: 2.8493 (2.5638)  loss_ce_4: 1.1392 (1.3214)  loss_bbox_4: 1.6923 (2.5189)  loss_giou_4: 2.8019 (2.5592)  loss_ce_unscaled: 1.1751 (1.3326)  class_error_unscaled: 41.9355 (56.7769)  loss_bbox_unscaled: 0.3375 (0.5092)  loss_giou_unscaled: 1.4162 (1.2786)  cardinality_error_unscaled: 76.0000 (48.9262)  loss_ce_0_unscaled: 1.1413 (1.3143)  loss_bbox_0_unscaled: 0.3370 (0.5135)  loss_giou_0_unscaled: 1.4218 (1.2648)  cardinality_error_0_unscaled: 63.5000 (48.2377)  loss_ce_1_unscaled: 1.1438 (1.3254)  loss_bbox_1_unscaled: 0.3384 (0.5047)  loss_giou_1_unscaled: 1.4249 (1.2764)  cardinality_error_1_unscaled: 72.0000 (50.0492)  loss_ce_2_unscaled: 1.1372 (1.3178)  loss_bbox_2_unscaled: 0.3410 (0.5067)  loss_giou_2_unscaled: 1.4225 (1.2789)  cardinality_error_2_unscaled: 68.5000 (47.8033)  loss_ce_3_unscaled: 1.1597 (1.3351)  loss_bbox_3_unscaled: 0.3416 (0.5071)  loss_giou_3_unscaled: 1.4246 (1.2819)  cardinality_error_3_unscaled: 66.5000 (46.1311)  loss_ce_4_unscaled: 1.1392 (1.3214)  loss_bbox_4_unscaled: 0.3385 (0.5038)  loss_giou_4_unscaled: 1.4009 (1.2796)  cardinality_error_4_unscaled: 66.0000 (50.0574)  time: 0.2671  data: 0.0102  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.7321, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(55.5556, device='cuda:0'), 'loss_bbox': tensor(0.3624, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3154, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(90., device='cuda:0'), 'loss_ce_0': tensor(1.6314, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3646, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3589, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(83., device='cuda:0'), 'loss_ce_1': tensor(1.6573, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3652, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3318, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(87., device='cuda:0'), 'loss_ce_2': tensor(1.7036, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3696, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3122, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(89., device='cuda:0'), 'loss_ce_3': tensor(1.6720, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3557, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3030, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(86.5000, device='cuda:0'), 'loss_ce_4': tensor(1.7130, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3619, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3187, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(90.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.7105, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2632, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(20., device='cuda:0'), 'loss_bbox': tensor(0.3258, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3415, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(87., device='cuda:0'), 'loss_ce_0': tensor(1.2323, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3239, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3720, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(77.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2885, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3367, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3517, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(82.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2713, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3424, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3553, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(86.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2820, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3499, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3496, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(85., device='cuda:0'), 'loss_ce_4': tensor(1.2872, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3420, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3533, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(86.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(36.7771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2119, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(28.1250, device='cuda:0'), 'loss_bbox': tensor(0.3778, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4050, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(83.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2053, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3741, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4091, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(73., device='cuda:0'), 'loss_ce_1': tensor(1.2157, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3794, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4078, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(81.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2107, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3858, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4244, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(84., device='cuda:0'), 'loss_ce_3': tensor(1.1937, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3823, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4327, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(81.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2071, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3814, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4234, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(84., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.4880, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1258, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(4., device='cuda:0'), 'loss_bbox': tensor(0.2352, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1658, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(85.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1286, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2334, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1725, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(70.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1919, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2343, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1384, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(84.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2073, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2478, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1687, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(86.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1620, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2431, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1386, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(84., device='cuda:0'), 'loss_ce_4': tensor(1.1515, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2406, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1319, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(87., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.7986, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1013, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(21.4286, device='cuda:0'), 'loss_bbox': tensor(0.2550, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1546, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(77.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1434, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2514, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1817, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(64.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1232, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2584, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1442, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(73., device='cuda:0'), 'loss_ce_2': tensor(1.1563, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2640, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1522, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(75.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1277, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2606, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1593, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(75., device='cuda:0'), 'loss_ce_4': tensor(1.1310, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2612, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1451, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(79., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(31.3560, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0746, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(15.7895, device='cuda:0'), 'loss_bbox': tensor(0.3538, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4334, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(90., device='cuda:0'), 'loss_ce_0': tensor(1.0628, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3507, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4390, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(79.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0523, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3560, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4339, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(88., device='cuda:0'), 'loss_ce_2': tensor(1.1060, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3608, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4351, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(88., device='cuda:0'), 'loss_ce_3': tensor(1.0841, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3541, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4364, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(88., device='cuda:0'), 'loss_ce_4': tensor(1.0890, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3610, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4338, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(90.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.2284, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0035, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(12.8205, device='cuda:0'), 'loss_bbox': tensor(0.2855, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2229, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(80., device='cuda:0'), 'loss_ce_0': tensor(1.0639, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2812, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2280, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(73., device='cuda:0'), 'loss_ce_1': tensor(1.0522, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2953, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2460, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(76.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0661, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2921, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2438, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(79.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0501, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2980, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2485, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(78.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0215, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2917, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2259, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(79.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.5823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1256, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(24., device='cuda:0'), 'loss_bbox': tensor(0.3428, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4379, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(86., device='cuda:0'), 'loss_ce_0': tensor(1.1148, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3399, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4429, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(83., device='cuda:0'), 'loss_ce_1': tensor(1.1406, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3461, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4341, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(86.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1458, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3465, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4471, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(86.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1038, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3476, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4450, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(86., device='cuda:0'), 'loss_ce_4': tensor(1.1389, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3480, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4336, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(87., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.2467, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0649, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(25., device='cuda:0'), 'loss_bbox': tensor(0.3493, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4337, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(87., device='cuda:0'), 'loss_ce_0': tensor(1.0860, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3505, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4609, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(85., device='cuda:0'), 'loss_ce_1': tensor(1.1129, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3495, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4472, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(86., device='cuda:0'), 'loss_ce_2': tensor(1.0871, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3577, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4606, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(86.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0913, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3568, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4514, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(86.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0949, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3560, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4450, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(89., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.4338, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.8454, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(11.1111, device='cuda:0'), 'loss_bbox': tensor(0.3447, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4409, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(68., device='cuda:0'), 'loss_ce_0': tensor(0.8577, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3413, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4652, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(69.5000, device='cuda:0'), 'loss_ce_1': tensor(0.8529, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3444, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4495, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(74.5000, device='cuda:0'), 'loss_ce_2': tensor(0.8516, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3505, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4655, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(73., device='cuda:0'), 'loss_ce_3': tensor(0.8465, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3537, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4544, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(73.5000, device='cuda:0'), 'loss_ce_4': tensor(0.8504, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3532, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4679, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(76., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.9184, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [0]  [ 70/100]  eta: 0:00:12  lr: 0.000100  class_error: 11.11  loss: 34.3741 (37.7054)  loss_ce: 1.1573 (1.3075)  loss_bbox: 1.6902 (2.4152)  loss_giou: 2.7606 (2.5731)  loss_ce_0: 1.1330 (1.2915)  loss_bbox_0: 1.6994 (2.4321)  loss_giou_0: 2.8182 (2.5544)  loss_ce_1: 1.1406 (1.3033)  loss_bbox_1: 1.7127 (2.3979)  loss_giou_1: 2.7691 (2.5703)  loss_ce_2: 1.1563 (1.2984)  loss_bbox_2: 1.7323 (2.4105)  loss_giou_2: 2.7424 (2.5768)  loss_ce_3: 1.1277 (1.3106)  loss_bbox_3: 1.7380 (2.4111)  loss_giou_3: 2.7842 (2.5807)  loss_ce_4: 1.1389 (1.2999)  loss_bbox_4: 1.7116 (2.3963)  loss_giou_4: 2.7577 (2.5756)  loss_ce_unscaled: 1.1573 (1.3075)  class_error_unscaled: 41.9355 (51.8482)  loss_bbox_unscaled: 0.3380 (0.4830)  loss_giou_unscaled: 1.3803 (1.2866)  cardinality_error_unscaled: 69.0000 (53.7887)  loss_ce_0_unscaled: 1.1330 (1.2915)  loss_bbox_0_unscaled: 0.3399 (0.4864)  loss_giou_0_unscaled: 1.4091 (1.2772)  cardinality_error_0_unscaled: 69.5000 (52.1268)  loss_ce_1_unscaled: 1.1406 (1.3033)  loss_bbox_1_unscaled: 0.3425 (0.4796)  loss_giou_1_unscaled: 1.3845 (1.2852)  cardinality_error_1_unscaled: 72.0000 (54.5493)  loss_ce_2_unscaled: 1.1563 (1.2984)  loss_bbox_2_unscaled: 0.3465 (0.4821)  loss_giou_2_unscaled: 1.3712 (1.2884)  cardinality_error_2_unscaled: 73.0000 (52.8310)  loss_ce_3_unscaled: 1.1277 (1.3106)  loss_bbox_3_unscaled: 0.3476 (0.4822)  loss_giou_3_unscaled: 1.3921 (1.2904)  cardinality_error_3_unscaled: 66.5000 (51.2465)  loss_ce_4_unscaled: 1.1389 (1.2999)  loss_bbox_4_unscaled: 0.3423 (0.4793)  loss_giou_4_unscaled: 1.3788 (1.2878)  cardinality_error_4_unscaled: 76.0000 (54.9648)  time: 0.2676  data: 0.0101  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.7535, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(58.4906, device='cuda:0'), 'loss_bbox': tensor(0.3290, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3455, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(48., device='cuda:0'), 'loss_ce_0': tensor(1.6719, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3331, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3697, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(58.5000, device='cuda:0'), 'loss_ce_1': tensor(1.7213, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3364, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3550, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58., device='cuda:0'), 'loss_ce_2': tensor(1.7294, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3345, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3641, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(60.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6823, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3385, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3603, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(56.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6807, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3388, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3652, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(51.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.5525, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0928, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(41.1765, device='cuda:0'), 'loss_bbox': tensor(0.2876, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2524, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(46.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1295, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2908, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2631, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(70., device='cuda:0'), 'loss_ce_1': tensor(1.1287, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2609, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(71.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1183, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2959, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2577, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(66., device='cuda:0'), 'loss_ce_3': tensor(1.0959, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3038, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2589, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(60.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0961, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3037, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2611, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(58., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.5310, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1957, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(46.4286, device='cuda:0'), 'loss_bbox': tensor(0.2793, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2099, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(3., device='cuda:0'), 'loss_ce_0': tensor(1.1477, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2839, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2317, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(40.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1729, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2851, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2174, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(39.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1983, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2866, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2381, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(28.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1831, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2923, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2362, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(30.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2095, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2893, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2361, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(13.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.2394, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5156, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(51.7241, device='cuda:0'), 'loss_bbox': tensor(0.2818, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2156, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(22., device='cuda:0'), 'loss_ce_0': tensor(1.5514, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2822, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2382, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(63., device='cuda:0'), 'loss_ce_1': tensor(1.5479, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2894, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2406, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58., device='cuda:0'), 'loss_ce_2': tensor(1.5132, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2896, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2502, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(44., device='cuda:0'), 'loss_ce_3': tensor(1.4937, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2989, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2554, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(42., device='cuda:0'), 'loss_ce_4': tensor(1.4682, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2951, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2560, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(30.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.5014, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0085, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(31.5789, device='cuda:0'), 'loss_bbox': tensor(0.2521, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1530, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(28.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0487, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2430, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1502, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(71., device='cuda:0'), 'loss_ce_1': tensor(0.9956, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2550, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1521, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(68., device='cuda:0'), 'loss_ce_2': tensor(0.9916, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2547, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1735, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(57.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0043, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2616, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1694, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(53.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0036, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2515, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1475, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(37.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.3611, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9272, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(28.5714, device='cuda:0'), 'loss_bbox': tensor(0.2942, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3508, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(26.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9166, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2833, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3370, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(55., device='cuda:0'), 'loss_ce_1': tensor(0.9618, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2869, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3236, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(51.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9570, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2875, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3359, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(31., device='cuda:0'), 'loss_ce_3': tensor(0.9445, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2882, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3123, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(36.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9803, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2829, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2966, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(28.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.0691, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0156, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(28.1250, device='cuda:0'), 'loss_bbox': tensor(0.3778, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.6451, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(31.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0461, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3747, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.6491, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(53.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0483, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3741, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.6455, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0540, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3755, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.6501, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(45., device='cuda:0'), 'loss_ce_3': tensor(1.0379, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3733, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.6310, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(46.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0463, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3691, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.6268, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(35.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.9643, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3601, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(43.1373, device='cuda:0'), 'loss_bbox': tensor(0.3051, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3854, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(33.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3436, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2965, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3910, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(51.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3985, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2982, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3590, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(47., device='cuda:0'), 'loss_ce_2': tensor(1.3861, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2981, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3766, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(39.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3641, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3042, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3653, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(38.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3764, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2962, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3449, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(34., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(36.5051, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5335, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(57.1429, device='cuda:0'), 'loss_bbox': tensor(0.2268, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1714, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(57., device='cuda:0'), 'loss_ce_0': tensor(1.5513, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2274, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1699, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(67.5000, device='cuda:0'), 'loss_ce_1': tensor(1.6122, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2279, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1635, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(65.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5834, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2282, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1770, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(58.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5460, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2319, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1731, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(59.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5378, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2296, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1736, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(62.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.0912, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1278, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(26.3158, device='cuda:0'), 'loss_bbox': tensor(0.4652, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.6092, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(67.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0912, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4627, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.6182, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(72.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1211, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4649, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.6188, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(70.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1051, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4673, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.6197, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(67., device='cuda:0'), 'loss_ce_3': tensor(1.1350, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4742, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.6241, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(61.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1271, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4673, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.6237, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(69., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(42.9898, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [0]  [ 80/100]  eta: 0:00:07  lr: 0.000100  class_error: 26.32  loss: 33.0310 (37.1170)  loss_ce: 1.1256 (1.3008)  loss_bbox: 1.5254 (2.3083)  loss_giou: 2.6830 (2.5848)  loss_ce_0: 1.1286 (1.2864)  loss_bbox_0: 1.4825 (2.3218)  loss_giou_0: 2.7179 (2.5704)  loss_ce_1: 1.1287 (1.2993)  loss_bbox_1: 1.4908 (2.2942)  loss_giou_1: 2.6636 (2.5823)  loss_ce_2: 1.1458 (1.2941)  loss_bbox_2: 1.4905 (2.3054)  loss_giou_2: 2.6717 (2.5906)  loss_ce_3: 1.1277 (1.3030)  loss_bbox_3: 1.5210 (2.3089)  loss_giou_3: 2.6245 (2.5926)  loss_ce_4: 1.1310 (1.2941)  loss_bbox_4: 1.5183 (2.2933)  loss_giou_4: 2.6375 (2.5868)  loss_ce_unscaled: 1.1256 (1.3008)  class_error_unscaled: 28.1250 (50.5421)  loss_bbox_unscaled: 0.3051 (0.4617)  loss_giou_unscaled: 1.3415 (1.2924)  cardinality_error_unscaled: 67.5000 (51.6420)  loss_ce_0_unscaled: 1.1286 (1.2864)  loss_bbox_0_unscaled: 0.2965 (0.4644)  loss_giou_0_unscaled: 1.3589 (1.2852)  cardinality_error_0_unscaled: 70.0000 (53.1358)  loss_ce_1_unscaled: 1.1287 (1.2993)  loss_bbox_1_unscaled: 0.2982 (0.4588)  loss_giou_1_unscaled: 1.3318 (1.2911)  cardinality_error_1_unscaled: 71.5000 (54.9506)  loss_ce_2_unscaled: 1.1458 (1.2941)  loss_bbox_2_unscaled: 0.2981 (0.4611)  loss_giou_2_unscaled: 1.3359 (1.2953)  cardinality_error_2_unscaled: 67.0000 (52.4506)  loss_ce_3_unscaled: 1.1277 (1.3030)  loss_bbox_3_unscaled: 0.3042 (0.4618)  loss_giou_3_unscaled: 1.3123 (1.2963)  cardinality_error_3_unscaled: 61.5000 (50.9136)  loss_ce_4_unscaled: 1.1310 (1.2941)  loss_bbox_4_unscaled: 0.3037 (0.4587)  loss_giou_4_unscaled: 1.3187 (1.2934)  cardinality_error_4_unscaled: 69.0000 (53.3704)  time: 0.2655  data: 0.0097  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.5132, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(50., device='cuda:0'), 'loss_bbox': tensor(0.3027, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2826, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(41.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5897, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3024, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2896, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(44., device='cuda:0'), 'loss_ce_1': tensor(1.5520, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3055, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2924, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(25.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6169, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3037, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2989, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(41.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5606, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3029, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2904, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(29.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5312, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3028, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2960, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(40.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(36.7606, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9607, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(10., device='cuda:0'), 'loss_bbox': tensor(0.2771, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3525, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(72., device='cuda:0'), 'loss_ce_0': tensor(0.9673, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2805, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3571, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(71.5000, device='cuda:0'), 'loss_ce_1': tensor(0.9502, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2788, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3608, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(68., device='cuda:0'), 'loss_ce_2': tensor(0.9384, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2872, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3881, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(68., device='cuda:0'), 'loss_ce_3': tensor(0.9555, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2804, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3586, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9611, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2807, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3520, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(73., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.5073, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1921, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(28.9474, device='cuda:0'), 'loss_bbox': tensor(0.2481, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3303, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(70.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2192, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2522, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3518, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(63.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2136, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2481, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3370, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2055, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2510, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3622, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(60.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2039, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2383, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3048, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(64.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2150, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2380, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2826, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(70., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.3708, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6348, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(95., device='cuda:0'), 'loss_bbox': tensor(0.3295, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.5294, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(71., device='cuda:0'), 'loss_ce_0': tensor(1.7447, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3301, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.5309, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(62., device='cuda:0'), 'loss_ce_1': tensor(1.6780, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3334, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.5315, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(62.5000, device='cuda:0'), 'loss_ce_2': tensor(1.7493, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3316, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.5573, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(70., device='cuda:0'), 'loss_ce_3': tensor(1.7283, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3292, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.5263, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(72., device='cuda:0'), 'loss_ce_4': tensor(1.7236, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3264, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.5123, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(76., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(41.3284, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0004, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(8.1081, device='cuda:0'), 'loss_bbox': tensor(0.3764, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4570, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(71.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9646, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3693, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4380, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(69.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0189, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3684, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4257, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(61.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9954, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3709, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4637, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(71.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9963, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3731, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4411, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(71., device='cuda:0'), 'loss_ce_4': tensor(0.9769, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3601, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3981, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(75.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.1151, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0424, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(19.3548, device='cuda:0'), 'loss_bbox': tensor(0.3649, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.5609, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(76.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0371, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3656, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.5673, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(69.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0695, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3597, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.5377, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(68., device='cuda:0'), 'loss_ce_2': tensor(1.0512, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3661, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.5788, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(79.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0303, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3514, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.5200, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(77.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0469, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3443, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.5109, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(80., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.4529, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1749, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(27.2727, device='cuda:0'), 'loss_bbox': tensor(0.3827, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4371, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(89., device='cuda:0'), 'loss_ce_0': tensor(1.1295, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3823, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4442, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(78.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1071, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3766, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4265, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(85.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1569, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3824, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4427, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(92., device='cuda:0'), 'loss_ce_3': tensor(1.1019, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3851, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4349, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(87., device='cuda:0'), 'loss_ce_4': tensor(1.1409, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3651, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3859, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(92., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.1894, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2458, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(21.2121, device='cuda:0'), 'loss_bbox': tensor(0.3306, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4365, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(73.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2153, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3291, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4594, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(67., device='cuda:0'), 'loss_ce_1': tensor(1.2349, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3332, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4559, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(67., device='cuda:0'), 'loss_ce_2': tensor(1.2434, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3334, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.5016, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(76.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2447, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3300, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4538, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(75., device='cuda:0'), 'loss_ce_4': tensor(1.2554, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3249, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4367, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(80.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.6943, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3635, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(47.0588, device='cuda:0'), 'loss_bbox': tensor(0.3947, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3672, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(73., device='cuda:0'), 'loss_ce_0': tensor(1.3255, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4019, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3937, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(62., device='cuda:0'), 'loss_ce_1': tensor(1.3568, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3944, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3735, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(65.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3358, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4040, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4180, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(74.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3787, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3983, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3959, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(72.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3604, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3881, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3709, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(75.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.5540, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2621, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(37.5000, device='cuda:0'), 'loss_bbox': tensor(0.2856, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2909, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(77., device='cuda:0'), 'loss_ce_0': tensor(1.2350, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2935, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3049, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(61.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2615, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2875, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2864, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(63., device='cuda:0'), 'loss_ce_2': tensor(1.2726, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2956, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3382, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(73.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2395, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2923, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3117, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(74., device='cuda:0'), 'loss_ce_4': tensor(1.2396, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2857, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2908, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(77.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(34.7946, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [0]  [ 90/100]  eta: 0:00:03  lr: 0.000100  class_error: 37.50  loss: 33.6640 (36.7977)  loss_ce: 1.1921 (1.2940)  loss_bbox: 1.5137 (2.2355)  loss_giou: 2.7016 (2.6094)  loss_ce_0: 1.1477 (1.2816)  loss_bbox_0: 1.4825 (2.2484)  loss_giou_0: 2.7143 (2.5986)  loss_ce_1: 1.1729 (1.2933)  loss_bbox_1: 1.4908 (2.2226)  loss_giou_1: 2.7101 (2.6068)  loss_ce_2: 1.1983 (1.2900)  loss_bbox_2: 1.4905 (2.2348)  loss_giou_2: 2.7283 (2.6213)  loss_ce_3: 1.1831 (1.2965)  loss_bbox_3: 1.5192 (2.2355)  loss_giou_3: 2.7172 (2.6163)  loss_ce_4: 1.2095 (1.2887)  loss_bbox_4: 1.5138 (2.2180)  loss_giou_4: 2.6898 (2.6066)  loss_ce_unscaled: 1.1921 (1.2940)  class_error_unscaled: 31.5789 (48.7733)  loss_bbox_unscaled: 0.3027 (0.4471)  loss_giou_unscaled: 1.3508 (1.3047)  cardinality_error_unscaled: 57.0000 (53.8297)  loss_ce_0_unscaled: 1.1477 (1.2816)  loss_bbox_0_unscaled: 0.2965 (0.4497)  loss_giou_0_unscaled: 1.3571 (1.2993)  cardinality_error_0_unscaled: 63.0000 (54.4286)  loss_ce_1_unscaled: 1.1729 (1.2933)  loss_bbox_1_unscaled: 0.2982 (0.4445)  loss_giou_1_unscaled: 1.3550 (1.3034)  cardinality_error_1_unscaled: 62.5000 (55.7802)  loss_ce_2_unscaled: 1.1983 (1.2900)  loss_bbox_2_unscaled: 0.2981 (0.4470)  loss_giou_2_unscaled: 1.3641 (1.3106)  cardinality_error_2_unscaled: 60.5000 (54.4615)  loss_ce_3_unscaled: 1.1831 (1.2965)  loss_bbox_3_unscaled: 0.3038 (0.4471)  loss_giou_3_unscaled: 1.3586 (1.3081)  cardinality_error_3_unscaled: 60.5000 (52.8516)  loss_ce_4_unscaled: 1.2095 (1.2887)  loss_bbox_4_unscaled: 0.3028 (0.4436)  loss_giou_4_unscaled: 1.3449 (1.3033)  cardinality_error_4_unscaled: 62.5000 (55.6429)  time: 0.2672  data: 0.0101  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.3082, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(55.5556, device='cuda:0'), 'loss_bbox': tensor(0.3908, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.5305, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(49.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3691, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3874, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.5258, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(35.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3262, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3886, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.5286, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(49., device='cuda:0'), 'loss_ce_2': tensor(1.3398, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3854, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.5571, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(57., device='cuda:0'), 'loss_ce_3': tensor(1.3467, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3944, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.5527, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(51., device='cuda:0'), 'loss_ce_4': tensor(1.3506, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3884, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.5355, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(55., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(40.9725, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9838, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(4.3478, device='cuda:0'), 'loss_bbox': tensor(0.3306, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.5411, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(54.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9995, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3305, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.5590, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(35., device='cuda:0'), 'loss_ce_1': tensor(1.0255, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3389, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.5746, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(44., device='cuda:0'), 'loss_ce_2': tensor(1.0131, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3369, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.6024, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(68., device='cuda:0'), 'loss_ce_3': tensor(0.9884, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3461, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.5792, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62., device='cuda:0'), 'loss_ce_4': tensor(0.9882, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3493, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.5717, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(58.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.9421, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2056, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(28.5714, device='cuda:0'), 'loss_bbox': tensor(0.3009, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3178, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(32.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1694, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2917, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2869, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(36., device='cuda:0'), 'loss_ce_1': tensor(1.2015, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3020, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3241, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(41., device='cuda:0'), 'loss_ce_2': tensor(1.1937, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2958, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3391, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(55.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1597, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3047, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3352, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(44., device='cuda:0'), 'loss_ce_4': tensor(1.2091, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3046, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3259, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(46.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(34.7751, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1144, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(32.2581, device='cuda:0'), 'loss_bbox': tensor(0.2999, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4339, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(46., device='cuda:0'), 'loss_ce_0': tensor(1.1099, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2951, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4533, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(47.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1356, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2996, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4409, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(50., device='cuda:0'), 'loss_ce_2': tensor(1.1122, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2908, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4630, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(64.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0815, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2993, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4684, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(55.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1097, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3038, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4517, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(53.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.8661, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0952, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(34.7826, device='cuda:0'), 'loss_bbox': tensor(0.3191, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4453, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(38.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1014, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3141, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4530, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(41., device='cuda:0'), 'loss_ce_1': tensor(1.1228, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3281, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4664, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(50., device='cuda:0'), 'loss_ce_2': tensor(1.1121, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3157, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4900, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1043, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3264, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4861, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(55.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1231, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3239, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4562, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(48., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(36.6615, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0807, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(34.6154, device='cuda:0'), 'loss_bbox': tensor(0.3127, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2930, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(67., device='cuda:0'), 'loss_ce_0': tensor(1.0958, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3106, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3235, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(68.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1067, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3208, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3248, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(75.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0991, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3128, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3561, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(78., device='cuda:0'), 'loss_ce_3': tensor(1.0990, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3203, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3464, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(74.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0869, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3203, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3153, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(72., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(34.8785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3056, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(59.2593, device='cuda:0'), 'loss_bbox': tensor(0.4413, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.5552, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(71., device='cuda:0'), 'loss_ce_0': tensor(1.2693, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4525, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.5817, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(63., device='cuda:0'), 'loss_ce_1': tensor(1.2848, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4547, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.5759, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(73., device='cuda:0'), 'loss_ce_2': tensor(1.2774, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4472, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.5871, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(79., device='cuda:0'), 'loss_ce_3': tensor(1.2597, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4591, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.5915, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(73., device='cuda:0'), 'loss_ce_4': tensor(1.3018, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4507, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.5749, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(73.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(43.0107, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1275, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(15.3846, device='cuda:0'), 'loss_bbox': tensor(0.2938, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1305, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(68.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1629, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2944, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1303, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(73.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1907, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2991, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1482, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(81., device='cuda:0'), 'loss_ce_2': tensor(1.1815, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2992, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1590, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(82., device='cuda:0'), 'loss_ce_3': tensor(1.1857, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2995, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1753, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(76., device='cuda:0'), 'loss_ce_4': tensor(1.1948, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1290, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(76., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.6600, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0745, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(34.3750, device='cuda:0'), 'loss_bbox': tensor(0.3455, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4272, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(36.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0899, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3567, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4752, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(53.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0907, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3535, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4382, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(54.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0819, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3506, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4685, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(45.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0820, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3605, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4866, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(33., device='cuda:0'), 'loss_ce_4': tensor(1.1143, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3548, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4663, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(41., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.4679, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [0]  [ 99/100]  eta: 0:00:00  lr: 0.000100  class_error: 34.38  loss: 34.2912 (36.5720)  loss_ce: 1.1278 (1.2805)  loss_bbox: 1.6474 (2.1861)  loss_giou: 2.8678 (2.6281)  loss_ce_0: 1.1295 (1.2699)  loss_bbox_0: 1.6457 (2.1977)  loss_giou_0: 2.8883 (2.6205)  loss_ce_1: 1.1356 (1.2817)  loss_bbox_1: 1.6659 (2.1768)  loss_giou_1: 2.8531 (2.6286)  loss_ce_2: 1.1569 (1.2780)  loss_bbox_2: 1.6578 (2.1854)  loss_giou_2: 2.9259 (2.6458)  loss_ce_3: 1.1350 (1.2829)  loss_bbox_3: 1.6459 (2.1898)  loss_giou_3: 2.8822 (2.6412)  loss_ce_4: 1.1409 (1.2775)  loss_bbox_4: 1.6243 (2.1730)  loss_giou_4: 2.7962 (2.6285)  loss_ce_unscaled: 1.1278 (1.2805)  class_error_unscaled: 28.9474 (47.3752)  loss_bbox_unscaled: 0.3295 (0.4372)  loss_giou_unscaled: 1.4339 (1.3140)  cardinality_error_unscaled: 68.5000 (53.6250)  loss_ce_0_unscaled: 1.1295 (1.2699)  loss_bbox_0_unscaled: 0.3291 (0.4395)  loss_giou_0_unscaled: 1.4442 (1.3103)  cardinality_error_0_unscaled: 62.0000 (54.0650)  loss_ce_1_unscaled: 1.1356 (1.2817)  loss_bbox_1_unscaled: 0.3332 (0.4354)  loss_giou_1_unscaled: 1.4265 (1.3143)  cardinality_error_1_unscaled: 62.5000 (55.9400)  loss_ce_2_unscaled: 1.1569 (1.2780)  loss_bbox_2_unscaled: 0.3316 (0.4371)  loss_giou_2_unscaled: 1.4630 (1.3229)  cardinality_error_2_unscaled: 68.0000 (55.4800)  loss_ce_3_unscaled: 1.1350 (1.2829)  loss_bbox_3_unscaled: 0.3292 (0.4380)  loss_giou_3_unscaled: 1.4411 (1.3206)  cardinality_error_3_unscaled: 64.5000 (53.3400)  loss_ce_4_unscaled: 1.1409 (1.2775)  loss_bbox_4_unscaled: 0.3249 (0.4346)  loss_giou_4_unscaled: 1.3981 (1.3143)  cardinality_error_4_unscaled: 72.0000 (55.8750)  time: 0.2662  data: 0.0100  max mem: 1537\n",
            "Epoch: [0] Total time: 0:00:36 (0.3659 s / it)\n",
            "Averaged stats: lr: 0.000100  class_error: 34.38  loss: 34.2912 (36.5720)  loss_ce: 1.1278 (1.2805)  loss_bbox: 1.6474 (2.1861)  loss_giou: 2.8678 (2.6281)  loss_ce_0: 1.1295 (1.2699)  loss_bbox_0: 1.6457 (2.1977)  loss_giou_0: 2.8883 (2.6205)  loss_ce_1: 1.1356 (1.2817)  loss_bbox_1: 1.6659 (2.1768)  loss_giou_1: 2.8531 (2.6286)  loss_ce_2: 1.1569 (1.2780)  loss_bbox_2: 1.6578 (2.1854)  loss_giou_2: 2.9259 (2.6458)  loss_ce_3: 1.1350 (1.2829)  loss_bbox_3: 1.6459 (2.1898)  loss_giou_3: 2.8822 (2.6412)  loss_ce_4: 1.1409 (1.2775)  loss_bbox_4: 1.6243 (2.1730)  loss_giou_4: 2.7962 (2.6285)  loss_ce_unscaled: 1.1278 (1.2805)  class_error_unscaled: 28.9474 (47.3752)  loss_bbox_unscaled: 0.3295 (0.4372)  loss_giou_unscaled: 1.4339 (1.3140)  cardinality_error_unscaled: 68.5000 (53.6250)  loss_ce_0_unscaled: 1.1295 (1.2699)  loss_bbox_0_unscaled: 0.3291 (0.4395)  loss_giou_0_unscaled: 1.4442 (1.3103)  cardinality_error_0_unscaled: 62.0000 (54.0650)  loss_ce_1_unscaled: 1.1356 (1.2817)  loss_bbox_1_unscaled: 0.3332 (0.4354)  loss_giou_1_unscaled: 1.4265 (1.3143)  cardinality_error_1_unscaled: 62.5000 (55.9400)  loss_ce_2_unscaled: 1.1569 (1.2780)  loss_bbox_2_unscaled: 0.3316 (0.4371)  loss_giou_2_unscaled: 1.4630 (1.3229)  cardinality_error_2_unscaled: 68.0000 (55.4800)  loss_ce_3_unscaled: 1.1350 (1.2829)  loss_bbox_3_unscaled: 0.3292 (0.4380)  loss_giou_3_unscaled: 1.4411 (1.3206)  cardinality_error_3_unscaled: 64.5000 (53.3400)  loss_ce_4_unscaled: 1.1409 (1.2775)  loss_bbox_4_unscaled: 0.3249 (0.4346)  loss_giou_4_unscaled: 1.3981 (1.3143)  cardinality_error_4_unscaled: 72.0000 (55.8750)\n",
            "Test:  [ 0/25]  eta: 0:00:07  class_error: 100.00  loss: 38.4679 (38.4679)  loss_ce: 1.4004 (1.4004)  loss_bbox: 1.9739 (1.9739)  loss_giou: 3.1551 (3.1551)  loss_ce_0: 1.3493 (1.3493)  loss_bbox_0: 1.8980 (1.8980)  loss_giou_0: 3.0779 (3.0779)  loss_ce_1: 1.3453 (1.3453)  loss_bbox_1: 1.9510 (1.9510)  loss_giou_1: 3.1193 (3.1193)  loss_ce_2: 1.3723 (1.3723)  loss_bbox_2: 1.9243 (1.9243)  loss_giou_2: 3.1489 (3.1489)  loss_ce_3: 1.3514 (1.3514)  loss_bbox_3: 1.9145 (1.9145)  loss_giou_3: 3.1017 (3.1017)  loss_ce_4: 1.3610 (1.3610)  loss_bbox_4: 1.9154 (1.9154)  loss_giou_4: 3.1082 (3.1082)  loss_ce_unscaled: 1.4004 (1.4004)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3948 (0.3948)  loss_giou_unscaled: 1.5776 (1.5776)  cardinality_error_unscaled: 21.5000 (21.5000)  loss_ce_0_unscaled: 1.3493 (1.3493)  loss_bbox_0_unscaled: 0.3796 (0.3796)  loss_giou_0_unscaled: 1.5389 (1.5389)  cardinality_error_0_unscaled: 21.5000 (21.5000)  loss_ce_1_unscaled: 1.3453 (1.3453)  loss_bbox_1_unscaled: 0.3902 (0.3902)  loss_giou_1_unscaled: 1.5597 (1.5597)  cardinality_error_1_unscaled: 21.5000 (21.5000)  loss_ce_2_unscaled: 1.3723 (1.3723)  loss_bbox_2_unscaled: 0.3849 (0.3849)  loss_giou_2_unscaled: 1.5745 (1.5745)  cardinality_error_2_unscaled: 21.5000 (21.5000)  loss_ce_3_unscaled: 1.3514 (1.3514)  loss_bbox_3_unscaled: 0.3829 (0.3829)  loss_giou_3_unscaled: 1.5509 (1.5509)  cardinality_error_3_unscaled: 21.5000 (21.5000)  loss_ce_4_unscaled: 1.3610 (1.3610)  loss_bbox_4_unscaled: 0.3831 (0.3831)  loss_giou_4_unscaled: 1.5541 (1.5541)  cardinality_error_4_unscaled: 21.5000 (21.5000)  time: 0.2977  data: 0.1619  max mem: 1537\n",
            "Test:  [10/25]  eta: 0:00:02  class_error: 100.00  loss: 37.0744 (36.4995)  loss_ce: 1.6509 (1.6333)  loss_bbox: 1.5675 (1.5783)  loss_giou: 2.8535 (2.8412)  loss_ce_0: 1.6123 (1.5978)  loss_bbox_0: 1.6192 (1.6161)  loss_giou_0: 2.9474 (2.9096)  loss_ce_1: 1.6145 (1.6020)  loss_bbox_1: 1.5897 (1.5971)  loss_giou_1: 2.8564 (2.8435)  loss_ce_2: 1.6327 (1.6240)  loss_bbox_2: 1.5606 (1.5735)  loss_giou_2: 2.9037 (2.8862)  loss_ce_3: 1.6194 (1.6081)  loss_bbox_3: 1.6052 (1.6058)  loss_giou_3: 2.9207 (2.8911)  loss_ce_4: 1.6208 (1.6078)  loss_bbox_4: 1.5901 (1.5982)  loss_giou_4: 2.9224 (2.8858)  loss_ce_unscaled: 1.6509 (1.6333)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3135 (0.3157)  loss_giou_unscaled: 1.4268 (1.4206)  cardinality_error_unscaled: 20.0000 (19.0000)  loss_ce_0_unscaled: 1.6123 (1.5978)  loss_bbox_0_unscaled: 0.3238 (0.3232)  loss_giou_0_unscaled: 1.4737 (1.4548)  cardinality_error_0_unscaled: 20.0000 (19.0000)  loss_ce_1_unscaled: 1.6145 (1.6020)  loss_bbox_1_unscaled: 0.3179 (0.3194)  loss_giou_1_unscaled: 1.4282 (1.4218)  cardinality_error_1_unscaled: 20.0000 (19.0000)  loss_ce_2_unscaled: 1.6327 (1.6240)  loss_bbox_2_unscaled: 0.3121 (0.3147)  loss_giou_2_unscaled: 1.4518 (1.4431)  cardinality_error_2_unscaled: 20.0000 (19.0000)  loss_ce_3_unscaled: 1.6194 (1.6081)  loss_bbox_3_unscaled: 0.3210 (0.3212)  loss_giou_3_unscaled: 1.4603 (1.4455)  cardinality_error_3_unscaled: 20.0000 (19.0000)  loss_ce_4_unscaled: 1.6208 (1.6078)  loss_bbox_4_unscaled: 0.3180 (0.3196)  loss_giou_4_unscaled: 1.4612 (1.4429)  cardinality_error_4_unscaled: 20.0000 (19.0000)  time: 0.1384  data: 0.0233  max mem: 1537\n",
            "Test:  [20/25]  eta: 0:00:00  class_error: 100.00  loss: 34.8730 (36.2075)  loss_ce: 1.4340 (1.5832)  loss_bbox: 1.5752 (1.5876)  loss_giou: 2.8548 (2.8694)  loss_ce_0: 1.3957 (1.5325)  loss_bbox_0: 1.5813 (1.6081)  loss_giou_0: 2.8744 (2.9107)  loss_ce_1: 1.3901 (1.5329)  loss_bbox_1: 1.5897 (1.6001)  loss_giou_1: 2.8463 (2.8621)  loss_ce_2: 1.4147 (1.5592)  loss_bbox_2: 1.5606 (1.5766)  loss_giou_2: 2.8809 (2.9066)  loss_ce_3: 1.4075 (1.5392)  loss_bbox_3: 1.5963 (1.6055)  loss_giou_3: 2.8836 (2.9007)  loss_ce_4: 1.4049 (1.5430)  loss_bbox_4: 1.5855 (1.5947)  loss_giou_4: 2.8978 (2.8954)  loss_ce_unscaled: 1.4340 (1.5832)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3150 (0.3175)  loss_giou_unscaled: 1.4274 (1.4347)  cardinality_error_unscaled: 21.0000 (22.2143)  loss_ce_0_unscaled: 1.3957 (1.5325)  loss_bbox_0_unscaled: 0.3163 (0.3216)  loss_giou_0_unscaled: 1.4372 (1.4553)  cardinality_error_0_unscaled: 21.0000 (22.2143)  loss_ce_1_unscaled: 1.3901 (1.5329)  loss_bbox_1_unscaled: 0.3179 (0.3200)  loss_giou_1_unscaled: 1.4232 (1.4311)  cardinality_error_1_unscaled: 21.0000 (22.2143)  loss_ce_2_unscaled: 1.4147 (1.5592)  loss_bbox_2_unscaled: 0.3121 (0.3153)  loss_giou_2_unscaled: 1.4404 (1.4533)  cardinality_error_2_unscaled: 21.0000 (22.2143)  loss_ce_3_unscaled: 1.4075 (1.5392)  loss_bbox_3_unscaled: 0.3193 (0.3211)  loss_giou_3_unscaled: 1.4418 (1.4503)  cardinality_error_3_unscaled: 21.0000 (22.2143)  loss_ce_4_unscaled: 1.4049 (1.5430)  loss_bbox_4_unscaled: 0.3171 (0.3189)  loss_giou_4_unscaled: 1.4489 (1.4477)  cardinality_error_4_unscaled: 21.0000 (22.2143)  time: 0.1301  data: 0.0097  max mem: 1537\n",
            "Test:  [24/25]  eta: 0:00:00  class_error: 100.00  loss: 35.0091 (36.2160)  loss_ce: 1.4340 (1.5919)  loss_bbox: 1.5812 (1.5857)  loss_giou: 2.8535 (2.8637)  loss_ce_0: 1.4006 (1.5478)  loss_bbox_0: 1.5813 (1.6030)  loss_giou_0: 2.8372 (2.8977)  loss_ce_1: 1.4001 (1.5493)  loss_bbox_1: 1.5916 (1.5969)  loss_giou_1: 2.8219 (2.8544)  loss_ce_2: 1.4147 (1.5744)  loss_bbox_2: 1.5606 (1.5733)  loss_giou_2: 2.8809 (2.8980)  loss_ce_3: 1.4118 (1.5563)  loss_bbox_3: 1.5712 (1.6003)  loss_giou_3: 2.8410 (2.8898)  loss_ce_4: 1.4075 (1.5581)  loss_bbox_4: 1.5745 (1.5908)  loss_giou_4: 2.8478 (2.8846)  loss_ce_unscaled: 1.4340 (1.5919)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3162 (0.3171)  loss_giou_unscaled: 1.4268 (1.4318)  cardinality_error_unscaled: 21.0000 (21.2000)  loss_ce_0_unscaled: 1.4006 (1.5478)  loss_bbox_0_unscaled: 0.3163 (0.3206)  loss_giou_0_unscaled: 1.4186 (1.4488)  cardinality_error_0_unscaled: 21.0000 (21.2000)  loss_ce_1_unscaled: 1.4001 (1.5493)  loss_bbox_1_unscaled: 0.3183 (0.3194)  loss_giou_1_unscaled: 1.4110 (1.4272)  cardinality_error_1_unscaled: 21.0000 (21.2000)  loss_ce_2_unscaled: 1.4147 (1.5744)  loss_bbox_2_unscaled: 0.3121 (0.3147)  loss_giou_2_unscaled: 1.4404 (1.4490)  cardinality_error_2_unscaled: 21.0000 (21.2000)  loss_ce_3_unscaled: 1.4118 (1.5563)  loss_bbox_3_unscaled: 0.3142 (0.3201)  loss_giou_3_unscaled: 1.4205 (1.4449)  cardinality_error_3_unscaled: 21.0000 (21.2000)  loss_ce_4_unscaled: 1.4075 (1.5581)  loss_bbox_4_unscaled: 0.3149 (0.3182)  loss_giou_4_unscaled: 1.4239 (1.4423)  cardinality_error_4_unscaled: 21.0000 (21.2000)  time: 0.1315  data: 0.0096  max mem: 1537\n",
            "Test: Total time: 0:00:03 (0.1382 s / it)\n",
            "Averaged stats: class_error: 100.00  loss: 35.0091 (36.2160)  loss_ce: 1.4340 (1.5919)  loss_bbox: 1.5812 (1.5857)  loss_giou: 2.8535 (2.8637)  loss_ce_0: 1.4006 (1.5478)  loss_bbox_0: 1.5813 (1.6030)  loss_giou_0: 2.8372 (2.8977)  loss_ce_1: 1.4001 (1.5493)  loss_bbox_1: 1.5916 (1.5969)  loss_giou_1: 2.8219 (2.8544)  loss_ce_2: 1.4147 (1.5744)  loss_bbox_2: 1.5606 (1.5733)  loss_giou_2: 2.8809 (2.8980)  loss_ce_3: 1.4118 (1.5563)  loss_bbox_3: 1.5712 (1.6003)  loss_giou_3: 2.8410 (2.8898)  loss_ce_4: 1.4075 (1.5581)  loss_bbox_4: 1.5745 (1.5908)  loss_giou_4: 2.8478 (2.8846)  loss_ce_unscaled: 1.4340 (1.5919)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.3162 (0.3171)  loss_giou_unscaled: 1.4268 (1.4318)  cardinality_error_unscaled: 21.0000 (21.2000)  loss_ce_0_unscaled: 1.4006 (1.5478)  loss_bbox_0_unscaled: 0.3163 (0.3206)  loss_giou_0_unscaled: 1.4186 (1.4488)  cardinality_error_0_unscaled: 21.0000 (21.2000)  loss_ce_1_unscaled: 1.4001 (1.5493)  loss_bbox_1_unscaled: 0.3183 (0.3194)  loss_giou_1_unscaled: 1.4110 (1.4272)  cardinality_error_1_unscaled: 21.0000 (21.2000)  loss_ce_2_unscaled: 1.4147 (1.5744)  loss_bbox_2_unscaled: 0.3121 (0.3147)  loss_giou_2_unscaled: 1.4404 (1.4490)  cardinality_error_2_unscaled: 21.0000 (21.2000)  loss_ce_3_unscaled: 1.4118 (1.5563)  loss_bbox_3_unscaled: 0.3142 (0.3201)  loss_giou_3_unscaled: 1.4205 (1.4449)  cardinality_error_3_unscaled: 21.0000 (21.2000)  loss_ce_4_unscaled: 1.4075 (1.5581)  loss_bbox_4_unscaled: 0.3149 (0.3182)  loss_giou_4_unscaled: 1.4239 (1.4423)  cardinality_error_4_unscaled: 21.0000 (21.2000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "loss_dict {'loss_ce': tensor(1.0817, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(27.2727, device='cuda:0'), 'loss_bbox': tensor(0.3856, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4705, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(28., device='cuda:0'), 'loss_ce_0': tensor(1.1353, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3624, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4228, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(34., device='cuda:0'), 'loss_ce_1': tensor(1.1298, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3805, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4731, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(35., device='cuda:0'), 'loss_ce_2': tensor(1.1391, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3791, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4949, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(25.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1432, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3797, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4760, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(14., device='cuda:0'), 'loss_ce_4': tensor(1.1529, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3733, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4514, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(30., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.4922, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [1]  [  0/100]  eta: 0:00:49  lr: 0.000100  class_error: 27.27  loss: 35.6626 (35.6626)  loss_ce: 1.0817 (1.0817)  loss_bbox: 1.9281 (1.9281)  loss_giou: 2.9410 (2.9410)  loss_ce_0: 1.1353 (1.1353)  loss_bbox_0: 1.8120 (1.8120)  loss_giou_0: 2.8456 (2.8456)  loss_ce_1: 1.1298 (1.1298)  loss_bbox_1: 1.9026 (1.9026)  loss_giou_1: 2.9461 (2.9461)  loss_ce_2: 1.1391 (1.1391)  loss_bbox_2: 1.8954 (1.8954)  loss_giou_2: 2.9899 (2.9899)  loss_ce_3: 1.1432 (1.1432)  loss_bbox_3: 1.8984 (1.8984)  loss_giou_3: 2.9520 (2.9520)  loss_ce_4: 1.1529 (1.1529)  loss_bbox_4: 1.8665 (1.8665)  loss_giou_4: 2.9028 (2.9028)  loss_ce_unscaled: 1.0817 (1.0817)  class_error_unscaled: 27.2727 (27.2727)  loss_bbox_unscaled: 0.3856 (0.3856)  loss_giou_unscaled: 1.4705 (1.4705)  cardinality_error_unscaled: 28.0000 (28.0000)  loss_ce_0_unscaled: 1.1353 (1.1353)  loss_bbox_0_unscaled: 0.3624 (0.3624)  loss_giou_0_unscaled: 1.4228 (1.4228)  cardinality_error_0_unscaled: 34.0000 (34.0000)  loss_ce_1_unscaled: 1.1298 (1.1298)  loss_bbox_1_unscaled: 0.3805 (0.3805)  loss_giou_1_unscaled: 1.4731 (1.4731)  cardinality_error_1_unscaled: 35.0000 (35.0000)  loss_ce_2_unscaled: 1.1391 (1.1391)  loss_bbox_2_unscaled: 0.3791 (0.3791)  loss_giou_2_unscaled: 1.4949 (1.4949)  cardinality_error_2_unscaled: 25.5000 (25.5000)  loss_ce_3_unscaled: 1.1432 (1.1432)  loss_bbox_3_unscaled: 0.3797 (0.3797)  loss_giou_3_unscaled: 1.4760 (1.4760)  cardinality_error_3_unscaled: 14.0000 (14.0000)  loss_ce_4_unscaled: 1.1529 (1.1529)  loss_bbox_4_unscaled: 0.3733 (0.3733)  loss_giou_4_unscaled: 1.4514 (1.4514)  cardinality_error_4_unscaled: 30.0000 (30.0000)  time: 0.4917  data: 0.1607  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.2450, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(47.3684, device='cuda:0'), 'loss_bbox': tensor(0.3487, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4626, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(3.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2878, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3342, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4602, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(7., device='cuda:0'), 'loss_ce_1': tensor(1.2814, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3493, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4699, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(14., device='cuda:0'), 'loss_ce_2': tensor(1.2730, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3379, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4922, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(5., device='cuda:0'), 'loss_ce_3': tensor(1.3032, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3408, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4746, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(8.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2562, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3438, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4757, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(8.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.4238, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1761, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75., device='cuda:0'), 'loss_bbox': tensor(0.2143, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0730, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(5., device='cuda:0'), 'loss_ce_0': tensor(1.1427, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2128, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0873, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(12., device='cuda:0'), 'loss_ce_1': tensor(1.1580, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2177, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1010, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(13.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1602, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2233, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1449, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(11., device='cuda:0'), 'loss_ce_3': tensor(1.1354, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2247, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1288, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(3.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1575, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2144, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0836, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(12.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.6032, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1705, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(81.4815, device='cuda:0'), 'loss_bbox': tensor(0.3295, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4149, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(2.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1877, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3306, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4582, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(4., device='cuda:0'), 'loss_ce_1': tensor(1.1328, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3250, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3958, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(13., device='cuda:0'), 'loss_ce_2': tensor(1.1547, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3326, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4712, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(4., device='cuda:0'), 'loss_ce_3': tensor(1.1586, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3415, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4812, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(5., device='cuda:0'), 'loss_ce_4': tensor(1.1861, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3266, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4135, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(6.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(36.9738, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1941, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60.9756, device='cuda:0'), 'loss_bbox': tensor(0.3242, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3375, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(11.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1856, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3115, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3132, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(10.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1729, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3335, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3847, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(6., device='cuda:0'), 'loss_ce_2': tensor(1.2464, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3165, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3664, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(10., device='cuda:0'), 'loss_ce_3': tensor(1.2335, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3143, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3408, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(8., device='cuda:0'), 'loss_ce_4': tensor(1.2030, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3201, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3340, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(8.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.7829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0237, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(52.9412, device='cuda:0'), 'loss_bbox': tensor(0.2358, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2009, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(3., device='cuda:0'), 'loss_ce_0': tensor(1.0561, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2239, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1841, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(1., device='cuda:0'), 'loss_ce_1': tensor(1.0394, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2449, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2257, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(17.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0577, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2359, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2518, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(7., device='cuda:0'), 'loss_ce_3': tensor(1.0780, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2353, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2285, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(7.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0697, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2295, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2120, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(3.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.7669, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3661, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(55.2632, device='cuda:0'), 'loss_bbox': tensor(0.3277, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4264, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(11., device='cuda:0'), 'loss_ce_0': tensor(1.3773, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3241, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4427, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(11., device='cuda:0'), 'loss_ce_1': tensor(1.3846, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3234, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4000, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(22., device='cuda:0'), 'loss_ce_2': tensor(1.4212, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3378, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4846, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(9., device='cuda:0'), 'loss_ce_3': tensor(1.3926, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3409, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4855, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(3.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3830, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3320, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4322, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(12.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.4427, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2638, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(66.6667, device='cuda:0'), 'loss_bbox': tensor(0.1831, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0503, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(33.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2320, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1713, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0412, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(28.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2523, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1867, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0495, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(47., device='cuda:0'), 'loss_ce_2': tensor(1.2979, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1807, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0723, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(50.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2783, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1872, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0802, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(33.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3137, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1835, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0503, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(45., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.6871, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3782, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(62.5000, device='cuda:0'), 'loss_bbox': tensor(0.3418, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2558, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(15., device='cuda:0'), 'loss_ce_0': tensor(1.3529, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3379, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2480, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(10.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4042, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3478, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2531, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(29., device='cuda:0'), 'loss_ce_2': tensor(1.4235, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3511, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2973, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(25.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3736, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3541, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3381, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(18.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3880, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3470, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2727, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(31.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(36.9630, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4769, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75.6757, device='cuda:0'), 'loss_bbox': tensor(0.3218, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3217, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(4.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4516, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3201, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3366, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(8.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4380, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3385, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3675, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(12.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4669, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3347, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3837, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(8.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4472, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3399, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3783, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(11., device='cuda:0'), 'loss_ce_4': tensor(1.4407, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3339, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3637, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(6.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.9685, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0924, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(59.3750, device='cuda:0'), 'loss_bbox': tensor(0.2068, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1013, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(6., device='cuda:0'), 'loss_ce_0': tensor(1.0679, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2140, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1558, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(5.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0667, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2071, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1215, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(18., device='cuda:0'), 'loss_ce_2': tensor(1.0516, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1953, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1414, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(14., device='cuda:0'), 'loss_ce_3': tensor(1.1477, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2083, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1563, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(8., device='cuda:0'), 'loss_ce_4': tensor(1.0955, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1993, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1136, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(9.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.1493, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [1]  [ 10/100]  eta: 0:00:26  lr: 0.000100  class_error: 59.38  loss: 34.0487 (31.7956)  loss_ce: 1.1941 (1.2244)  loss_bbox: 1.6208 (1.4633)  loss_giou: 2.6435 (2.5663)  loss_ce_0: 1.1877 (1.2252)  loss_bbox_0: 1.6004 (1.4285)  loss_giou_0: 2.6264 (2.5727)  loss_ce_1: 1.1729 (1.2237)  loss_bbox_1: 1.6251 (1.4793)  loss_giou_1: 2.7351 (2.5894)  loss_ce_2: 1.2464 (1.2447)  loss_bbox_2: 1.6629 (1.4659)  loss_giou_2: 2.7328 (2.6547)  loss_ce_3: 1.2335 (1.2447)  loss_bbox_3: 1.6996 (1.4849)  loss_giou_3: 2.6817 (2.6488)  loss_ce_4: 1.2030 (1.2406)  loss_bbox_4: 1.6330 (1.4561)  loss_giou_4: 2.6681 (2.5823)  loss_ce_unscaled: 1.1941 (1.2244)  class_error_unscaled: 60.9756 (60.4109)  loss_bbox_unscaled: 0.3242 (0.2927)  loss_giou_unscaled: 1.3217 (1.2832)  cardinality_error_unscaled: 6.0000 (11.2273)  loss_ce_0_unscaled: 1.1877 (1.2252)  loss_bbox_0_unscaled: 0.3201 (0.2857)  loss_giou_0_unscaled: 1.3132 (1.2864)  cardinality_error_0_unscaled: 10.5000 (12.0455)  loss_ce_1_unscaled: 1.1729 (1.2237)  loss_bbox_1_unscaled: 0.3250 (0.2959)  loss_giou_1_unscaled: 1.3675 (1.2947)  cardinality_error_1_unscaled: 17.5000 (20.6818)  loss_ce_2_unscaled: 1.2464 (1.2447)  loss_bbox_2_unscaled: 0.3326 (0.2932)  loss_giou_2_unscaled: 1.3664 (1.3273)  cardinality_error_2_unscaled: 10.0000 (15.4545)  loss_ce_3_unscaled: 1.2335 (1.2447)  loss_bbox_3_unscaled: 0.3399 (0.2970)  loss_giou_3_unscaled: 1.3408 (1.3244)  cardinality_error_3_unscaled: 8.0000 (11.0000)  loss_ce_4_unscaled: 1.2030 (1.2406)  loss_bbox_4_unscaled: 0.3266 (0.2912)  loss_giou_4_unscaled: 1.3340 (1.2912)  cardinality_error_4_unscaled: 9.5000 (15.8636)  time: 0.2896  data: 0.0235  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.2020, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(63.1579, device='cuda:0'), 'loss_bbox': tensor(0.4140, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.6020, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(6.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2487, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4302, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.6659, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(8., device='cuda:0'), 'loss_ce_1': tensor(1.2648, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4125, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.6427, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(23., device='cuda:0'), 'loss_ce_2': tensor(1.2580, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4058, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.6385, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(13., device='cuda:0'), 'loss_ce_3': tensor(1.2262, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4143, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.6569, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(9.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2155, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4215, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.6541, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(9., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(42.4423, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2188, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(67.7419, device='cuda:0'), 'loss_bbox': tensor(0.4069, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3358, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(7., device='cuda:0'), 'loss_ce_0': tensor(1.2616, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.4052, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3601, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(3.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2959, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.4002, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3494, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(4., device='cuda:0'), 'loss_ce_2': tensor(1.2544, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4013, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3825, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(7.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2770, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4082, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3895, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(4., device='cuda:0'), 'loss_ce_4': tensor(1.2714, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4030, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3661, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(4., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(38.9131, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0548, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(61.1111, device='cuda:0'), 'loss_bbox': tensor(0.2844, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2539, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(8.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0588, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2892, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2956, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(1.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0623, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2785, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2586, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(2., device='cuda:0'), 'loss_ce_2': tensor(1.0741, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2739, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2553, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(2., device='cuda:0'), 'loss_ce_3': tensor(1.0314, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3399, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(2.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1061, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2899, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2902, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(8., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.1803, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3017, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(81.8182, device='cuda:0'), 'loss_bbox': tensor(0.3135, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4162, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(10., device='cuda:0'), 'loss_ce_0': tensor(1.4066, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3056, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4365, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(3.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3584, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3090, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4351, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(11.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4135, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3059, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4389, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(4., device='cuda:0'), 'loss_ce_3': tensor(1.3719, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3185, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4773, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(4., device='cuda:0'), 'loss_ce_4': tensor(1.3939, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3128, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4428, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(4., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.6713, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1612, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(73.9130, device='cuda:0'), 'loss_bbox': tensor(0.2990, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2990, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(10.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9939, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2922, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2950, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(11.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0548, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3210, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3929, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(6.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0557, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3240, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4346, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(7.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0209, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3210, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4074, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(4.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0516, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3144, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3734, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(3., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(34.8676, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5802, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(69.4444, device='cuda:0'), 'loss_bbox': tensor(0.2469, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2526, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(26., device='cuda:0'), 'loss_ce_0': tensor(1.5025, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2375, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2439, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(30.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5324, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2648, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3092, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(27., device='cuda:0'), 'loss_ce_2': tensor(1.5019, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2747, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3599, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(31.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5547, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2637, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3402, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(32.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5604, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2493, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2621, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(28., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.3190, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2008, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(47.9167, device='cuda:0'), 'loss_bbox': tensor(0.2864, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3118, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(48., device='cuda:0'), 'loss_ce_0': tensor(1.2579, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2842, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3426, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(45., device='cuda:0'), 'loss_ce_1': tensor(1.2353, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3075, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3987, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(36., device='cuda:0'), 'loss_ce_2': tensor(1.2504, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3224, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4677, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(47., device='cuda:0'), 'loss_ce_3': tensor(1.2539, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3125, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4360, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(48., device='cuda:0'), 'loss_ce_4': tensor(1.2615, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2969, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3545, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(45.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.9356, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1742, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(36.5854, device='cuda:0'), 'loss_bbox': tensor(0.3473, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.5265, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(74., device='cuda:0'), 'loss_ce_0': tensor(1.1868, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3345, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.5299, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(62., device='cuda:0'), 'loss_ce_1': tensor(1.1977, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3498, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.5570, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(67., device='cuda:0'), 'loss_ce_2': tensor(1.1763, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3532, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.5908, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(67.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1729, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3553, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.5988, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(61.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1939, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3459, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.5496, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(68.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.0487, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4120, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(34.7826, device='cuda:0'), 'loss_bbox': tensor(0.2207, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0660, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(85.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4093, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2108, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0633, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(86., device='cuda:0'), 'loss_ce_1': tensor(1.3691, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2272, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1121, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(81.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3922, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2303, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1609, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(81., device='cuda:0'), 'loss_ce_3': tensor(1.4130, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2347, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1451, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(80.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4161, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2217, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0786, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(85., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(31.1679, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1715, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(51.8519, device='cuda:0'), 'loss_bbox': tensor(0.3199, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4225, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(85.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1012, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2942, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3720, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(80., device='cuda:0'), 'loss_ce_1': tensor(1.1619, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3287, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4552, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(85., device='cuda:0'), 'loss_ce_2': tensor(1.1539, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3460, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4925, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(84.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1174, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3370, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4841, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(83.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1376, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3274, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4488, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(84.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(36.8068, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [1]  [ 20/100]  eta: 0:00:22  lr: 0.000100  class_error: 51.85  loss: 33.1319 (32.7107)  loss_ce: 1.2008 (1.2355)  loss_bbox: 1.5675 (1.5138)  loss_giou: 2.6236 (2.6287)  loss_ce_0: 1.2320 (1.2335)  loss_bbox_0: 1.4710 (1.4824)  loss_giou_0: 2.6264 (2.6433)  loss_ce_1: 1.2353 (1.2378)  loss_bbox_1: 1.6052 (1.5366)  loss_giou_1: 2.7351 (2.6812)  loss_ce_2: 1.2504 (1.2487)  loss_bbox_2: 1.6121 (1.5387)  loss_giou_2: 2.7649 (2.7450)  loss_ce_3: 1.2335 (1.2443)  loss_bbox_3: 1.5926 (1.5545)  loss_giou_3: 2.7566 (2.7470)  loss_ce_4: 1.2155 (1.2502)  loss_bbox_4: 1.5722 (1.5206)  loss_giou_4: 2.7090 (2.6689)  loss_ce_unscaled: 1.2008 (1.2355)  class_error_unscaled: 61.1111 (59.6592)  loss_bbox_unscaled: 0.3135 (0.3028)  loss_giou_unscaled: 1.3118 (1.3143)  cardinality_error_unscaled: 10.0000 (23.0952)  loss_ce_0_unscaled: 1.2320 (1.2335)  loss_bbox_0_unscaled: 0.2942 (0.2965)  loss_giou_0_unscaled: 1.3132 (1.3217)  cardinality_error_0_unscaled: 10.5000 (22.0952)  loss_ce_1_unscaled: 1.2353 (1.2378)  loss_bbox_1_unscaled: 0.3210 (0.3073)  loss_giou_1_unscaled: 1.3675 (1.3406)  cardinality_error_1_unscaled: 17.5000 (27.1905)  loss_ce_2_unscaled: 1.2504 (1.2487)  loss_bbox_2_unscaled: 0.3224 (0.3077)  loss_giou_2_unscaled: 1.3825 (1.3725)  cardinality_error_2_unscaled: 10.0000 (24.5476)  loss_ce_3_unscaled: 1.2335 (1.2443)  loss_bbox_3_unscaled: 0.3185 (0.3109)  loss_giou_3_unscaled: 1.3783 (1.3735)  cardinality_error_3_unscaled: 8.0000 (21.5000)  loss_ce_4_unscaled: 1.2155 (1.2502)  loss_bbox_4_unscaled: 0.3144 (0.3041)  loss_giou_4_unscaled: 1.3545 (1.3344)  cardinality_error_4_unscaled: 9.0000 (24.4762)  time: 0.2707  data: 0.0107  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.1906, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(58.5366, device='cuda:0'), 'loss_bbox': tensor(0.2304, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1326, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(77.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1127, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2163, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0999, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(72., device='cuda:0'), 'loss_ce_1': tensor(1.1204, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2498, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2294, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(77.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1530, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2626, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2899, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(78.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0442, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2579, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2951, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(78., device='cuda:0'), 'loss_ce_4': tensor(1.1419, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2374, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1706, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(78., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(31.2927, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2268, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(30.9524, device='cuda:0'), 'loss_bbox': tensor(0.2856, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3928, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(77.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2472, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2451, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3556, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(69., device='cuda:0'), 'loss_ce_1': tensor(1.2514, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2743, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4275, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(75., device='cuda:0'), 'loss_ce_2': tensor(1.2393, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2902, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4715, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(75.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2184, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2901, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4856, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(78., device='cuda:0'), 'loss_ce_4': tensor(1.2313, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2774, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4350, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(78.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.6632, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1666, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(15.3846, device='cuda:0'), 'loss_bbox': tensor(0.2720, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2148, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(82., device='cuda:0'), 'loss_ce_0': tensor(1.1776, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2410, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1701, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(78., device='cuda:0'), 'loss_ce_1': tensor(1.1847, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2639, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2679, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(83.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1776, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2756, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3260, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(86.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1207, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2862, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3655, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(86., device='cuda:0'), 'loss_ce_4': tensor(1.1609, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2659, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2656, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(87., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.1382, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2001, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(19.1489, device='cuda:0'), 'loss_bbox': tensor(0.2561, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2364, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(76., device='cuda:0'), 'loss_ce_0': tensor(1.2320, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2182, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1583, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(64.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2071, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2532, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3001, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(74.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2110, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2624, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3010, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(74.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2067, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2669, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3477, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(75., device='cuda:0'), 'loss_ce_4': tensor(1.2291, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2501, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2689, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(76., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.8253, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3324, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(47.6190, device='cuda:0'), 'loss_bbox': tensor(0.2610, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3047, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(89., device='cuda:0'), 'loss_ce_0': tensor(1.3151, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2532, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3712, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(83., device='cuda:0'), 'loss_ce_1': tensor(1.3504, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2701, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4303, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(88., device='cuda:0'), 'loss_ce_2': tensor(1.3037, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2768, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3932, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(89., device='cuda:0'), 'loss_ce_3': tensor(1.3511, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2845, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4047, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(89.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3964, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2568, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3311, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(89.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.3714, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1684, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(36.3636, device='cuda:0'), 'loss_bbox': tensor(0.2476, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1845, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(82.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1817, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2188, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1184, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(76., device='cuda:0'), 'loss_ce_1': tensor(1.1621, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2435, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2167, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(81.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1816, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2513, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2573, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(83., device='cuda:0'), 'loss_ce_3': tensor(1.1988, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2585, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2757, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(83.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2057, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2357, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1830, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(83., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(31.5928, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6495, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(66.6667, device='cuda:0'), 'loss_bbox': tensor(0.3166, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4385, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(84., device='cuda:0'), 'loss_ce_0': tensor(1.5785, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4166, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(78., device='cuda:0'), 'loss_ce_1': tensor(1.6188, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3156, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4574, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(84.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5811, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3239, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4664, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(86.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6107, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3375, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4964, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(87., device='cuda:0'), 'loss_ce_4': tensor(1.6787, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3174, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4519, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(87., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.4953, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3953, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(50., device='cuda:0'), 'loss_bbox': tensor(0.2476, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1753, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(67.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4100, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2219, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0819, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(50.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3921, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2377, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1574, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(70.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3646, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2445, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2071, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(68., device='cuda:0'), 'loss_ce_3': tensor(1.3939, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2615, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2603, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(70., device='cuda:0'), 'loss_ce_4': tensor(1.4158, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2339, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1514, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(72.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.4536, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4383, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(56.8182, device='cuda:0'), 'loss_bbox': tensor(0.2645, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2816, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(60., device='cuda:0'), 'loss_ce_0': tensor(1.4205, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2366, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2350, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(42., device='cuda:0'), 'loss_ce_1': tensor(1.4220, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2600, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2594, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(66., device='cuda:0'), 'loss_ce_2': tensor(1.3951, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2628, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3080, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(65.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4131, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2787, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3486, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(64.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4417, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2664, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2785, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(68.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(34.6537, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4230, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(71.4286, device='cuda:0'), 'loss_bbox': tensor(0.2912, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2205, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(33.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4208, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2746, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1872, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(30., device='cuda:0'), 'loss_ce_1': tensor(1.4981, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2984, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2546, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(56., device='cuda:0'), 'loss_ce_2': tensor(1.4641, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3057, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2997, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(60., device='cuda:0'), 'loss_ce_3': tensor(1.4533, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3368, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4136, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(57.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4550, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2937, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2336, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(57., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.6726, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [1]  [ 30/100]  eta: 0:00:19  lr: 0.000100  class_error: 71.43  loss: 32.4520 (32.2914)  loss_ce: 1.2020 (1.2625)  loss_bbox: 1.4222 (1.4566)  loss_giou: 2.5633 (2.5924)  loss_ce_0: 1.2487 (1.2581)  loss_bbox_0: 1.2658 (1.3950)  loss_giou_0: 2.5901 (2.5774)  loss_ce_1: 1.2514 (1.2645)  loss_bbox_1: 1.3716 (1.4710)  loss_giou_1: 2.6185 (2.6551)  loss_ce_2: 1.2504 (1.2675)  loss_bbox_2: 1.3840 (1.4868)  loss_giou_2: 2.7198 (2.7189)  loss_ce_3: 1.2262 (1.2626)  loss_bbox_3: 1.4505 (1.5141)  loss_giou_3: 2.7789 (2.7443)  loss_ce_4: 1.2313 (1.2778)  loss_bbox_4: 1.3869 (1.4550)  loss_giou_4: 2.5804 (2.6318)  loss_ce_unscaled: 1.2020 (1.2625)  class_error_unscaled: 51.8519 (55.0246)  loss_bbox_unscaled: 0.2844 (0.2913)  loss_giou_unscaled: 1.2816 (1.2962)  cardinality_error_unscaled: 67.5000 (39.1774)  loss_ce_0_unscaled: 1.2487 (1.2581)  loss_bbox_0_unscaled: 0.2532 (0.2790)  loss_giou_0_unscaled: 1.2950 (1.2887)  cardinality_error_0_unscaled: 50.5000 (35.7097)  loss_ce_1_unscaled: 1.2514 (1.2645)  loss_bbox_1_unscaled: 0.2743 (0.2942)  loss_giou_1_unscaled: 1.3092 (1.3275)  cardinality_error_1_unscaled: 67.0000 (42.8387)  loss_ce_2_unscaled: 1.2504 (1.2675)  loss_bbox_2_unscaled: 0.2768 (0.2974)  loss_giou_2_unscaled: 1.3599 (1.3594)  cardinality_error_2_unscaled: 67.5000 (41.3710)  loss_ce_3_unscaled: 1.2262 (1.2626)  loss_bbox_3_unscaled: 0.2901 (0.3028)  loss_giou_3_unscaled: 1.3895 (1.3721)  cardinality_error_3_unscaled: 64.5000 (39.3710)  loss_ce_4_unscaled: 1.2313 (1.2778)  loss_bbox_4_unscaled: 0.2774 (0.2910)  loss_giou_4_unscaled: 1.2902 (1.3159)  cardinality_error_4_unscaled: 68.5000 (41.6452)  time: 0.2703  data: 0.0108  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.0241, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(58.8235, device='cuda:0'), 'loss_bbox': tensor(0.2151, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1457, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(6., device='cuda:0'), 'loss_ce_0': tensor(1.0286, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1995, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0711, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(19.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0116, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1990, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0771, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(26., device='cuda:0'), 'loss_ce_2': tensor(1.0069, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2030, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0985, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(34., device='cuda:0'), 'loss_ce_3': tensor(1.0195, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2163, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1585, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(29.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0890, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2001, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0982, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(22.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.5066, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4224, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(90.1408, device='cuda:0'), 'loss_bbox': tensor(0.2575, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2885, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(26.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4435, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2459, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2385, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(20.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4288, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2407, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2629, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(23.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4543, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2495, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2870, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(20., device='cuda:0'), 'loss_ce_3': tensor(1.3981, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2572, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3621, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(23.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3960, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2477, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2836, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(24.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(34.3518, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3322, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(92.5926, device='cuda:0'), 'loss_bbox': tensor(0.2702, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2752, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(22., device='cuda:0'), 'loss_ce_0': tensor(1.4293, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2106, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1121, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(16.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4047, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2403, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2484, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(18., device='cuda:0'), 'loss_ce_2': tensor(1.4153, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2431, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2701, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(21., device='cuda:0'), 'loss_ce_3': tensor(1.3513, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2673, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3690, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(22., device='cuda:0'), 'loss_ce_4': tensor(1.3497, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2401, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2271, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(21., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.4300, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4555, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(88.4615, device='cuda:0'), 'loss_bbox': tensor(0.2606, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4128, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(6.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3429, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2505, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2927, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(2., device='cuda:0'), 'loss_ce_1': tensor(1.4520, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2682, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3547, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(9., device='cuda:0'), 'loss_ce_2': tensor(1.4400, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2505, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3715, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(9.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4131, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2936, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4769, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(9., device='cuda:0'), 'loss_ce_4': tensor(1.4854, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2494, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3538, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(5.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(35.7815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3150, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(79.3103, device='cuda:0'), 'loss_bbox': tensor(0.2167, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1395, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(7., device='cuda:0'), 'loss_ce_0': tensor(1.2583, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1668, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9918, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(13., device='cuda:0'), 'loss_ce_1': tensor(1.2776, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1708, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9730, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(8.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2398, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1807, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0453, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(7.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2990, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2140, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2005, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(9.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3401, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2075, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1439, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(7.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.3216, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1863, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(78.7879, device='cuda:0'), 'loss_bbox': tensor(0.2220, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1543, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(10., device='cuda:0'), 'loss_ce_0': tensor(1.1409, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1744, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9826, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(5., device='cuda:0'), 'loss_ce_1': tensor(1.1962, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1828, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9938, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(2.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2019, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1999, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0888, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(4., device='cuda:0'), 'loss_ce_3': tensor(1.1927, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2424, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2800, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(8., device='cuda:0'), 'loss_ce_4': tensor(1.2880, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2163, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1413, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(5., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.4526, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1985, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(68.9655, device='cuda:0'), 'loss_bbox': tensor(0.3046, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2771, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(4.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1025, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2650, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1391, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(9.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1829, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2928, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2107, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(13.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1916, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2718, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1286, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(0.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1632, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3253, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3421, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(4.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1930, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2800, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1595, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(3., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.0847, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2809, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(100., device='cuda:0'), 'loss_bbox': tensor(0.2963, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3725, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(3., device='cuda:0'), 'loss_ce_0': tensor(1.1167, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3057, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4138, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(27., device='cuda:0'), 'loss_ce_1': tensor(1.1283, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3190, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4433, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(40., device='cuda:0'), 'loss_ce_2': tensor(1.0816, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2590, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2276, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(17.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0713, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2914, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4142, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(8., device='cuda:0'), 'loss_ce_4': tensor(1.1353, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3016, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3438, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(12.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(34.8978, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3172, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(81.0811, device='cuda:0'), 'loss_bbox': tensor(0.3454, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4668, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(15.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2257, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3375, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3841, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(34., device='cuda:0'), 'loss_ce_1': tensor(1.2589, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3391, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4308, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(56., device='cuda:0'), 'loss_ce_2': tensor(1.2678, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3363, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3757, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(33., device='cuda:0'), 'loss_ce_3': tensor(1.2276, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3397, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4553, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(27., device='cuda:0'), 'loss_ce_4': tensor(1.2665, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3705, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4555, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(30., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.8170, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2449, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60.6557, device='cuda:0'), 'loss_bbox': tensor(0.2193, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0916, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(2.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2059, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1922, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9822, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(36.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1664, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1847, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9749, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(60., device='cuda:0'), 'loss_ce_2': tensor(1.1891, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1961, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9839, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(48., device='cuda:0'), 'loss_ce_3': tensor(1.2081, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2291, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1680, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(40.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2224, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1952, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9994, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(54., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.4961, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [1]  [ 40/100]  eta: 0:00:16  lr: 0.000100  class_error: 60.66  loss: 30.2430 (31.6603)  loss_ce: 1.2809 (1.2662)  loss_bbox: 1.3032 (1.4193)  loss_giou: 2.4727 (2.5759)  loss_ce_0: 1.2320 (1.2511)  loss_bbox_0: 1.1828 (1.3411)  loss_giou_0: 2.3165 (2.5150)  loss_ce_1: 1.2514 (1.2612)  loss_bbox_1: 1.2659 (1.4094)  loss_giou_1: 2.5093 (2.5914)  loss_ce_2: 1.2393 (1.2630)  loss_bbox_2: 1.2949 (1.4156)  loss_giou_2: 2.5740 (2.6351)  loss_ce_3: 1.2184 (1.2557)  loss_bbox_3: 1.3367 (1.4712)  loss_giou_3: 2.6973 (2.7202)  loss_ce_4: 1.2665 (1.2775)  loss_bbox_4: 1.2471 (1.4060)  loss_giou_4: 2.4672 (2.5853)  loss_ce_unscaled: 1.2809 (1.2662)  class_error_unscaled: 60.6557 (61.0873)  loss_bbox_unscaled: 0.2606 (0.2839)  loss_giou_unscaled: 1.2364 (1.2880)  cardinality_error_unscaled: 26.5000 (32.1463)  loss_ce_0_unscaled: 1.2320 (1.2511)  loss_bbox_0_unscaled: 0.2366 (0.2682)  loss_giou_0_unscaled: 1.1583 (1.2575)  cardinality_error_0_unscaled: 34.0000 (31.4756)  loss_ce_1_unscaled: 1.2514 (1.2612)  loss_bbox_1_unscaled: 0.2532 (0.2819)  loss_giou_1_unscaled: 1.2546 (1.2957)  cardinality_error_1_unscaled: 56.0000 (38.6585)  loss_ce_2_unscaled: 1.2393 (1.2630)  loss_bbox_2_unscaled: 0.2590 (0.2831)  loss_giou_2_unscaled: 1.2870 (1.3175)  cardinality_error_2_unscaled: 48.0000 (36.0366)  loss_ce_3_unscaled: 1.2184 (1.2557)  loss_bbox_3_unscaled: 0.2673 (0.2942)  loss_giou_3_unscaled: 1.3486 (1.3601)  cardinality_error_3_unscaled: 40.5000 (34.1951)  loss_ce_4_unscaled: 1.2665 (1.2775)  loss_bbox_4_unscaled: 0.2494 (0.2812)  loss_giou_4_unscaled: 1.2336 (1.2927)  cardinality_error_4_unscaled: 54.0000 (36.0122)  time: 0.2660  data: 0.0099  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.0490, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(22.7273, device='cuda:0'), 'loss_bbox': tensor(0.2273, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9700, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(55., device='cuda:0'), 'loss_ce_0': tensor(1.0457, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1982, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9009, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(81., device='cuda:0'), 'loss_ce_1': tensor(1.1463, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1988, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8950, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(88.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0691, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2118, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9608, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(86.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0669, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0426, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(85.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0866, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2136, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9317, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(89., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.2046, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0680, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(22.5806, device='cuda:0'), 'loss_bbox': tensor(0.2408, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1952, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(76.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1249, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1800, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0055, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(83., device='cuda:0'), 'loss_ce_1': tensor(1.0925, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1769, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9823, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(84.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0548, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2187, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0654, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(84.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0606, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2408, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2113, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(84.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0831, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2334, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1003, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(84.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.8509, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3495, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(30.4348, device='cuda:0'), 'loss_bbox': tensor(0.2087, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1919, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(87., device='cuda:0'), 'loss_ce_0': tensor(1.2502, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1349, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8116, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(86.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3227, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1310, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8536, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(88.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2531, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1383, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8981, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(88.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2722, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1864, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0715, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(88.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3144, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1476, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8935, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(88.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.7086, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1704, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(25., device='cuda:0'), 'loss_bbox': tensor(0.2276, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2061, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(85.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1579, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1828, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9872, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(85., device='cuda:0'), 'loss_ce_1': tensor(1.1706, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2277, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1684, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(86., device='cuda:0'), 'loss_ce_2': tensor(1.2058, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2090, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0950, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(86., device='cuda:0'), 'loss_ce_3': tensor(1.2181, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2568, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2549, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(86., device='cuda:0'), 'loss_ce_4': tensor(1.1968, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2118, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1039, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(86., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.1091, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2033, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(41.3793, device='cuda:0'), 'loss_bbox': tensor(0.2016, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1651, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(71., device='cuda:0'), 'loss_ce_0': tensor(1.2057, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1722, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0535, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(68.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2435, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1827, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0936, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(70.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2891, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1699, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0327, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(69.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2298, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2276, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2583, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(71., device='cuda:0'), 'loss_ce_4': tensor(1.2791, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1943, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1239, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(71., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.5513, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5187, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(57.7778, device='cuda:0'), 'loss_bbox': tensor(0.2281, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2183, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(76.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4578, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1775, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0648, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(71., device='cuda:0'), 'loss_ce_1': tensor(1.5311, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1534, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9828, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(73., device='cuda:0'), 'loss_ce_2': tensor(1.5309, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1558, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9456, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(74.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5333, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1860, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1111, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(77.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5873, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1757, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0526, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(77.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.1116, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3988, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(40.7407, device='cuda:0'), 'loss_bbox': tensor(0.3353, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3473, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(85., device='cuda:0'), 'loss_ce_0': tensor(1.3494, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2651, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1447, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(68., device='cuda:0'), 'loss_ce_1': tensor(1.3335, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3331, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3561, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(77.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3840, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3274, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3070, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(79.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3982, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3888, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4848, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(86., device='cuda:0'), 'loss_ce_4': tensor(1.4551, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3581, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3577, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(86.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(37.1628, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4491, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(51.1111, device='cuda:0'), 'loss_bbox': tensor(0.2695, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1543, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(76.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4012, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2097, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9224, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(29., device='cuda:0'), 'loss_ce_1': tensor(1.3579, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2623, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1021, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(37.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3942, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2369, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0564, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(46.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3767, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2889, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1686, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(72.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3920, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2784, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1257, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(73., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.0124, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3201, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(45.2381, device='cuda:0'), 'loss_bbox': tensor(0.2233, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0584, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(73.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3428, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1704, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9421, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(15., device='cuda:0'), 'loss_ce_1': tensor(1.3786, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1826, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9741, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(14., device='cuda:0'), 'loss_ce_2': tensor(1.3431, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1650, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9104, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(17.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3521, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2076, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0720, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(61.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3506, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2137, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0597, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(61., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.7870, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1193, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(27.0270, device='cuda:0'), 'loss_bbox': tensor(0.1931, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9828, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(59.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1495, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1654, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8802, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(8., device='cuda:0'), 'loss_ce_1': tensor(1.1775, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1543, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9084, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(9., device='cuda:0'), 'loss_ce_2': tensor(1.1421, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1644, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8949, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(5., device='cuda:0'), 'loss_ce_3': tensor(1.1158, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1808, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9821, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(55., device='cuda:0'), 'loss_ce_4': tensor(1.1105, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1528, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8644, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(36., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.7602, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [1]  [ 50/100]  eta: 0:00:13  lr: 0.000100  class_error: 27.03  loss: 26.6767 (30.7049)  loss_ce: 1.2809 (1.2659)  loss_bbox: 1.1380 (1.3719)  loss_giou: 2.3838 (2.5214)  loss_ce_0: 1.2059 (1.2506)  loss_bbox_0: 0.9612 (1.2602)  loss_giou_0: 2.0109 (2.4027)  loss_ce_1: 1.2435 (1.2640)  loss_bbox_1: 0.9941 (1.3295)  loss_giou_1: 2.1542 (2.4878)  loss_ce_2: 1.2398 (1.2637)  loss_bbox_2: 1.0448 (1.3339)  loss_giou_2: 2.1309 (2.5171)  loss_ce_3: 1.2276 (1.2570)  loss_bbox_3: 1.2038 (1.4177)  loss_giou_3: 2.4226 (2.6439)  loss_ce_4: 1.2791 (1.2791)  loss_bbox_4: 1.0683 (1.3440)  loss_giou_4: 2.2478 (2.4946)  loss_ce_unscaled: 1.2809 (1.2659)  class_error_unscaled: 57.7778 (56.2470)  loss_bbox_unscaled: 0.2276 (0.2744)  loss_giou_unscaled: 1.1919 (1.2607)  cardinality_error_unscaled: 26.5000 (40.4706)  loss_ce_0_unscaled: 1.2059 (1.2506)  loss_bbox_0_unscaled: 0.1922 (0.2520)  loss_giou_0_unscaled: 1.0055 (1.2014)  cardinality_error_0_unscaled: 27.0000 (36.9706)  loss_ce_1_unscaled: 1.2435 (1.2640)  loss_bbox_1_unscaled: 0.1988 (0.2659)  loss_giou_1_unscaled: 1.0771 (1.2439)  cardinality_error_1_unscaled: 37.5000 (43.4118)  loss_ce_2_unscaled: 1.2398 (1.2637)  loss_bbox_2_unscaled: 0.2090 (0.2668)  loss_giou_2_unscaled: 1.0654 (1.2585)  cardinality_error_2_unscaled: 33.0000 (41.4804)  loss_ce_3_unscaled: 1.2276 (1.2570)  loss_bbox_3_unscaled: 0.2408 (0.2835)  loss_giou_3_unscaled: 1.2113 (1.3220)  cardinality_error_3_unscaled: 40.5000 (42.5490)  loss_ce_4_unscaled: 1.2791 (1.2791)  loss_bbox_4_unscaled: 0.2137 (0.2688)  loss_giou_4_unscaled: 1.1239 (1.2473)  cardinality_error_4_unscaled: 36.0000 (43.7157)  time: 0.2657  data: 0.0093  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.4273, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(46.8750, device='cuda:0'), 'loss_bbox': tensor(0.2524, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3272, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(49., device='cuda:0'), 'loss_ce_0': tensor(1.3905, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1939, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0673, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(5.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4282, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1878, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1493, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(16., device='cuda:0'), 'loss_ce_2': tensor(1.3681, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1486, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8990, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(6.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4108, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2136, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2133, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(42., device='cuda:0'), 'loss_ce_4': tensor(1.4130, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1606, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9648, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(20.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.2522, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9631, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(27.7778, device='cuda:0'), 'loss_bbox': tensor(0.2682, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3030, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(40.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9214, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1962, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0396, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(13.5000, device='cuda:0'), 'loss_ce_1': tensor(0.9760, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1998, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0879, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(13.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9917, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2022, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1087, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(7., device='cuda:0'), 'loss_ce_3': tensor(0.9532, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2421, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2435, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(41., device='cuda:0'), 'loss_ce_4': tensor(0.9668, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2135, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1640, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(11., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.1212, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2519, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(65.2174, device='cuda:0'), 'loss_bbox': tensor(0.2169, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1572, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(19.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2695, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1705, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0006, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(8., device='cuda:0'), 'loss_ce_1': tensor(1.2580, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1784, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0800, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(13.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2490, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1577, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0043, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(6., device='cuda:0'), 'loss_ce_3': tensor(1.2243, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2009, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1833, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(32., device='cuda:0'), 'loss_ce_4': tensor(1.2118, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1614, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9951, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(2., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.5731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2988, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(71.4286, device='cuda:0'), 'loss_bbox': tensor(0.1967, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0849, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(10., device='cuda:0'), 'loss_ce_0': tensor(1.3727, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1549, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9226, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(16., device='cuda:0'), 'loss_ce_1': tensor(1.4110, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1512, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9439, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(15.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3376, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1622, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9624, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(13.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3140, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2011, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0200, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(13., device='cuda:0'), 'loss_ce_4': tensor(1.3406, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1708, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9883, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(14., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.9339, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1268, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(85.1852, device='cuda:0'), 'loss_bbox': tensor(0.1969, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9758, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(14.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1972, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1793, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8887, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(11.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1515, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1653, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8921, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(16., device='cuda:0'), 'loss_ce_2': tensor(1.1547, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1627, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8736, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(11., device='cuda:0'), 'loss_ce_3': tensor(1.0820, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1766, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9280, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(10.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1487, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1792, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9743, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(11.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.0342, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3393, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(79.1667, device='cuda:0'), 'loss_bbox': tensor(0.2104, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0958, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(25., device='cuda:0'), 'loss_ce_0': tensor(1.3642, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2123, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1010, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(13., device='cuda:0'), 'loss_ce_1': tensor(1.2954, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1524, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9736, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(5., device='cuda:0'), 'loss_ce_2': tensor(1.3171, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1628, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9775, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(11., device='cuda:0'), 'loss_ce_3': tensor(1.3178, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1715, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0304, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(9., device='cuda:0'), 'loss_ce_4': tensor(1.4210, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1779, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0554, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(29.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.7397, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9191, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(45.4545, device='cuda:0'), 'loss_bbox': tensor(0.1951, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9228, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(9.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9710, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1569, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8351, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(32., device='cuda:0'), 'loss_ce_1': tensor(1.0102, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1358, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7787, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(56.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9974, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1493, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7969, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(43.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9995, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1741, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9366, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(25.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0031, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1471, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8514, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(8., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.7429, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0912, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(47.3684, device='cuda:0'), 'loss_bbox': tensor(0.2106, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1318, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(4.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1295, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1820, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9469, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(46.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0885, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1406, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9084, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(71.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1737, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1715, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9977, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(60., device='cuda:0'), 'loss_ce_3': tensor(1.1346, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1846, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0893, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(56.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2380, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1629, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0043, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(13., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.0593, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5555, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(83.0189, device='cuda:0'), 'loss_bbox': tensor(0.2165, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1056, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(5.5000, device='cuda:0'), 'loss_ce_0': tensor(1.6629, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2022, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0075, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(66., device='cuda:0'), 'loss_ce_1': tensor(1.7625, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2244, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0990, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(71.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6344, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1964, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9756, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(67.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5903, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1856, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0021, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(68.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4887, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1985, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0248, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(31.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(31.0604, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1330, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(59.0909, device='cuda:0'), 'loss_bbox': tensor(0.1773, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9674, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(3.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9846, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1700, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9310, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(64.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0011, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1808, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0185, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(67., device='cuda:0'), 'loss_ce_2': tensor(0.9719, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1773, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9760, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(65., device='cuda:0'), 'loss_ce_3': tensor(0.9710, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1467, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8981, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(65., device='cuda:0'), 'loss_ce_4': tensor(1.0589, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1454, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9172, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(50.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.3537, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [1]  [ 60/100]  eta: 0:00:10  lr: 0.000100  class_error: 59.09  loss: 25.9334 (29.7654)  loss_ce: 1.2033 (1.2568)  loss_bbox: 1.0827 (1.3225)  loss_giou: 2.2636 (2.4711)  loss_ce_0: 1.2057 (1.2466)  loss_bbox_0: 0.8964 (1.2026)  loss_giou_0: 1.8937 (2.3282)  loss_ce_1: 1.2435 (1.2597)  loss_bbox_1: 0.8917 (1.2522)  loss_giou_1: 1.9647 (2.4056)  loss_ce_2: 1.2490 (1.2564)  loss_bbox_2: 0.8250 (1.2538)  loss_giou_2: 1.9512 (2.4183)  loss_ce_3: 1.2243 (1.2476)  loss_bbox_3: 1.0045 (1.3407)  loss_giou_3: 2.1440 (2.5562)  loss_ce_4: 1.2380 (1.2709)  loss_bbox_4: 0.8894 (1.2644)  loss_giou_4: 2.0086 (2.4115)  loss_ce_unscaled: 1.2033 (1.2568)  class_error_unscaled: 45.4545 (57.0358)  loss_bbox_unscaled: 0.2165 (0.2645)  loss_giou_unscaled: 1.1318 (1.2355)  cardinality_error_unscaled: 49.0000 (36.8115)  loss_ce_0_unscaled: 1.2057 (1.2466)  loss_bbox_0_unscaled: 0.1793 (0.2405)  loss_giou_0_unscaled: 0.9469 (1.1641)  cardinality_error_0_unscaled: 32.0000 (35.4426)  loss_ce_1_unscaled: 1.2435 (1.2597)  loss_bbox_1_unscaled: 0.1784 (0.2504)  loss_giou_1_unscaled: 0.9823 (1.2028)  cardinality_error_1_unscaled: 56.5000 (41.9672)  loss_ce_2_unscaled: 1.2490 (1.2564)  loss_bbox_2_unscaled: 0.1650 (0.2508)  loss_giou_2_unscaled: 0.9756 (1.2091)  cardinality_error_2_unscaled: 46.5000 (39.4508)  loss_ce_3_unscaled: 1.2243 (1.2476)  loss_bbox_3_unscaled: 0.2009 (0.2681)  loss_giou_3_unscaled: 1.0720 (1.2781)  cardinality_error_3_unscaled: 61.5000 (41.5246)  loss_ce_4_unscaled: 1.2380 (1.2709)  loss_bbox_4_unscaled: 0.1779 (0.2529)  loss_giou_4_unscaled: 1.0043 (1.2058)  cardinality_error_4_unscaled: 36.0000 (39.6885)  time: 0.2699  data: 0.0108  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.4940, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(78.9474, device='cuda:0'), 'loss_bbox': tensor(0.1511, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9433, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(64.5000, device='cuda:0'), 'loss_ce_0': tensor(1.6926, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1558, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9233, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(90., device='cuda:0'), 'loss_ce_1': tensor(1.7703, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1474, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9028, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(90.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6999, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1464, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8762, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(90., device='cuda:0'), 'loss_ce_3': tensor(1.6891, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1441, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9296, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(90., device='cuda:0'), 'loss_ce_4': tensor(1.5240, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1387, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9281, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(89.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.1030, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0399, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(32., device='cuda:0'), 'loss_bbox': tensor(0.2412, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3139, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(59., device='cuda:0'), 'loss_ce_0': tensor(1.1707, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2329, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2044, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(85.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1273, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2068, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2476, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(87.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1726, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1930, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1276, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(86., device='cuda:0'), 'loss_ce_3': tensor(1.1068, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1837, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2175, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(87.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0722, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2157, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2937, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(85.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.7139, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1166, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(15., device='cuda:0'), 'loss_bbox': tensor(0.2165, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0754, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(61., device='cuda:0'), 'loss_ce_0': tensor(1.2859, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2014, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9444, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(86., device='cuda:0'), 'loss_ce_1': tensor(1.2427, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1989, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9706, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(81.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3423, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2060, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9662, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(86.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2716, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2131, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0653, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(83., device='cuda:0'), 'loss_ce_4': tensor(1.2898, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2153, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0931, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(88.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.8142, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0129, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(25., device='cuda:0'), 'loss_bbox': tensor(0.2278, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1427, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(60., device='cuda:0'), 'loss_ce_0': tensor(1.1302, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1718, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9358, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(72., device='cuda:0'), 'loss_ce_1': tensor(1.0588, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1577, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9887, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(49.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1915, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1859, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9828, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(58.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1710, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2073, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0743, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(60.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1897, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2003, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0654, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(88.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.7228, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1334, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(41.6667, device='cuda:0'), 'loss_bbox': tensor(0.1256, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9303, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(42., device='cuda:0'), 'loss_ce_0': tensor(1.1925, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1349, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9413, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(50.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1611, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1216, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9244, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(13., device='cuda:0'), 'loss_ce_2': tensor(1.1395, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1262, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8671, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(25.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1363, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1290, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9320, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(16.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1980, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1050, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8504, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(76., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.5470, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.8952, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(26.6667, device='cuda:0'), 'loss_bbox': tensor(0.1671, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1664, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9927, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1102, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9366, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(29., device='cuda:0'), 'loss_ce_1': tensor(0.9375, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1410, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0821, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(4.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9853, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1313, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8939, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(5.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0117, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1399, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0451, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(1.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9954, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1474, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0717, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(43.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.1926, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2455, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(57.8947, device='cuda:0'), 'loss_bbox': tensor(0.2640, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0791, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(14.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2352, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2134, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9609, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(17.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1881, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2229, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0695, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(4.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2888, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1947, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9337, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(7., device='cuda:0'), 'loss_ce_3': tensor(1.2444, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2159, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0036, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(5.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2772, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2237, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0190, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(18.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.1056, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2740, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(57.1429, device='cuda:0'), 'loss_bbox': tensor(0.1783, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1081, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(6.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4084, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1385, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9715, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(6.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3651, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1889, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2071, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(18., device='cuda:0'), 'loss_ce_2': tensor(1.5734, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1602, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0206, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(15.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5044, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1543, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0826, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(19., device='cuda:0'), 'loss_ce_4': tensor(1.4532, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1523, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0520, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(9., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.1747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1160, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(65.3846, device='cuda:0'), 'loss_bbox': tensor(0.1653, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9215, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(4.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1949, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1395, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8420, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(5.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2875, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1353, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7955, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(11., device='cuda:0'), 'loss_ce_2': tensor(1.2632, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1600, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0663, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(10.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1858, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1616, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0666, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(10.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1327, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1385, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8569, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(8., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.5279, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9658, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(79.3103, device='cuda:0'), 'loss_bbox': tensor(0.2254, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1153, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(5.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9609, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2224, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1107, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(2.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1723, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1405, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9105, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(12.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9605, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3154, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2433, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(7.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9593, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3294, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2429, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(7.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9651, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2039, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0334, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(9.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.2960, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [1]  [ 70/100]  eta: 0:00:08  lr: 0.000100  class_error: 79.31  loss: 25.2939 (29.0929)  loss_ce: 1.1268 (1.2389)  loss_bbox: 1.0519 (1.2745)  loss_giou: 2.1916 (2.4272)  loss_ce_0: 1.1949 (1.2438)  loss_bbox_0: 0.8590 (1.1544)  loss_giou_0: 1.8888 (2.2755)  loss_ce_1: 1.1723 (1.2557)  loss_bbox_1: 0.7885 (1.1928)  loss_giou_1: 1.9472 (2.3513)  loss_ce_2: 1.1915 (1.2572)  loss_bbox_2: 0.8134 (1.2053)  loss_giou_2: 1.9512 (2.3587)  loss_ce_3: 1.1710 (1.2449)  loss_bbox_3: 0.9185 (1.2842)  loss_giou_3: 2.0903 (2.4965)  loss_ce_4: 1.1980 (1.2622)  loss_bbox_4: 0.8147 (1.2090)  loss_giou_4: 2.0086 (2.3610)  loss_ce_unscaled: 1.1268 (1.2389)  class_error_unscaled: 57.1429 (55.7492)  loss_bbox_unscaled: 0.2104 (0.2549)  loss_giou_unscaled: 1.0958 (1.2136)  cardinality_error_unscaled: 14.5000 (36.5282)  loss_ce_0_unscaled: 1.1949 (1.2438)  loss_bbox_0_unscaled: 0.1718 (0.2309)  loss_giou_0_unscaled: 0.9444 (1.1378)  cardinality_error_0_unscaled: 17.5000 (36.7183)  loss_ce_1_unscaled: 1.1723 (1.2557)  loss_bbox_1_unscaled: 0.1577 (0.2386)  loss_giou_1_unscaled: 0.9736 (1.1756)  cardinality_error_1_unscaled: 16.0000 (41.3028)  loss_ce_2_unscaled: 1.1915 (1.2572)  loss_bbox_2_unscaled: 0.1627 (0.2411)  loss_giou_2_unscaled: 0.9756 (1.1794)  cardinality_error_2_unscaled: 13.5000 (39.4225)  loss_ce_3_unscaled: 1.1710 (1.2449)  loss_bbox_3_unscaled: 0.1837 (0.2568)  loss_giou_3_unscaled: 1.0451 (1.2482)  cardinality_error_3_unscaled: 25.5000 (41.0493)  loss_ce_4_unscaled: 1.1980 (1.2622)  loss_bbox_4_unscaled: 0.1629 (0.2418)  loss_giou_4_unscaled: 1.0043 (1.1805)  cardinality_error_4_unscaled: 18.5000 (41.3732)  time: 0.2741  data: 0.0116  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.1246, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(68.4211, device='cuda:0'), 'loss_bbox': tensor(0.1634, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8688, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(3., device='cuda:0'), 'loss_ce_0': tensor(1.1778, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1562, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9069, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(4.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1944, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1448, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8206, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(13., device='cuda:0'), 'loss_ce_2': tensor(1.1744, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0560, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(10., device='cuda:0'), 'loss_ce_3': tensor(1.1833, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2162, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1184, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(3., device='cuda:0'), 'loss_ce_4': tensor(1.2324, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1567, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9147, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(7., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.4833, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2280, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(76.9231, device='cuda:0'), 'loss_bbox': tensor(0.2478, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3518, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(6., device='cuda:0'), 'loss_ce_0': tensor(1.2285, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2403, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4373, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(7., device='cuda:0'), 'loss_ce_1': tensor(1.3598, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1399, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1072, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(7.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2104, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3702, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.5808, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(6.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1185, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4035, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.5560, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(29.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1638, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1997, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3215, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(10.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(34.8731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2407, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(64.2857, device='cuda:0'), 'loss_bbox': tensor(0.1382, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0571, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(26., device='cuda:0'), 'loss_ce_0': tensor(1.2417, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1339, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0683, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(22.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2762, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1158, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9161, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(25.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2242, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1806, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1735, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(40., device='cuda:0'), 'loss_ce_3': tensor(1.2710, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1518, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0639, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(75.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2342, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1272, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9580, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(41., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.1001, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.8610, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(80.4878, device='cuda:0'), 'loss_bbox': tensor(0.2867, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3824, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(28.5000, device='cuda:0'), 'loss_ce_0': tensor(1.8919, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3065, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3386, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(11.5000, device='cuda:0'), 'loss_ce_1': tensor(2.0272, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1455, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0380, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(33., device='cuda:0'), 'loss_ce_2': tensor(2.0203, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4438, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4604, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(39., device='cuda:0'), 'loss_ce_3': tensor(2.0566, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4711, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4918, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(73.5000, device='cuda:0'), 'loss_ce_4': tensor(1.9531, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1948, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1464, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(48., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(39.6008, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9601, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(38.0952, device='cuda:0'), 'loss_bbox': tensor(0.2069, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1857, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(40., device='cuda:0'), 'loss_ce_0': tensor(0.8768, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1611, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0639, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(22., device='cuda:0'), 'loss_ce_1': tensor(0.9282, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1519, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9971, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(59.5000, device='cuda:0'), 'loss_ce_2': tensor(0.8456, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1758, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1063, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(56., device='cuda:0'), 'loss_ce_3': tensor(0.9303, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1986, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1906, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(75.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9005, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1523, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9933, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(62., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.5187, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9598, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(51.7241, device='cuda:0'), 'loss_bbox': tensor(0.1499, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0262, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(36., device='cuda:0'), 'loss_ce_0': tensor(0.9821, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1306, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9840, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(20., device='cuda:0'), 'loss_ce_1': tensor(0.8993, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1417, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0787, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(56.5000, device='cuda:0'), 'loss_ce_2': tensor(0.8231, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2256, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3203, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(56.5000, device='cuda:0'), 'loss_ce_3': tensor(0.8773, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2117, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2622, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(69., device='cuda:0'), 'loss_ce_4': tensor(0.9397, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1380, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9915, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(57.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.8748, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1249, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(45., device='cuda:0'), 'loss_bbox': tensor(0.1414, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0713, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(37.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1417, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1226, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9964, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(23.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0689, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1258, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0655, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(55.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1072, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1630, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0583, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(59.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1190, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1631, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0671, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(50.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1473, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1251, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9887, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(60., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.2794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0043, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(30., device='cuda:0'), 'loss_bbox': tensor(0.1209, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7697, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(40., device='cuda:0'), 'loss_ce_0': tensor(1.1015, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1154, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7299, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(33., device='cuda:0'), 'loss_ce_1': tensor(0.9952, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1161, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8158, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(65., device='cuda:0'), 'loss_ce_2': tensor(1.0697, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1151, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.6979, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(58., device='cuda:0'), 'loss_ce_3': tensor(1.1452, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1202, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7582, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(49., device='cuda:0'), 'loss_ce_4': tensor(1.1301, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1370, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8955, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(61., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.2880, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.8478, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(25., device='cuda:0'), 'loss_bbox': tensor(0.1889, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8077, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(64.5000, device='cuda:0'), 'loss_ce_0': tensor(0.7940, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1707, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7926, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(55.5000, device='cuda:0'), 'loss_ce_1': tensor(0.8201, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2509, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1758, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(76., device='cuda:0'), 'loss_ce_2': tensor(0.8782, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1823, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7936, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(78., device='cuda:0'), 'loss_ce_3': tensor(0.8829, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1796, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7751, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(58.5000, device='cuda:0'), 'loss_ce_4': tensor(0.8779, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2550, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1266, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(69.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.9903, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2231, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(50., device='cuda:0'), 'loss_bbox': tensor(0.0937, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8879, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(64., device='cuda:0'), 'loss_ce_0': tensor(1.3252, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0959, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8587, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(55.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2996, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1564, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1027, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(72., device='cuda:0'), 'loss_ce_2': tensor(1.3712, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1075, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8073, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(79., device='cuda:0'), 'loss_ce_3': tensor(1.2489, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1061, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8328, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(54., device='cuda:0'), 'loss_ce_4': tensor(1.3262, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1064, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9035, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(73.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.7801, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [1]  [ 80/100]  eta: 0:00:05  lr: 0.000100  class_error: 50.00  loss: 23.7947 (28.6015)  loss_ce: 1.1166 (1.2288)  loss_bbox: 0.8354 (1.2244)  loss_giou: 2.1427 (2.3845)  loss_ce_0: 1.1778 (1.2354)  loss_bbox_0: 0.7791 (1.1127)  loss_giou_0: 1.8888 (2.2459)  loss_ce_1: 1.1723 (1.2472)  loss_bbox_1: 0.7240 (1.1375)  loss_giou_1: 1.9941 (2.3108)  loss_ce_2: 1.1744 (1.2467)  loss_bbox_2: 0.9032 (1.1906)  loss_giou_2: 2.0413 (2.3405)  loss_ce_3: 1.1452 (1.2373)  loss_bbox_3: 0.8981 (1.2628)  loss_giou_3: 2.1331 (2.4627)  loss_ce_4: 1.1638 (1.2534)  loss_bbox_4: 0.7615 (1.1580)  loss_giou_4: 1.9865 (2.3224)  loss_ce_unscaled: 1.1166 (1.2288)  class_error_unscaled: 50.0000 (55.4090)  loss_bbox_unscaled: 0.1671 (0.2449)  loss_giou_unscaled: 1.0713 (1.1923)  cardinality_error_unscaled: 36.0000 (36.2840)  loss_ce_0_unscaled: 1.1778 (1.2354)  loss_bbox_0_unscaled: 0.1558 (0.2225)  loss_giou_0_unscaled: 0.9444 (1.1229)  cardinality_error_0_unscaled: 22.5000 (35.3333)  loss_ce_1_unscaled: 1.1723 (1.2472)  loss_bbox_1_unscaled: 0.1448 (0.2275)  loss_giou_1_unscaled: 0.9971 (1.1554)  cardinality_error_1_unscaled: 33.0000 (41.9259)  loss_ce_2_unscaled: 1.1744 (1.2467)  loss_bbox_2_unscaled: 0.1806 (0.2381)  loss_giou_2_unscaled: 1.0206 (1.1702)  cardinality_error_2_unscaled: 40.0000 (40.5123)  loss_ce_3_unscaled: 1.1452 (1.2373)  loss_bbox_3_unscaled: 0.1796 (0.2526)  loss_giou_3_unscaled: 1.0666 (1.2314)  cardinality_error_3_unscaled: 50.5000 (42.6235)  loss_ce_4_unscaled: 1.1638 (1.2534)  loss_bbox_4_unscaled: 0.1523 (0.2316)  loss_giou_4_unscaled: 0.9933 (1.1612)  cardinality_error_4_unscaled: 57.5000 (42.3148)  time: 0.2768  data: 0.0114  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.1920, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(28.5714, device='cuda:0'), 'loss_bbox': tensor(0.1491, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8211, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(52.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2255, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1458, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8345, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(58., device='cuda:0'), 'loss_ce_1': tensor(1.0839, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1672, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0329, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(63.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1805, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1544, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8448, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(68.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1733, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1634, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8784, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(40., device='cuda:0'), 'loss_ce_4': tensor(1.2573, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1450, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8506, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(62.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.0607, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3527, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(56.2500, device='cuda:0'), 'loss_bbox': tensor(0.1454, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9481, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(33.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3969, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1347, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9165, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(44.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3314, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1936, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0992, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(55.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4247, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1444, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8897, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(58.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3499, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2217, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1209, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(21., device='cuda:0'), 'loss_ce_4': tensor(1.4003, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1384, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8915, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(47.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.6817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9031, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(40.9091, device='cuda:0'), 'loss_bbox': tensor(0.1781, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9031, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(37., device='cuda:0'), 'loss_ce_0': tensor(0.8896, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1583, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8378, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(43.5000, device='cuda:0'), 'loss_ce_1': tensor(0.8969, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2050, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9619, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(49., device='cuda:0'), 'loss_ce_2': tensor(0.9667, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1663, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8360, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(57., device='cuda:0'), 'loss_ce_3': tensor(0.9816, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2677, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1173, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(28.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9778, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1586, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8291, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(43.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.0695, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6753, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(83.7209, device='cuda:0'), 'loss_bbox': tensor(0.1441, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9228, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(15.5000, device='cuda:0'), 'loss_ce_0': tensor(1.6819, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2292, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2196, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(17., device='cuda:0'), 'loss_ce_1': tensor(1.6401, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3104, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3394, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(24.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6328, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1890, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0352, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(31.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5571, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3523, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4098, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(6.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6985, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1338, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8664, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(34., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.0795, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9407, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(53.3333, device='cuda:0'), 'loss_bbox': tensor(0.1549, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9639, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(1., device='cuda:0'), 'loss_ce_0': tensor(0.8685, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1754, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0631, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(10., device='cuda:0'), 'loss_ce_1': tensor(0.7953, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2098, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1223, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(13.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9137, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1619, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0143, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(23.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9833, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3244, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3230, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(8., device='cuda:0'), 'loss_ce_4': tensor(0.9613, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1448, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9419, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(15.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.0482, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9674, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(80.9524, device='cuda:0'), 'loss_bbox': tensor(0.1808, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9558, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(9.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9143, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1591, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9279, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(22.5000, device='cuda:0'), 'loss_ce_1': tensor(0.8827, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1553, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9138, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(32., device='cuda:0'), 'loss_ce_2': tensor(0.9284, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1475, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8951, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(38.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9115, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2507, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2166, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(18.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9108, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1574, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9163, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(28.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.2244, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2828, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(65.7143, device='cuda:0'), 'loss_bbox': tensor(0.1403, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9030, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(12., device='cuda:0'), 'loss_ce_0': tensor(1.3482, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1467, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9556, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(17.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2846, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1442, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9591, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(20., device='cuda:0'), 'loss_ce_2': tensor(1.3090, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1436, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9398, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(22., device='cuda:0'), 'loss_ce_3': tensor(1.2846, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2023, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1748, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(18., device='cuda:0'), 'loss_ce_4': tensor(1.2572, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1361, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9580, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(35., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.9519, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9697, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(54.5455, device='cuda:0'), 'loss_bbox': tensor(0.2000, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1368, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(6.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9485, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1837, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9745, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(19., device='cuda:0'), 'loss_ce_1': tensor(1.0053, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1911, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1091, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(28.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9731, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1921, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0809, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(26.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9935, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2061, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1356, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(32.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9223, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2032, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1675, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(38., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.6854, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6925, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(69.8413, device='cuda:0'), 'loss_bbox': tensor(0.1843, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1565, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(11., device='cuda:0'), 'loss_ce_0': tensor(1.7649, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2024, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1379, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(7.5000, device='cuda:0'), 'loss_ce_1': tensor(1.7226, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1823, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1656, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(15.5000, device='cuda:0'), 'loss_ce_2': tensor(1.7075, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1757, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1020, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(15., device='cuda:0'), 'loss_ce_3': tensor(1.7620, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1653, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0710, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(37.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6703, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1897, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2372, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(42., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.4333, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6321, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75., device='cuda:0'), 'loss_bbox': tensor(0.2437, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2789, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(23., device='cuda:0'), 'loss_ce_0': tensor(1.6061, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2379, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1853, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(18.5000, device='cuda:0'), 'loss_ce_1': tensor(1.6472, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2246, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2097, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(23., device='cuda:0'), 'loss_ce_2': tensor(1.5411, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2127, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1176, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(31.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6641, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2028, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1193, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(54.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6245, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2335, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2021, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(55., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.5174, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [1]  [ 90/100]  eta: 0:00:02  lr: 0.000100  class_error: 75.00  loss: 23.7947 (28.2667)  loss_ce: 1.1249 (1.2323)  loss_bbox: 0.7744 (1.1844)  loss_giou: 1.9116 (2.3420)  loss_ce_0: 1.1778 (1.2386)  loss_bbox_0: 0.7916 (1.0879)  loss_giou_0: 1.9490 (2.2200)  loss_ce_1: 1.0839 (1.2452)  loss_bbox_1: 0.7764 (1.1215)  loss_giou_1: 2.1310 (2.2967)  loss_ce_2: 1.1744 (1.2479)  loss_bbox_2: 0.8787 (1.1525)  loss_giou_2: 2.0704 (2.2977)  loss_ce_3: 1.1452 (1.2404)  loss_bbox_3: 1.0141 (1.2535)  loss_giou_3: 2.2386 (2.4463)  loss_ce_4: 1.1638 (1.2550)  loss_bbox_4: 0.7249 (1.1209)  loss_giou_4: 1.9160 (2.2839)  loss_ce_unscaled: 1.1249 (1.2323)  class_error_unscaled: 54.5455 (56.0107)  loss_bbox_unscaled: 0.1549 (0.2369)  loss_giou_unscaled: 0.9558 (1.1710)  cardinality_error_unscaled: 26.0000 (34.5110)  loss_ce_0_unscaled: 1.1778 (1.2386)  loss_bbox_0_unscaled: 0.1583 (0.2176)  loss_giou_0_unscaled: 0.9745 (1.1100)  cardinality_error_0_unscaled: 20.0000 (34.2857)  loss_ce_1_unscaled: 1.0839 (1.2452)  loss_bbox_1_unscaled: 0.1553 (0.2243)  loss_giou_1_unscaled: 1.0655 (1.1484)  cardinality_error_1_unscaled: 32.0000 (40.8901)  loss_ce_2_unscaled: 1.1744 (1.2479)  loss_bbox_2_unscaled: 0.1757 (0.2305)  loss_giou_2_unscaled: 1.0352 (1.1488)  cardinality_error_2_unscaled: 39.0000 (40.1538)  loss_ce_3_unscaled: 1.1452 (1.2404)  loss_bbox_3_unscaled: 0.2028 (0.2507)  loss_giou_3_unscaled: 1.1193 (1.2232)  cardinality_error_3_unscaled: 37.5000 (40.8516)  loss_ce_4_unscaled: 1.1638 (1.2550)  loss_bbox_4_unscaled: 0.1450 (0.2242)  loss_giou_4_unscaled: 0.9580 (1.1419)  cardinality_error_4_unscaled: 43.5000 (42.0769)  time: 0.2753  data: 0.0115  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.4106, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(56.8627, device='cuda:0'), 'loss_bbox': tensor(0.1243, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9230, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4688, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1525, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0300, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(30., device='cuda:0'), 'loss_ce_1': tensor(1.4726, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1480, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0013, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(38., device='cuda:0'), 'loss_ce_2': tensor(1.3723, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1263, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9198, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(33., device='cuda:0'), 'loss_ce_3': tensor(1.4207, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1180, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9058, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4067, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1210, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9543, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(57.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.7688, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2076, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(47.3684, device='cuda:0'), 'loss_bbox': tensor(0.1744, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1323, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(54.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1940, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1786, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1259, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(42.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2719, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1562, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0731, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(45.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2847, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1705, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1197, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(49., device='cuda:0'), 'loss_ce_3': tensor(1.2684, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1722, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1437, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(73., device='cuda:0'), 'loss_ce_4': tensor(1.2519, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1683, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1846, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(70.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.0869, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(2.2671, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(90.4762, device='cuda:0'), 'loss_bbox': tensor(0.2395, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2331, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(65., device='cuda:0'), 'loss_ce_0': tensor(2.2305, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1623, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9044, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(35., device='cuda:0'), 'loss_ce_1': tensor(2.1542, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1526, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8673, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(46., device='cuda:0'), 'loss_ce_2': tensor(2.0576, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1563, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8690, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(48., device='cuda:0'), 'loss_ce_3': tensor(2.2914, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1508, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8976, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(72., device='cuda:0'), 'loss_ce_4': tensor(2.3133, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1584, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9131, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(74., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.6005, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4081, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60.8696, device='cuda:0'), 'loss_bbox': tensor(0.2496, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2402, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(66., device='cuda:0'), 'loss_ce_0': tensor(1.5117, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1661, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9295, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(47.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4527, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1698, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9675, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5253, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1873, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0161, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(50., device='cuda:0'), 'loss_ce_3': tensor(1.4922, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1824, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0269, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(78., device='cuda:0'), 'loss_ce_4': tensor(1.5481, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1765, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0399, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(78., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.8858, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1741, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(38.4615, device='cuda:0'), 'loss_bbox': tensor(0.2320, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1157, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(51., device='cuda:0'), 'loss_ce_0': tensor(1.2173, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2113, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9711, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(39.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2663, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2173, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9673, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(43.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2890, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2429, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0258, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(38.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2305, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2334, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0260, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(53., device='cuda:0'), 'loss_ce_4': tensor(1.2804, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2134, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0302, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(63.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.3128, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0758, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(45.8333, device='cuda:0'), 'loss_bbox': tensor(0.2635, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1144, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(38.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0621, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2496, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0448, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(33., device='cuda:0'), 'loss_ce_1': tensor(1.1063, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2512, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0421, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(36., device='cuda:0'), 'loss_ce_2': tensor(1.3022, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2784, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0855, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(39., device='cuda:0'), 'loss_ce_3': tensor(1.2255, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2651, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0589, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(33., device='cuda:0'), 'loss_ce_4': tensor(1.1383, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2500, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0846, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(38., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.3902, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4754, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(80., device='cuda:0'), 'loss_bbox': tensor(0.2243, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0521, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(21., device='cuda:0'), 'loss_ce_0': tensor(1.5568, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2283, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9888, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(25., device='cuda:0'), 'loss_ce_1': tensor(1.5281, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2191, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9571, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(29.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4694, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2572, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9112, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(49., device='cuda:0'), 'loss_ce_3': tensor(1.4971, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2262, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9029, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(14.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5493, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2029, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9187, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(9., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.1385, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4407, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(83.8710, device='cuda:0'), 'loss_bbox': tensor(0.1313, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8453, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(50.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3024, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1459, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8658, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(20.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3773, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1426, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8465, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(33., device='cuda:0'), 'loss_ce_2': tensor(1.5801, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2300, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0309, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(64., device='cuda:0'), 'loss_ce_3': tensor(1.5044, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1628, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8871, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(25., device='cuda:0'), 'loss_ce_4': tensor(1.4539, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1340, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8634, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(18.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.8540, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6721, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(80.5556, device='cuda:0'), 'loss_bbox': tensor(0.1169, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7979, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(42.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5338, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1269, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8215, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(24.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5796, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1166, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7708, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(39., device='cuda:0'), 'loss_ce_2': tensor(1.6802, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1437, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8222, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(63.5000, device='cuda:0'), 'loss_ce_3': tensor(1.7193, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1191, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7692, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(24.5000, device='cuda:0'), 'loss_ce_4': tensor(1.7062, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1090, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8018, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(22., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.9257, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [1]  [ 99/100]  eta: 0:00:00  lr: 0.000100  class_error: 80.56  loss: 24.8793 (28.0776)  loss_ce: 1.2828 (1.2527)  loss_bbox: 0.8718 (1.1656)  loss_giou: 1.9116 (2.3203)  loss_ce_0: 1.3252 (1.2579)  loss_bbox_0: 0.8113 (1.0710)  loss_giou_0: 1.9111 (2.1939)  loss_ce_1: 1.2996 (1.2652)  loss_bbox_1: 0.8488 (1.0992)  loss_giou_1: 2.0027 (2.2599)  loss_ce_2: 1.3712 (1.2712)  loss_bbox_2: 0.8316 (1.1384)  loss_giou_2: 1.8796 (2.2669)  loss_ce_3: 1.2846 (1.2653)  loss_bbox_3: 1.0117 (1.2222)  loss_giou_3: 2.1177 (2.3985)  loss_ce_4: 1.3262 (1.2785)  loss_bbox_4: 0.7869 (1.0967)  loss_giou_4: 1.8374 (2.2541)  loss_ce_unscaled: 1.2828 (1.2527)  class_error_unscaled: 56.8627 (56.8127)  loss_bbox_unscaled: 0.1744 (0.2331)  loss_giou_unscaled: 0.9558 (1.1602)  cardinality_error_unscaled: 33.5000 (35.6000)  loss_ce_0_unscaled: 1.3252 (1.2579)  loss_bbox_0_unscaled: 0.1623 (0.2142)  loss_giou_0_unscaled: 0.9556 (1.0969)  cardinality_error_0_unscaled: 25.0000 (34.1750)  loss_ce_1_unscaled: 1.2996 (1.2652)  loss_bbox_1_unscaled: 0.1698 (0.2198)  loss_giou_1_unscaled: 1.0013 (1.1299)  cardinality_error_1_unscaled: 36.0000 (40.9000)  loss_ce_2_unscaled: 1.3712 (1.2712)  loss_bbox_2_unscaled: 0.1663 (0.2277)  loss_giou_2_unscaled: 0.9398 (1.1335)  cardinality_error_2_unscaled: 39.0000 (40.8800)  loss_ce_3_unscaled: 1.2846 (1.2653)  loss_bbox_3_unscaled: 0.2023 (0.2444)  loss_giou_3_unscaled: 1.0589 (1.1993)  cardinality_error_3_unscaled: 32.5000 (41.5300)  loss_ce_4_unscaled: 1.3262 (1.2785)  loss_bbox_4_unscaled: 0.1574 (0.2193)  loss_giou_4_unscaled: 0.9187 (1.1271)  cardinality_error_4_unscaled: 42.0000 (42.6000)  time: 0.2707  data: 0.0106  max mem: 1537\n",
            "Epoch: [1] Total time: 0:00:27 (0.2739 s / it)\n",
            "Averaged stats: lr: 0.000100  class_error: 80.56  loss: 24.8793 (28.0776)  loss_ce: 1.2828 (1.2527)  loss_bbox: 0.8718 (1.1656)  loss_giou: 1.9116 (2.3203)  loss_ce_0: 1.3252 (1.2579)  loss_bbox_0: 0.8113 (1.0710)  loss_giou_0: 1.9111 (2.1939)  loss_ce_1: 1.2996 (1.2652)  loss_bbox_1: 0.8488 (1.0992)  loss_giou_1: 2.0027 (2.2599)  loss_ce_2: 1.3712 (1.2712)  loss_bbox_2: 0.8316 (1.1384)  loss_giou_2: 1.8796 (2.2669)  loss_ce_3: 1.2846 (1.2653)  loss_bbox_3: 1.0117 (1.2222)  loss_giou_3: 2.1177 (2.3985)  loss_ce_4: 1.3262 (1.2785)  loss_bbox_4: 0.7869 (1.0967)  loss_giou_4: 1.8374 (2.2541)  loss_ce_unscaled: 1.2828 (1.2527)  class_error_unscaled: 56.8627 (56.8127)  loss_bbox_unscaled: 0.1744 (0.2331)  loss_giou_unscaled: 0.9558 (1.1602)  cardinality_error_unscaled: 33.5000 (35.6000)  loss_ce_0_unscaled: 1.3252 (1.2579)  loss_bbox_0_unscaled: 0.1623 (0.2142)  loss_giou_0_unscaled: 0.9556 (1.0969)  cardinality_error_0_unscaled: 25.0000 (34.1750)  loss_ce_1_unscaled: 1.2996 (1.2652)  loss_bbox_1_unscaled: 0.1698 (0.2198)  loss_giou_1_unscaled: 1.0013 (1.1299)  cardinality_error_1_unscaled: 36.0000 (40.9000)  loss_ce_2_unscaled: 1.3712 (1.2712)  loss_bbox_2_unscaled: 0.1663 (0.2277)  loss_giou_2_unscaled: 0.9398 (1.1335)  cardinality_error_2_unscaled: 39.0000 (40.8800)  loss_ce_3_unscaled: 1.2846 (1.2653)  loss_bbox_3_unscaled: 0.2023 (0.2444)  loss_giou_3_unscaled: 1.0589 (1.1993)  cardinality_error_3_unscaled: 32.5000 (41.5300)  loss_ce_4_unscaled: 1.3262 (1.2785)  loss_bbox_4_unscaled: 0.1574 (0.2193)  loss_giou_4_unscaled: 0.9187 (1.1271)  cardinality_error_4_unscaled: 42.0000 (42.6000)\n",
            "Test:  [ 0/25]  eta: 0:00:07  class_error: 58.14  loss: 41.6071 (41.6071)  loss_ce: 1.4087 (1.4087)  loss_bbox: 1.9292 (1.9292)  loss_giou: 3.4037 (3.4037)  loss_ce_0: 1.3220 (1.3220)  loss_bbox_0: 1.8220 (1.8220)  loss_giou_0: 3.3013 (3.3013)  loss_ce_1: 1.3940 (1.3940)  loss_bbox_1: 1.8127 (1.8127)  loss_giou_1: 3.2963 (3.2963)  loss_ce_2: 1.4645 (1.4645)  loss_bbox_2: 2.7962 (2.7962)  loss_giou_2: 3.5482 (3.5482)  loss_ce_3: 1.4461 (1.4461)  loss_bbox_3: 2.2297 (2.2297)  loss_giou_3: 3.3845 (3.3845)  loss_ce_4: 1.3829 (1.3829)  loss_bbox_4: 2.2849 (2.2849)  loss_giou_4: 3.3802 (3.3802)  loss_ce_unscaled: 1.4087 (1.4087)  class_error_unscaled: 58.1395 (58.1395)  loss_bbox_unscaled: 0.3858 (0.3858)  loss_giou_unscaled: 1.7018 (1.7018)  cardinality_error_unscaled: 78.5000 (78.5000)  loss_ce_0_unscaled: 1.3220 (1.3220)  loss_bbox_0_unscaled: 0.3644 (0.3644)  loss_giou_0_unscaled: 1.6507 (1.6507)  cardinality_error_0_unscaled: 21.5000 (21.5000)  loss_ce_1_unscaled: 1.3940 (1.3940)  loss_bbox_1_unscaled: 0.3625 (0.3625)  loss_giou_1_unscaled: 1.6482 (1.6482)  cardinality_error_1_unscaled: 78.5000 (78.5000)  loss_ce_2_unscaled: 1.4645 (1.4645)  loss_bbox_2_unscaled: 0.5592 (0.5592)  loss_giou_2_unscaled: 1.7741 (1.7741)  cardinality_error_2_unscaled: 78.5000 (78.5000)  loss_ce_3_unscaled: 1.4461 (1.4461)  loss_bbox_3_unscaled: 0.4459 (0.4459)  loss_giou_3_unscaled: 1.6922 (1.6922)  cardinality_error_3_unscaled: 78.5000 (78.5000)  loss_ce_4_unscaled: 1.3829 (1.3829)  loss_bbox_4_unscaled: 0.4570 (0.4570)  loss_giou_4_unscaled: 1.6901 (1.6901)  cardinality_error_4_unscaled: 78.5000 (78.5000)  time: 0.2920  data: 0.1670  max mem: 1537\n",
            "Test:  [10/25]  eta: 0:00:01  class_error: 88.46  loss: 39.4647 (39.7271)  loss_ce: 1.6112 (1.6616)  loss_bbox: 1.4543 (1.4540)  loss_giou: 3.1070 (3.1019)  loss_ce_0: 1.5732 (1.5766)  loss_bbox_0: 1.8220 (1.8251)  loss_giou_0: 3.3524 (3.3423)  loss_ce_1: 1.6090 (1.6387)  loss_bbox_1: 1.4857 (1.5146)  loss_giou_1: 3.1857 (3.1902)  loss_ce_2: 1.6639 (1.7112)  loss_bbox_2: 2.2368 (2.1497)  loss_giou_2: 3.4441 (3.4323)  loss_ce_3: 1.6246 (1.6682)  loss_bbox_3: 1.6426 (1.6331)  loss_giou_3: 3.3057 (3.2433)  loss_ce_4: 1.6291 (1.6540)  loss_bbox_4: 1.6854 (1.6705)  loss_giou_4: 3.3232 (3.2597)  loss_ce_unscaled: 1.6112 (1.6616)  class_error_unscaled: 68.4211 (67.5439)  loss_bbox_unscaled: 0.2909 (0.2908)  loss_giou_unscaled: 1.5535 (1.5510)  cardinality_error_unscaled: 80.0000 (81.0000)  loss_ce_0_unscaled: 1.5732 (1.5766)  loss_bbox_0_unscaled: 0.3644 (0.3650)  loss_giou_0_unscaled: 1.6762 (1.6711)  cardinality_error_0_unscaled: 20.0000 (19.0000)  loss_ce_1_unscaled: 1.6090 (1.6387)  loss_bbox_1_unscaled: 0.2971 (0.3029)  loss_giou_1_unscaled: 1.5928 (1.5951)  cardinality_error_1_unscaled: 80.0000 (81.0000)  loss_ce_2_unscaled: 1.6639 (1.7112)  loss_bbox_2_unscaled: 0.4474 (0.4299)  loss_giou_2_unscaled: 1.7221 (1.7162)  cardinality_error_2_unscaled: 80.0000 (81.0000)  loss_ce_3_unscaled: 1.6246 (1.6682)  loss_bbox_3_unscaled: 0.3285 (0.3266)  loss_giou_3_unscaled: 1.6529 (1.6216)  cardinality_error_3_unscaled: 80.0000 (81.0000)  loss_ce_4_unscaled: 1.6291 (1.6540)  loss_bbox_4_unscaled: 0.3371 (0.3341)  loss_giou_4_unscaled: 1.6616 (1.6299)  cardinality_error_4_unscaled: 80.0000 (81.0000)  time: 0.1232  data: 0.0238  max mem: 1537\n",
            "Test:  [20/25]  eta: 0:00:00  class_error: 24.44  loss: 39.5262 (39.4301)  loss_ce: 1.5587 (1.5683)  loss_bbox: 1.4617 (1.4646)  loss_giou: 3.1251 (3.1509)  loss_ce_0: 1.3895 (1.4831)  loss_bbox_0: 1.7508 (1.8021)  loss_giou_0: 3.3336 (3.3321)  loss_ce_1: 1.4841 (1.5382)  loss_bbox_1: 1.4857 (1.5104)  loss_giou_1: 3.1857 (3.2061)  loss_ce_2: 1.5966 (1.6022)  loss_bbox_2: 2.2439 (2.1737)  loss_giou_2: 3.4889 (3.4708)  loss_ce_3: 1.5777 (1.5782)  loss_bbox_3: 1.7159 (1.6605)  loss_giou_3: 3.3242 (3.3039)  loss_ce_4: 1.4999 (1.5522)  loss_bbox_4: 1.7541 (1.6987)  loss_giou_4: 3.3615 (3.3340)  loss_ce_unscaled: 1.5587 (1.5683)  class_error_unscaled: 65.8824 (62.3631)  loss_bbox_unscaled: 0.2923 (0.2929)  loss_giou_unscaled: 1.5625 (1.5754)  cardinality_error_unscaled: 79.0000 (77.7857)  loss_ce_0_unscaled: 1.3895 (1.4831)  loss_bbox_0_unscaled: 0.3502 (0.3604)  loss_giou_0_unscaled: 1.6668 (1.6660)  cardinality_error_0_unscaled: 21.0000 (22.2143)  loss_ce_1_unscaled: 1.4841 (1.5382)  loss_bbox_1_unscaled: 0.2971 (0.3021)  loss_giou_1_unscaled: 1.5928 (1.6031)  cardinality_error_1_unscaled: 79.0000 (77.7857)  loss_ce_2_unscaled: 1.5966 (1.6022)  loss_bbox_2_unscaled: 0.4488 (0.4347)  loss_giou_2_unscaled: 1.7445 (1.7354)  cardinality_error_2_unscaled: 79.0000 (77.7857)  loss_ce_3_unscaled: 1.5777 (1.5782)  loss_bbox_3_unscaled: 0.3432 (0.3321)  loss_giou_3_unscaled: 1.6621 (1.6520)  cardinality_error_3_unscaled: 79.0000 (77.7857)  loss_ce_4_unscaled: 1.4999 (1.5522)  loss_bbox_4_unscaled: 0.3508 (0.3397)  loss_giou_4_unscaled: 1.6807 (1.6670)  cardinality_error_4_unscaled: 79.0000 (77.7857)  time: 0.1219  data: 0.0103  max mem: 1537\n",
            "Test:  [24/25]  eta: 0:00:00  class_error: 81.40  loss: 39.5686 (39.6196)  loss_ce: 1.5648 (1.6018)  loss_bbox: 1.4617 (1.4696)  loss_giou: 3.1389 (3.1417)  loss_ce_0: 1.4139 (1.5143)  loss_bbox_0: 1.7205 (1.7899)  loss_giou_0: 3.2898 (3.3117)  loss_ce_1: 1.4988 (1.5752)  loss_bbox_1: 1.4821 (1.5092)  loss_giou_1: 3.1857 (3.1947)  loss_ce_2: 1.6027 (1.6434)  loss_bbox_2: 2.2739 (2.1841)  loss_giou_2: 3.4931 (3.4669)  loss_ce_3: 1.5895 (1.6120)  loss_bbox_3: 1.7318 (1.6730)  loss_giou_3: 3.3596 (3.2996)  loss_ce_4: 1.5032 (1.5871)  loss_bbox_4: 1.7829 (1.7126)  loss_giou_4: 3.3692 (3.3328)  loss_ce_unscaled: 1.5648 (1.6018)  class_error_unscaled: 68.4211 (64.9926)  loss_bbox_unscaled: 0.2923 (0.2939)  loss_giou_unscaled: 1.5694 (1.5709)  cardinality_error_unscaled: 79.0000 (78.8000)  loss_ce_0_unscaled: 1.4139 (1.5143)  loss_bbox_0_unscaled: 0.3441 (0.3580)  loss_giou_0_unscaled: 1.6449 (1.6559)  cardinality_error_0_unscaled: 21.0000 (21.2000)  loss_ce_1_unscaled: 1.4988 (1.5752)  loss_bbox_1_unscaled: 0.2964 (0.3018)  loss_giou_1_unscaled: 1.5928 (1.5973)  cardinality_error_1_unscaled: 79.0000 (78.8000)  loss_ce_2_unscaled: 1.6027 (1.6434)  loss_bbox_2_unscaled: 0.4548 (0.4368)  loss_giou_2_unscaled: 1.7465 (1.7334)  cardinality_error_2_unscaled: 79.0000 (78.8000)  loss_ce_3_unscaled: 1.5895 (1.6120)  loss_bbox_3_unscaled: 0.3464 (0.3346)  loss_giou_3_unscaled: 1.6798 (1.6498)  cardinality_error_3_unscaled: 79.0000 (78.8000)  loss_ce_4_unscaled: 1.5032 (1.5871)  loss_bbox_4_unscaled: 0.3566 (0.3425)  loss_giou_4_unscaled: 1.6846 (1.6664)  cardinality_error_4_unscaled: 79.0000 (78.8000)  time: 0.1195  data: 0.0099  max mem: 1537\n",
            "Test: Total time: 0:00:03 (0.1267 s / it)\n",
            "Averaged stats: class_error: 81.40  loss: 39.5686 (39.6196)  loss_ce: 1.5648 (1.6018)  loss_bbox: 1.4617 (1.4696)  loss_giou: 3.1389 (3.1417)  loss_ce_0: 1.4139 (1.5143)  loss_bbox_0: 1.7205 (1.7899)  loss_giou_0: 3.2898 (3.3117)  loss_ce_1: 1.4988 (1.5752)  loss_bbox_1: 1.4821 (1.5092)  loss_giou_1: 3.1857 (3.1947)  loss_ce_2: 1.6027 (1.6434)  loss_bbox_2: 2.2739 (2.1841)  loss_giou_2: 3.4931 (3.4669)  loss_ce_3: 1.5895 (1.6120)  loss_bbox_3: 1.7318 (1.6730)  loss_giou_3: 3.3596 (3.2996)  loss_ce_4: 1.5032 (1.5871)  loss_bbox_4: 1.7829 (1.7126)  loss_giou_4: 3.3692 (3.3328)  loss_ce_unscaled: 1.5648 (1.6018)  class_error_unscaled: 68.4211 (64.9926)  loss_bbox_unscaled: 0.2923 (0.2939)  loss_giou_unscaled: 1.5694 (1.5709)  cardinality_error_unscaled: 79.0000 (78.8000)  loss_ce_0_unscaled: 1.4139 (1.5143)  loss_bbox_0_unscaled: 0.3441 (0.3580)  loss_giou_0_unscaled: 1.6449 (1.6559)  cardinality_error_0_unscaled: 21.0000 (21.2000)  loss_ce_1_unscaled: 1.4988 (1.5752)  loss_bbox_1_unscaled: 0.2964 (0.3018)  loss_giou_1_unscaled: 1.5928 (1.5973)  cardinality_error_1_unscaled: 79.0000 (78.8000)  loss_ce_2_unscaled: 1.6027 (1.6434)  loss_bbox_2_unscaled: 0.4548 (0.4368)  loss_giou_2_unscaled: 1.7465 (1.7334)  cardinality_error_2_unscaled: 79.0000 (78.8000)  loss_ce_3_unscaled: 1.5895 (1.6120)  loss_bbox_3_unscaled: 0.3464 (0.3346)  loss_giou_3_unscaled: 1.6798 (1.6498)  cardinality_error_3_unscaled: 79.0000 (78.8000)  loss_ce_4_unscaled: 1.5032 (1.5871)  loss_bbox_4_unscaled: 0.3566 (0.3425)  loss_giou_4_unscaled: 1.6846 (1.6664)  cardinality_error_4_unscaled: 79.0000 (78.8000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "loss_dict {'loss_ce': tensor(1.5896, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(82.6087, device='cuda:0'), 'loss_bbox': tensor(0.1537, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8223, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(50.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5775, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1694, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8998, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(33.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5986, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1381, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8172, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(56.5000, device='cuda:0'), 'loss_ce_2': tensor(1.7332, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1478, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8048, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(63., device='cuda:0'), 'loss_ce_3': tensor(1.7147, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1348, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8112, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(37., device='cuda:0'), 'loss_ce_4': tensor(1.7693, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1417, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8585, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(40., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.2313, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [2]  [  0/100]  eta: 0:00:53  lr: 0.000100  class_error: 82.61  loss: 24.4378 (24.4378)  loss_ce: 1.5896 (1.5896)  loss_bbox: 0.7685 (0.7685)  loss_giou: 1.6446 (1.6446)  loss_ce_0: 1.5775 (1.5775)  loss_bbox_0: 0.8469 (0.8469)  loss_giou_0: 1.7996 (1.7996)  loss_ce_1: 1.5986 (1.5986)  loss_bbox_1: 0.6904 (0.6904)  loss_giou_1: 1.6343 (1.6343)  loss_ce_2: 1.7332 (1.7332)  loss_bbox_2: 0.7389 (0.7389)  loss_giou_2: 1.6095 (1.6095)  loss_ce_3: 1.7147 (1.7147)  loss_bbox_3: 0.6739 (0.6739)  loss_giou_3: 1.6225 (1.6225)  loss_ce_4: 1.7693 (1.7693)  loss_bbox_4: 0.7086 (0.7086)  loss_giou_4: 1.7170 (1.7170)  loss_ce_unscaled: 1.5896 (1.5896)  class_error_unscaled: 82.6087 (82.6087)  loss_bbox_unscaled: 0.1537 (0.1537)  loss_giou_unscaled: 0.8223 (0.8223)  cardinality_error_unscaled: 50.5000 (50.5000)  loss_ce_0_unscaled: 1.5775 (1.5775)  loss_bbox_0_unscaled: 0.1694 (0.1694)  loss_giou_0_unscaled: 0.8998 (0.8998)  cardinality_error_0_unscaled: 33.5000 (33.5000)  loss_ce_1_unscaled: 1.5986 (1.5986)  loss_bbox_1_unscaled: 0.1381 (0.1381)  loss_giou_1_unscaled: 0.8172 (0.8172)  cardinality_error_1_unscaled: 56.5000 (56.5000)  loss_ce_2_unscaled: 1.7332 (1.7332)  loss_bbox_2_unscaled: 0.1478 (0.1478)  loss_giou_2_unscaled: 0.8048 (0.8048)  cardinality_error_2_unscaled: 63.0000 (63.0000)  loss_ce_3_unscaled: 1.7147 (1.7147)  loss_bbox_3_unscaled: 0.1348 (0.1348)  loss_giou_3_unscaled: 0.8112 (0.8112)  cardinality_error_3_unscaled: 37.0000 (37.0000)  loss_ce_4_unscaled: 1.7693 (1.7693)  loss_bbox_4_unscaled: 0.1417 (0.1417)  loss_giou_4_unscaled: 0.8585 (0.8585)  cardinality_error_4_unscaled: 40.0000 (40.0000)  time: 0.5376  data: 0.1636  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.5109, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(65., device='cuda:0'), 'loss_bbox': tensor(0.1372, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9136, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30., device='cuda:0'), 'loss_ce_0': tensor(1.5309, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1404, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9102, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(22.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5040, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1358, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8948, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(28.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5013, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1474, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9724, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(37., device='cuda:0'), 'loss_ce_3': tensor(1.5435, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1296, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9218, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(28., device='cuda:0'), 'loss_ce_4': tensor(1.5778, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1304, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8931, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(36.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3244, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(71.7949, device='cuda:0'), 'loss_bbox': tensor(0.1706, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9129, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30., device='cuda:0'), 'loss_ce_0': tensor(1.3773, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1787, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9735, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(34., device='cuda:0'), 'loss_ce_1': tensor(1.4009, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1620, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9190, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(19., device='cuda:0'), 'loss_ce_2': tensor(1.4354, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1610, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9155, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(22.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3773, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1587, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9503, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(14.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3614, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1622, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9322, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(31., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.3521, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5617, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(87.7193, device='cuda:0'), 'loss_bbox': tensor(0.1665, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9160, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(5., device='cuda:0'), 'loss_ce_0': tensor(1.5714, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1905, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9763, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(28.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5673, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1714, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9602, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(3.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4855, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1889, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1337, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(11.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5599, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1528, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9353, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(3.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5358, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1694, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9676, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(23., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.0420, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2433, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(77.2727, device='cuda:0'), 'loss_bbox': tensor(0.1446, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9664, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(25., device='cuda:0'), 'loss_ce_0': tensor(1.2213, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2043, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2169, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(48., device='cuda:0'), 'loss_ce_1': tensor(1.1446, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1550, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1058, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(18.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1723, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1377, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0008, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(28.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2315, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1225, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9749, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(31., device='cuda:0'), 'loss_ce_4': tensor(1.2623, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1281, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9720, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(43., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.9827, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3283, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75., device='cuda:0'), 'loss_bbox': tensor(0.1383, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0437, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(20.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4699, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1524, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0996, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(39., device='cuda:0'), 'loss_ce_1': tensor(1.3727, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1157, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9255, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(16.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2777, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1030, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8854, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(33., device='cuda:0'), 'loss_ce_3': tensor(1.3163, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1130, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0026, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(40., device='cuda:0'), 'loss_ce_4': tensor(1.2945, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1165, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9527, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(43.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.3876, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1493, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(51.6129, device='cuda:0'), 'loss_bbox': tensor(0.1149, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9422, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(38.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3046, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1199, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8886, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(40.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2326, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0905, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7731, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(23., device='cuda:0'), 'loss_ce_2': tensor(1.1223, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0844, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7620, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(45., device='cuda:0'), 'loss_ce_3': tensor(1.1193, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0921, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8767, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(61.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1682, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1062, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9162, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(64., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.3920, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4230, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(73.0159, device='cuda:0'), 'loss_bbox': tensor(0.1196, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0043, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(49.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1929, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1526, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1298, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(27., device='cuda:0'), 'loss_ce_1': tensor(1.4102, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1095, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9795, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(20., device='cuda:0'), 'loss_ce_2': tensor(1.5769, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1178, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0408, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(48., device='cuda:0'), 'loss_ce_3': tensor(1.4544, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1099, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0320, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(59., device='cuda:0'), 'loss_ce_4': tensor(1.3421, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1170, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0059, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(63., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.2499, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0243, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(20.9302, device='cuda:0'), 'loss_bbox': tensor(0.1478, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0384, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(64., device='cuda:0'), 'loss_ce_0': tensor(1.1688, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1581, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9579, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(40., device='cuda:0'), 'loss_ce_1': tensor(1.1054, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1337, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9645, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(63., device='cuda:0'), 'loss_ce_2': tensor(1.0136, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1254, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9054, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(69., device='cuda:0'), 'loss_ce_3': tensor(1.0814, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1228, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9112, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(77., device='cuda:0'), 'loss_ce_4': tensor(1.0615, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1280, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9281, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(73., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.8095, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4063, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(58.1395, device='cuda:0'), 'loss_bbox': tensor(0.1469, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0319, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(73.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3218, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1282, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9281, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(54., device='cuda:0'), 'loss_ce_1': tensor(1.4091, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1358, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9799, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(74., device='cuda:0'), 'loss_ce_2': tensor(1.4685, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1295, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9201, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(76., device='cuda:0'), 'loss_ce_3': tensor(1.4011, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1297, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9293, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(77.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3269, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1327, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9209, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(78., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.5784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4601, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(25., device='cuda:0'), 'loss_bbox': tensor(0.1307, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8935, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(91., device='cuda:0'), 'loss_ce_0': tensor(1.5120, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1296, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8709, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(74.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5197, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1722, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2273, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(92., device='cuda:0'), 'loss_ce_2': tensor(1.5534, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1301, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0206, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(92., device='cuda:0'), 'loss_ce_3': tensor(1.4814, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1121, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7980, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(92., device='cuda:0'), 'loss_ce_4': tensor(1.5401, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1160, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8469, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(91.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.1601, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [2]  [ 10/100]  eta: 0:00:27  lr: 0.000100  class_error: 25.00  loss: 24.2845 (23.8299)  loss_ce: 1.4063 (1.3656)  loss_bbox: 0.7230 (0.7140)  loss_giou: 1.8843 (1.9064)  loss_ce_0: 1.3773 (1.3862)  loss_bbox_0: 0.7628 (0.7836)  loss_giou_0: 1.9157 (1.9730)  loss_ce_1: 1.4091 (1.3877)  loss_bbox_1: 0.6792 (0.6907)  loss_giou_1: 1.9204 (1.9176)  loss_ce_2: 1.4685 (1.3945)  loss_bbox_2: 0.6506 (0.6695)  loss_giou_2: 1.8401 (1.8839)  loss_ce_3: 1.4011 (1.3892)  loss_bbox_3: 0.6141 (0.6264)  loss_giou_3: 1.8585 (1.8442)  loss_ce_4: 1.3421 (1.3855)  loss_bbox_4: 0.6403 (0.6583)  loss_giou_4: 1.8561 (1.8535)  loss_ce_unscaled: 1.4063 (1.3656)  class_error_unscaled: 71.7949 (62.5540)  loss_bbox_unscaled: 0.1446 (0.1428)  loss_giou_unscaled: 0.9422 (0.9532)  cardinality_error_unscaled: 38.5000 (43.4091)  loss_ce_0_unscaled: 1.3773 (1.3862)  loss_bbox_0_unscaled: 0.1526 (0.1567)  loss_giou_0_unscaled: 0.9579 (0.9865)  cardinality_error_0_unscaled: 39.0000 (40.1364)  loss_ce_1_unscaled: 1.4091 (1.3877)  loss_bbox_1_unscaled: 0.1358 (0.1381)  loss_giou_1_unscaled: 0.9602 (0.9588)  cardinality_error_1_unscaled: 23.0000 (37.6818)  loss_ce_2_unscaled: 1.4685 (1.3945)  loss_bbox_2_unscaled: 0.1301 (0.1339)  loss_giou_2_unscaled: 0.9201 (0.9419)  cardinality_error_2_unscaled: 45.0000 (47.7727)  loss_ce_3_unscaled: 1.4011 (1.3892)  loss_bbox_3_unscaled: 0.1228 (0.1253)  loss_giou_3_unscaled: 0.9293 (0.9221)  cardinality_error_3_unscaled: 40.0000 (47.3636)  loss_ce_4_unscaled: 1.3421 (1.3855)  loss_bbox_4_unscaled: 0.1281 (0.1317)  loss_giou_4_unscaled: 0.9281 (0.9268)  cardinality_error_4_unscaled: 43.5000 (53.3182)  time: 0.3050  data: 0.0235  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.4117, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(42.8571, device='cuda:0'), 'loss_bbox': tensor(0.1627, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0758, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(80., device='cuda:0'), 'loss_ce_0': tensor(1.4155, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1539, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0267, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(77., device='cuda:0'), 'loss_ce_1': tensor(1.4918, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3010, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3766, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(82.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4601, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1627, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0991, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(81.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4407, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1535, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0642, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(82., device='cuda:0'), 'loss_ce_4': tensor(1.4296, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1569, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0551, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(82., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.2909, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2116, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(27.5862, device='cuda:0'), 'loss_bbox': tensor(0.1826, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0271, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(84., device='cuda:0'), 'loss_ce_0': tensor(1.2366, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1475, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9140, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(81.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2238, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1556, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0418, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(85.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1997, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1533, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9293, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(85., device='cuda:0'), 'loss_ce_3': tensor(1.1896, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1915, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1482, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(85.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2498, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2359, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2456, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(85.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.1036, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0395, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(12.1212, device='cuda:0'), 'loss_bbox': tensor(0.1881, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9712, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(82.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0872, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1963, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0457, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(75.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0744, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2239, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1171, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(83., device='cuda:0'), 'loss_ce_2': tensor(1.0521, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2015, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0756, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(83., device='cuda:0'), 'loss_ce_3': tensor(1.0216, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1847, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9556, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(82.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1273, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1974, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9866, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(83., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.4956, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9049, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(10.2564, device='cuda:0'), 'loss_bbox': tensor(0.1830, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1211, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(79., device='cuda:0'), 'loss_ce_0': tensor(0.9865, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1502, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9758, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(72.5000, device='cuda:0'), 'loss_ce_1': tensor(0.9821, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1960, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2088, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(80.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9582, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1535, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9459, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(79., device='cuda:0'), 'loss_ce_3': tensor(0.9054, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1968, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1455, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(80., device='cuda:0'), 'loss_ce_4': tensor(0.9294, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2732, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4248, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(78.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.8740, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4043, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(42.8571, device='cuda:0'), 'loss_bbox': tensor(0.1199, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8998, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(73.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3842, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1163, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9394, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(68.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4313, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2964, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2583, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(75., device='cuda:0'), 'loss_ce_2': tensor(1.4660, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1626, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0271, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(72.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3815, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1213, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8628, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(74.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3800, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1238, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9221, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(74.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.7597, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.8287, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(0., device='cuda:0'), 'loss_bbox': tensor(0.1542, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0704, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(81.5000, device='cuda:0'), 'loss_ce_0': tensor(0.8822, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1368, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0471, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(79., device='cuda:0'), 'loss_ce_1': tensor(0.9212, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2263, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1622, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(85., device='cuda:0'), 'loss_ce_2': tensor(0.8100, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1718, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9976, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(83.5000, device='cuda:0'), 'loss_ce_3': tensor(0.8171, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1589, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9986, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(80., device='cuda:0'), 'loss_ce_4': tensor(0.8872, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1603, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0066, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(83., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.5467, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.8328, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(3.5714, device='cuda:0'), 'loss_bbox': tensor(0.1254, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8303, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(78.5000, device='cuda:0'), 'loss_ce_0': tensor(0.8921, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1565, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0634, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(78.5000, device='cuda:0'), 'loss_ce_1': tensor(0.9019, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1119, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8153, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(86., device='cuda:0'), 'loss_ce_2': tensor(0.8467, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1212, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8191, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(83., device='cuda:0'), 'loss_ce_3': tensor(0.8045, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1303, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8441, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(78., device='cuda:0'), 'loss_ce_4': tensor(0.8693, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1334, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8574, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(84., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.3543, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1868, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(27.2727, device='cuda:0'), 'loss_bbox': tensor(0.2163, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2233, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(71., device='cuda:0'), 'loss_ce_0': tensor(1.1521, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1399, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9807, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(69., device='cuda:0'), 'loss_ce_1': tensor(1.2001, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2286, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2618, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(88., device='cuda:0'), 'loss_ce_2': tensor(1.1637, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1599, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8418, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(79.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2297, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1527, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8830, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(70., device='cuda:0'), 'loss_ce_4': tensor(1.1803, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1863, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0606, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(79.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.8756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1483, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(44.4444, device='cuda:0'), 'loss_bbox': tensor(0.1330, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8693, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(40.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0743, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1291, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9659, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(66., device='cuda:0'), 'loss_ce_1': tensor(1.1706, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1166, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9066, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(77., device='cuda:0'), 'loss_ce_2': tensor(1.1460, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1271, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8362, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(60.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2249, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1257, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8208, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(47., device='cuda:0'), 'loss_ce_4': tensor(1.1508, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1458, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8736, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(61.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.1322, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0446, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(54.1667, device='cuda:0'), 'loss_bbox': tensor(0.1181, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8554, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(35., device='cuda:0'), 'loss_ce_0': tensor(1.0069, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1664, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1359, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(62., device='cuda:0'), 'loss_ce_1': tensor(1.0668, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0991, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7131, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(77., device='cuda:0'), 'loss_ce_2': tensor(1.1372, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1002, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.6910, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(63.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0840, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1016, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7078, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(40., device='cuda:0'), 'loss_ce_4': tensor(1.0881, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1643, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0212, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(58., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.2449, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [2]  [ 20/100]  eta: 0:00:23  lr: 0.000100  class_error: 54.17  loss: 24.2845 (23.7451)  loss_ce: 1.2116 (1.2397)  loss_bbox: 0.7230 (0.7510)  loss_giou: 1.9328 (1.9456)  loss_ce_0: 1.2213 (1.2555)  loss_bbox_0: 0.7509 (0.7659)  loss_giou_0: 1.9515 (1.9949)  loss_ce_1: 1.2238 (1.2728)  loss_bbox_1: 0.7748 (0.8274)  loss_giou_1: 1.9589 (2.0389)  loss_ce_2: 1.1723 (1.2657)  loss_bbox_2: 0.6883 (0.7111)  loss_giou_2: 1.8586 (1.8690)  loss_ce_3: 1.2297 (1.2562)  loss_bbox_3: 0.6482 (0.6893)  loss_giou_3: 1.8585 (1.8642)  loss_ce_4: 1.2498 (1.2634)  loss_bbox_4: 0.6669 (0.7680)  loss_giou_4: 1.9054 (1.9664)  loss_ce_unscaled: 1.2116 (1.2397)  class_error_unscaled: 42.8571 (45.3918)  loss_bbox_unscaled: 0.1446 (0.1502)  loss_giou_unscaled: 0.9664 (0.9728)  cardinality_error_unscaled: 64.0000 (56.3333)  loss_ce_0_unscaled: 1.2213 (1.2555)  loss_bbox_0_unscaled: 0.1502 (0.1532)  loss_giou_0_unscaled: 0.9758 (0.9974)  cardinality_error_0_unscaled: 62.0000 (55.7619)  loss_ce_1_unscaled: 1.2238 (1.2728)  loss_bbox_1_unscaled: 0.1550 (0.1655)  loss_giou_1_unscaled: 0.9795 (1.0194)  cardinality_error_1_unscaled: 75.0000 (58.7619)  loss_ce_2_unscaled: 1.1723 (1.2657)  loss_bbox_2_unscaled: 0.1377 (0.1422)  loss_giou_2_unscaled: 0.9293 (0.9345)  cardinality_error_2_unscaled: 69.0000 (61.7381)  loss_ce_3_unscaled: 1.2297 (1.2562)  loss_bbox_3_unscaled: 0.1296 (0.1379)  loss_giou_3_unscaled: 0.9293 (0.9321)  cardinality_error_3_unscaled: 70.0000 (59.0714)  loss_ce_4_unscaled: 1.2498 (1.2634)  loss_bbox_4_unscaled: 0.1334 (0.1536)  loss_giou_4_unscaled: 0.9527 (0.9832)  cardinality_error_4_unscaled: 73.0000 (64.5714)  time: 0.2762  data: 0.0095  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.6535, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(88.8889, device='cuda:0'), 'loss_bbox': tensor(0.1144, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9656, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(23., device='cuda:0'), 'loss_ce_0': tensor(1.5114, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1473, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0868, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(65., device='cuda:0'), 'loss_ce_1': tensor(1.6250, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1026, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9185, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(67.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6686, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1158, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9141, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(56., device='cuda:0'), 'loss_ce_3': tensor(1.7369, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1265, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9925, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(31., device='cuda:0'), 'loss_ce_4': tensor(1.6119, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1389, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9886, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(53., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.0541, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2002, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(85.2941, device='cuda:0'), 'loss_bbox': tensor(0.1740, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2498, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(12.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1019, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2260, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4187, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(48.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1540, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1535, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1559, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(49., device='cuda:0'), 'loss_ce_2': tensor(1.1223, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1956, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3533, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(40., device='cuda:0'), 'loss_ce_3': tensor(1.1559, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2049, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3833, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(15., device='cuda:0'), 'loss_ce_4': tensor(1.1668, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2078, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2716, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(38.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(31.2354, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0354, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(68.0851, device='cuda:0'), 'loss_bbox': tensor(0.1497, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9520, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(4.5000, device='cuda:0'), 'loss_ce_0': tensor(0.8894, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2350, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2717, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(39.5000, device='cuda:0'), 'loss_ce_1': tensor(0.9359, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1521, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9516, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(30.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9840, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1737, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1084, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(24., device='cuda:0'), 'loss_ce_3': tensor(0.9983, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1665, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0532, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(6., device='cuda:0'), 'loss_ce_4': tensor(1.0331, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1453, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9050, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(26., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.2490, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9598, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(55.1724, device='cuda:0'), 'loss_bbox': tensor(0.1038, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9095, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(19.5000, device='cuda:0'), 'loss_ce_0': tensor(0.8816, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1063, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8942, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(47.5000, device='cuda:0'), 'loss_ce_1': tensor(0.8900, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1039, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8787, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(29.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9418, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1034, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9054, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(43., device='cuda:0'), 'loss_ce_3': tensor(0.9445, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1031, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9475, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(36., device='cuda:0'), 'loss_ce_4': tensor(0.9487, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1100, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8679, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(56., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.3925, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3951, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(57.5000, device='cuda:0'), 'loss_bbox': tensor(0.1627, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0960, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(32.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3512, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1890, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2040, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(46., device='cuda:0'), 'loss_ce_1': tensor(1.3360, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1094, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8865, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(26., device='cuda:0'), 'loss_ce_2': tensor(1.3789, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1269, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9622, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(34., device='cuda:0'), 'loss_ce_3': tensor(1.4112, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1230, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9771, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(43.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4149, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1207, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9234, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(60.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.4153, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.9350, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(87.0370, device='cuda:0'), 'loss_bbox': tensor(0.2179, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1556, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(38., device='cuda:0'), 'loss_ce_0': tensor(1.9624, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1832, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0681, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(40.5000, device='cuda:0'), 'loss_ce_1': tensor(1.9876, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1860, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0959, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(29., device='cuda:0'), 'loss_ce_2': tensor(1.9931, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1745, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0284, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(43., device='cuda:0'), 'loss_ce_3': tensor(1.9869, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1746, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9831, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(53., device='cuda:0'), 'loss_ce_4': tensor(1.9226, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1783, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0164, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(64., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.8503, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9882, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(27.5000, device='cuda:0'), 'loss_bbox': tensor(0.3025, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4730, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(60., device='cuda:0'), 'loss_ce_0': tensor(0.9995, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1279, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9664, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(48., device='cuda:0'), 'loss_ce_1': tensor(0.9413, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1231, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9388, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(29., device='cuda:0'), 'loss_ce_2': tensor(0.9640, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1421, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0808, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(52., device='cuda:0'), 'loss_ce_3': tensor(0.9925, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1317, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9809, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(70.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9817, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1472, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0018, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(73., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.5332, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0804, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(34.3750, device='cuda:0'), 'loss_bbox': tensor(0.2750, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3655, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(71., device='cuda:0'), 'loss_ce_0': tensor(1.1012, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1472, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0237, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(56., device='cuda:0'), 'loss_ce_1': tensor(1.1381, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1389, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8972, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(43., device='cuda:0'), 'loss_ce_2': tensor(1.1323, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1461, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9924, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(67., device='cuda:0'), 'loss_ce_3': tensor(1.1626, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1400, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9071, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(78., device='cuda:0'), 'loss_ce_4': tensor(1.1780, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1481, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9625, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(80.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.9318, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1480, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(22.2222, device='cuda:0'), 'loss_bbox': tensor(0.1128, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9147, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(85.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0741, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1192, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0824, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(63., device='cuda:0'), 'loss_ce_1': tensor(1.1431, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1505, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3092, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(49.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1991, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1140, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9311, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(77., device='cuda:0'), 'loss_ce_3': tensor(1.2856, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1299, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0045, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(87., device='cuda:0'), 'loss_ce_4': tensor(1.2134, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1103, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9286, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(89., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.9310, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4843, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(54.5455, device='cuda:0'), 'loss_bbox': tensor(0.1221, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9709, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(77., device='cuda:0'), 'loss_ce_0': tensor(1.5840, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0927, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8308, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(60.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4814, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0916, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7827, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(46., device='cuda:0'), 'loss_ce_2': tensor(1.5187, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0985, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7881, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(72., device='cuda:0'), 'loss_ce_3': tensor(1.4656, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1044, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7930, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(80., device='cuda:0'), 'loss_ce_4': tensor(1.4571, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1041, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7949, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(80., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.8035, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [2]  [ 30/100]  eta: 0:00:19  lr: 0.000100  class_error: 54.55  loss: 24.0658 (23.9561)  loss_ce: 1.1480 (1.2553)  loss_bbox: 0.7709 (0.7885)  loss_giou: 1.9424 (2.0311)  loss_ce_0: 1.0872 (1.2523)  loss_bbox_0: 0.7367 (0.7726)  loss_giou_0: 2.0533 (2.0512)  loss_ce_1: 1.1431 (1.2697)  loss_bbox_1: 0.7524 (0.7720)  loss_giou_1: 1.9033 (2.0144)  loss_ce_2: 1.1372 (1.2736)  loss_bbox_2: 0.7305 (0.7060)  loss_giou_2: 1.8917 (1.9154)  loss_ce_3: 1.1626 (1.2748)  loss_bbox_3: 0.6584 (0.6935)  loss_giou_3: 1.9542 (1.9094)  loss_ce_4: 1.1668 (1.2729)  loss_bbox_4: 0.7362 (0.7478)  loss_giou_4: 1.9732 (1.9554)  loss_ce_unscaled: 1.1480 (1.2553)  class_error_unscaled: 42.8571 (49.4790)  loss_bbox_unscaled: 0.1542 (0.1577)  loss_giou_unscaled: 0.9712 (1.0155)  cardinality_error_unscaled: 71.0000 (51.8226)  loss_ce_0_unscaled: 1.0872 (1.2523)  loss_bbox_0_unscaled: 0.1473 (0.1545)  loss_giou_0_unscaled: 1.0267 (1.0256)  cardinality_error_0_unscaled: 63.0000 (54.3710)  loss_ce_1_unscaled: 1.1431 (1.2697)  loss_bbox_1_unscaled: 0.1505 (0.1544)  loss_giou_1_unscaled: 0.9516 (1.0072)  cardinality_error_1_unscaled: 67.5000 (52.6774)  loss_ce_2_unscaled: 1.1372 (1.2736)  loss_bbox_2_unscaled: 0.1461 (0.1412)  loss_giou_2_unscaled: 0.9459 (0.9577)  cardinality_error_2_unscaled: 67.0000 (58.2097)  loss_ce_3_unscaled: 1.1626 (1.2748)  loss_bbox_3_unscaled: 0.1317 (0.1387)  loss_giou_3_unscaled: 0.9771 (0.9547)  cardinality_error_3_unscaled: 70.5000 (56.1452)  loss_ce_4_unscaled: 1.1668 (1.2729)  loss_bbox_4_unscaled: 0.1472 (0.1496)  loss_giou_4_unscaled: 0.9866 (0.9777)  cardinality_error_4_unscaled: 74.5000 (63.7581)  time: 0.2715  data: 0.0106  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.2577, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(64.2857, device='cuda:0'), 'loss_bbox': tensor(0.1005, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8852, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(82., device='cuda:0'), 'loss_ce_0': tensor(1.2376, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0933, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8622, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(58., device='cuda:0'), 'loss_ce_1': tensor(1.2802, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0903, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7984, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(50., device='cuda:0'), 'loss_ce_2': tensor(1.2560, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0921, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7604, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(78., device='cuda:0'), 'loss_ce_3': tensor(1.3254, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0947, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7588, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(85., device='cuda:0'), 'loss_ce_4': tensor(1.2722, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0931, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7741, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(85.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.9515, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1169, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(32.1429, device='cuda:0'), 'loss_bbox': tensor(0.1940, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0441, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(70., device='cuda:0'), 'loss_ce_0': tensor(1.1169, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1592, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8555, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(43.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1313, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1760, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9233, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(43.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0873, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1697, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9269, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(60.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1788, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1743, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9000, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(63., device='cuda:0'), 'loss_ce_4': tensor(1.1655, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1723, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9010, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(65.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.9486, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9765, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(24., device='cuda:0'), 'loss_bbox': tensor(0.1934, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3419, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(64., device='cuda:0'), 'loss_ce_0': tensor(0.8434, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1558, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2062, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(34.5000, device='cuda:0'), 'loss_ce_1': tensor(0.9714, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1577, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1651, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(40.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0544, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1715, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1852, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(52.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0627, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1609, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1403, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(39.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0041, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1800, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2711, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(46., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.4005, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9001, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(31.8182, device='cuda:0'), 'loss_bbox': tensor(0.1808, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0294, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(49., device='cuda:0'), 'loss_ce_0': tensor(0.9506, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1511, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9262, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(15.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0084, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1490, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8757, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(28., device='cuda:0'), 'loss_ce_2': tensor(0.9387, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1705, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0255, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(32., device='cuda:0'), 'loss_ce_3': tensor(0.9843, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1915, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0906, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(14.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0314, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2152, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1204, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(13., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.0273, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0154, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(30., device='cuda:0'), 'loss_bbox': tensor(0.1563, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0088, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(47.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0262, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1324, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9105, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(13.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0402, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1334, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9029, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(43.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0773, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1471, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9292, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(43., device='cuda:0'), 'loss_ce_3': tensor(1.0855, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1592, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9719, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(16., device='cuda:0'), 'loss_ce_4': tensor(1.0344, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2104, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1364, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(15.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.4749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4027, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(50., device='cuda:0'), 'loss_bbox': tensor(0.1946, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0092, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(60., device='cuda:0'), 'loss_ce_0': tensor(1.3657, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1882, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9208, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(36., device='cuda:0'), 'loss_ce_1': tensor(1.3724, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1978, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9597, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58., device='cuda:0'), 'loss_ce_2': tensor(1.3794, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1860, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9447, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(49.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3581, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1907, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9654, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(31., device='cuda:0'), 'loss_ce_4': tensor(1.4086, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2270, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1324, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(32.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.8556, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2777, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(72.2222, device='cuda:0'), 'loss_bbox': tensor(0.1383, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0433, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(60.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2581, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1259, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9696, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(29., device='cuda:0'), 'loss_ce_1': tensor(1.3302, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1364, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9941, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(70.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3292, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1248, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9199, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(57.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2373, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1348, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9534, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(34., device='cuda:0'), 'loss_ce_4': tensor(1.2193, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1425, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1205, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(39., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.5307, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4324, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75., device='cuda:0'), 'loss_bbox': tensor(0.1682, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1537, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(34., device='cuda:0'), 'loss_ce_0': tensor(1.5272, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1634, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2065, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(19., device='cuda:0'), 'loss_ce_1': tensor(1.4717, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1957, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1977, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(68.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3897, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1676, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0025, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(46.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3761, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1831, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0628, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(20.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4088, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1803, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1635, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(24.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.3420, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1037, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(50., device='cuda:0'), 'loss_bbox': tensor(0.1608, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1434, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(40.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9833, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1788, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2319, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(22.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1074, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2004, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3209, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(70., device='cuda:0'), 'loss_ce_2': tensor(1.0716, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1760, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1157, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(53., device='cuda:0'), 'loss_ce_3': tensor(1.0090, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1968, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1448, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(28., device='cuda:0'), 'loss_ce_4': tensor(1.0589, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1729, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1538, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(27., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.8397, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9589, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(44.1861, device='cuda:0'), 'loss_bbox': tensor(0.1336, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8888, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(16.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9588, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1822, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1646, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(4.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0570, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1179, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8809, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(52., device='cuda:0'), 'loss_ce_2': tensor(1.0482, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1192, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8040, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(37.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0290, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1328, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8434, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(16.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0635, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1322, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8872, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(18.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.9811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [2]  [ 40/100]  eta: 0:00:17  lr: 0.000100  class_error: 44.19  loss: 23.6227 (23.9461)  loss_ce: 1.1169 (1.2282)  loss_bbox: 0.8038 (0.7939)  loss_giou: 2.0589 (2.0502)  loss_ce_0: 1.1012 (1.2217)  loss_bbox_0: 0.7557 (0.7708)  loss_giou_0: 2.0474 (2.0511)  loss_ce_1: 1.1381 (1.2471)  loss_bbox_1: 0.6943 (0.7733)  loss_giou_1: 1.8467 (2.0118)  loss_ce_2: 1.1223 (1.2467)  loss_bbox_2: 0.7305 (0.7197)  loss_giou_2: 1.8895 (1.9172)  loss_ce_3: 1.1626 (1.2480)  loss_bbox_3: 0.6999 (0.7218)  loss_giou_3: 1.9542 (1.9233)  loss_ce_4: 1.1668 (1.2470)  loss_bbox_4: 0.7362 (0.7759)  loss_giou_4: 1.9772 (1.9985)  loss_ce_unscaled: 1.1169 (1.2282)  class_error_unscaled: 50.0000 (48.9635)  loss_bbox_unscaled: 0.1608 (0.1588)  loss_giou_unscaled: 1.0294 (1.0251)  cardinality_error_unscaled: 47.5000 (51.9634)  loss_ce_0_unscaled: 1.1012 (1.2217)  loss_bbox_0_unscaled: 0.1511 (0.1542)  loss_giou_0_unscaled: 1.0237 (1.0255)  cardinality_error_0_unscaled: 40.5000 (47.8415)  loss_ce_1_unscaled: 1.1381 (1.2471)  loss_bbox_1_unscaled: 0.1389 (0.1547)  loss_giou_1_unscaled: 0.9233 (1.0059)  cardinality_error_1_unscaled: 43.5000 (52.6220)  loss_ce_2_unscaled: 1.1223 (1.2467)  loss_bbox_2_unscaled: 0.1461 (0.1439)  loss_giou_2_unscaled: 0.9447 (0.9586)  cardinality_error_2_unscaled: 49.5000 (56.4512)  loss_ce_3_unscaled: 1.1626 (1.2480)  loss_bbox_3_unscaled: 0.1400 (0.1444)  loss_giou_3_unscaled: 0.9771 (0.9616)  cardinality_error_3_unscaled: 34.0000 (50.9390)  loss_ce_4_unscaled: 1.1668 (1.2470)  loss_bbox_4_unscaled: 0.1472 (0.1552)  loss_giou_4_unscaled: 0.9886 (0.9992)  cardinality_error_4_unscaled: 46.0000 (57.1585)  time: 0.2959  data: 0.0125  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.4942, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(63.8889, device='cuda:0'), 'loss_bbox': tensor(0.1197, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9162, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(15., device='cuda:0'), 'loss_ce_0': tensor(1.4102, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1300, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0326, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(10., device='cuda:0'), 'loss_ce_1': tensor(1.5675, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1362, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9121, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(62.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4945, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1107, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8657, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(50., device='cuda:0'), 'loss_ce_3': tensor(1.4768, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1184, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8886, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(37., device='cuda:0'), 'loss_ce_4': tensor(1.4505, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1203, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9252, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(22.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.4577, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0751, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(39.1304, device='cuda:0'), 'loss_bbox': tensor(0.1509, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9783, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(27., device='cuda:0'), 'loss_ce_0': tensor(1.0550, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1456, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9392, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(18.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2046, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1564, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0206, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(63.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1481, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1449, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8931, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(60., device='cuda:0'), 'loss_ce_3': tensor(1.1938, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1450, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8791, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(54., device='cuda:0'), 'loss_ce_4': tensor(1.1257, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1419, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9249, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(47., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.2687, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9872, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(65.7143, device='cuda:0'), 'loss_bbox': tensor(0.1355, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9589, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(16., device='cuda:0'), 'loss_ce_0': tensor(0.8796, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1422, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0344, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(7.5000, device='cuda:0'), 'loss_ce_1': tensor(0.9600, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1592, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0387, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(59.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9523, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1341, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9260, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(60., device='cuda:0'), 'loss_ce_3': tensor(1.0083, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1346, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9332, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(58.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9434, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1296, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9490, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(47., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.4545, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4830, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(73.3333, device='cuda:0'), 'loss_bbox': tensor(0.1381, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9181, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4579, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2354, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2755, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(14.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4811, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1503, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9477, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(66., device='cuda:0'), 'loss_ce_2': tensor(1.4848, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1452, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8968, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(68., device='cuda:0'), 'loss_ce_3': tensor(1.5310, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1489, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8800, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(74.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4862, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1410, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8977, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(58., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.1206, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4248, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(67.2727, device='cuda:0'), 'loss_bbox': tensor(0.1401, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9829, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(14., device='cuda:0'), 'loss_ce_0': tensor(1.4508, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1672, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0718, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(11., device='cuda:0'), 'loss_ce_1': tensor(1.4777, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1531, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9937, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(56.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4926, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1231, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8782, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(59., device='cuda:0'), 'loss_ce_3': tensor(1.4872, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1346, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9480, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(64., device='cuda:0'), 'loss_ce_4': tensor(1.4302, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1419, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0312, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(55.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.6794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2648, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(58.0645, device='cuda:0'), 'loss_bbox': tensor(0.1110, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8609, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(43., device='cuda:0'), 'loss_ce_0': tensor(1.2472, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1062, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8579, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(28.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3139, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1434, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0798, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(66.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3244, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1162, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8938, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(66., device='cuda:0'), 'loss_ce_3': tensor(1.3758, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1458, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9871, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(80., device='cuda:0'), 'loss_ce_4': tensor(1.2759, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1199, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8984, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(70., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.4735, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3215, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(65.5172, device='cuda:0'), 'loss_bbox': tensor(0.1828, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1331, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(43., device='cuda:0'), 'loss_ce_0': tensor(1.3085, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1523, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0354, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(44., device='cuda:0'), 'loss_ce_1': tensor(1.2921, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1671, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0957, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(66.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3500, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1544, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0871, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(64.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3566, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1670, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0711, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(77., device='cuda:0'), 'loss_ce_4': tensor(1.3268, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1754, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1016, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(78., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.8224, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2433, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(47.2222, device='cuda:0'), 'loss_bbox': tensor(0.1061, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8122, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(38., device='cuda:0'), 'loss_ce_0': tensor(1.2187, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1155, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9221, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(61., device='cuda:0'), 'loss_ce_1': tensor(1.2732, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0921, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7574, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(59., device='cuda:0'), 'loss_ce_2': tensor(1.3121, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0892, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7962, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(68., device='cuda:0'), 'loss_ce_3': tensor(1.3320, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0992, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7941, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(75.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2913, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0989, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7862, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(71., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.2287, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2426, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(49.0566, device='cuda:0'), 'loss_bbox': tensor(0.1680, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9249, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(34.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1713, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1684, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9844, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(57., device='cuda:0'), 'loss_ce_1': tensor(1.2189, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1576, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9885, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1836, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1573, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9881, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(54.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1676, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1750, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0210, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(67.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2186, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1598, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9383, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(64., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.6642, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6543, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(74.4186, device='cuda:0'), 'loss_bbox': tensor(0.1245, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9260, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(42.5000, device='cuda:0'), 'loss_ce_0': tensor(1.6817, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1185, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9332, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(74.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5952, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1042, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8802, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(50., device='cuda:0'), 'loss_ce_2': tensor(1.7675, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1094, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8498, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(48., device='cuda:0'), 'loss_ce_3': tensor(1.7623, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1201, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8855, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(71.5000, device='cuda:0'), 'loss_ce_4': tensor(1.7153, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1154, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9241, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(69.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.2135, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [2]  [ 50/100]  eta: 0:00:14  lr: 0.000100  class_error: 74.42  loss: 23.6510 (23.8644)  loss_ce: 1.2433 (1.2460)  loss_bbox: 0.7003 (0.7732)  loss_giou: 1.9566 (2.0173)  loss_ce_0: 1.2187 (1.2347)  loss_bbox_0: 0.7557 (0.7649)  loss_giou_0: 1.9391 (2.0445)  loss_ce_1: 1.2732 (1.2650)  loss_bbox_1: 0.7515 (0.7609)  loss_giou_1: 1.9194 (1.9983)  loss_ce_2: 1.2560 (1.2671)  loss_bbox_2: 0.7243 (0.7045)  loss_giou_2: 1.8397 (1.8971)  loss_ce_3: 1.2373 (1.2717)  loss_bbox_3: 0.7288 (0.7164)  loss_giou_3: 1.8960 (1.9104)  loss_ce_4: 1.2193 (1.2626)  loss_bbox_4: 0.7097 (0.7555)  loss_giou_4: 1.8767 (1.9743)  loss_ce_unscaled: 1.2433 (1.2460)  class_error_unscaled: 50.0000 (51.1985)  loss_bbox_unscaled: 0.1401 (0.1546)  loss_giou_unscaled: 0.9783 (1.0086)  cardinality_error_unscaled: 40.5000 (47.7255)  loss_ce_0_unscaled: 1.2187 (1.2347)  loss_bbox_0_unscaled: 0.1511 (0.1530)  loss_giou_0_unscaled: 0.9696 (1.0222)  cardinality_error_0_unscaled: 22.5000 (44.8627)  loss_ce_1_unscaled: 1.2732 (1.2650)  loss_bbox_1_unscaled: 0.1503 (0.1522)  loss_giou_1_unscaled: 0.9597 (0.9991)  cardinality_error_1_unscaled: 58.0000 (54.0392)  loss_ce_2_unscaled: 1.2560 (1.2671)  loss_bbox_2_unscaled: 0.1449 (0.1409)  loss_giou_2_unscaled: 0.9199 (0.9486)  cardinality_error_2_unscaled: 54.5000 (57.1078)  loss_ce_3_unscaled: 1.2373 (1.2717)  loss_bbox_3_unscaled: 0.1458 (0.1433)  loss_giou_3_unscaled: 0.9480 (0.9552)  cardinality_error_3_unscaled: 54.0000 (53.8824)  loss_ce_4_unscaled: 1.2193 (1.2626)  loss_bbox_4_unscaled: 0.1419 (0.1511)  loss_giou_4_unscaled: 0.9383 (0.9872)  cardinality_error_4_unscaled: 47.0000 (57.3725)  time: 0.3076  data: 0.0126  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.2338, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(32.2581, device='cuda:0'), 'loss_bbox': tensor(0.1799, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9906, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(49., device='cuda:0'), 'loss_ce_0': tensor(1.2753, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1696, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9392, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(82., device='cuda:0'), 'loss_ce_1': tensor(1.3542, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1467, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8699, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(55., device='cuda:0'), 'loss_ce_2': tensor(1.2229, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1475, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7928, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(60.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2754, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1551, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7715, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(77., device='cuda:0'), 'loss_ce_4': tensor(1.2870, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1595, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8785, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(73.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.7065, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4518, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(73.8095, device='cuda:0'), 'loss_bbox': tensor(0.1921, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2047, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(52., device='cuda:0'), 'loss_ce_0': tensor(1.4732, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1336, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9636, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(78., device='cuda:0'), 'loss_ce_1': tensor(1.3830, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1450, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0330, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(47.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4951, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1390, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0690, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(49.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4816, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1362, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0021, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(66.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5062, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1324, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0139, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(71.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.5771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1615, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(44.4444, device='cuda:0'), 'loss_bbox': tensor(0.1547, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0582, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(49., device='cuda:0'), 'loss_ce_0': tensor(1.2362, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1492, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9947, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(81.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2048, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1295, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9750, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(53., device='cuda:0'), 'loss_ce_2': tensor(1.2551, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1226, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9654, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(37.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2085, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1348, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0154, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(57.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1726, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1246, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9343, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(71.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.0639, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0824, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(27.7778, device='cuda:0'), 'loss_bbox': tensor(0.1254, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8186, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(62.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1989, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1136, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7171, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(90.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0731, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1308, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8710, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(49., device='cuda:0'), 'loss_ce_2': tensor(1.1216, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1114, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7952, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(42., device='cuda:0'), 'loss_ce_3': tensor(1.0467, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1165, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8189, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(56., device='cuda:0'), 'loss_ce_4': tensor(1.1205, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1104, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7980, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(75.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.7507, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1234, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(32., device='cuda:0'), 'loss_bbox': tensor(0.1358, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7817, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(60., device='cuda:0'), 'loss_ce_0': tensor(1.2428, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1448, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8933, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(86.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1777, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1453, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8461, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(39.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1974, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1440, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8981, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(26.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1244, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1538, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9520, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(43., device='cuda:0'), 'loss_ce_4': tensor(1.1759, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1309, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7766, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(65.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.3790, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3624, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(54.5455, device='cuda:0'), 'loss_bbox': tensor(0.1801, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9399, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(57.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4584, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1764, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9660, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(81., device='cuda:0'), 'loss_ce_1': tensor(1.2204, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1689, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9400, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(30., device='cuda:0'), 'loss_ce_2': tensor(1.3238, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1783, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0003, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(22., device='cuda:0'), 'loss_ce_3': tensor(1.2852, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2139, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1458, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(27.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3519, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1800, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9936, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(49., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.2672, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.8561, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(65.7895, device='cuda:0'), 'loss_bbox': tensor(0.0999, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8374, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(41.5000, device='cuda:0'), 'loss_ce_0': tensor(1.9084, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1218, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9334, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(72., device='cuda:0'), 'loss_ce_1': tensor(1.8157, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1108, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9394, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(25., device='cuda:0'), 'loss_ce_2': tensor(1.9030, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0941, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8472, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(17., device='cuda:0'), 'loss_ce_3': tensor(1.8172, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1200, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0161, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(19., device='cuda:0'), 'loss_ce_4': tensor(1.8664, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1259, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9915, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(31.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.4488, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3582, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(54.8387, device='cuda:0'), 'loss_bbox': tensor(0.1227, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7806, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(34.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3446, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1438, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8826, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(59., device='cuda:0'), 'loss_ce_1': tensor(1.3579, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1265, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8200, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(11.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3737, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1248, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8247, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(23.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3758, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1325, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8738, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(14.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3997, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1437, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9869, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(17., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.3758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1436, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(68.9655, device='cuda:0'), 'loss_bbox': tensor(0.1736, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8986, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(26., device='cuda:0'), 'loss_ce_0': tensor(1.1478, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2338, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1222, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(39., device='cuda:0'), 'loss_ce_1': tensor(1.2477, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1744, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9101, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(13.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1760, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1767, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9414, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(22., device='cuda:0'), 'loss_ce_3': tensor(1.1743, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1789, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0174, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(9.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2003, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1730, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9745, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(22., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.1422, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4088, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(79.4118, device='cuda:0'), 'loss_bbox': tensor(0.1735, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9588, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(14.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3726, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2100, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0830, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(31., device='cuda:0'), 'loss_ce_1': tensor(1.4364, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1696, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9160, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(9.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3574, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1660, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9172, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(23.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3908, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1853, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0239, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(7.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3901, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1836, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0145, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(10., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.4663, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [2]  [ 60/100]  eta: 0:00:11  lr: 0.000100  class_error: 79.41  loss: 23.6510 (23.8366)  loss_ce: 1.2648 (1.2579)  loss_bbox: 0.6905 (0.7725)  loss_giou: 1.8499 (1.9905)  loss_ce_0: 1.2753 (1.2562)  loss_bbox_0: 0.7238 (0.7704)  loss_giou_0: 1.9271 (2.0206)  loss_ce_1: 1.2921 (1.2752)  loss_bbox_1: 0.7264 (0.7548)  loss_giou_1: 1.8788 (1.9697)  loss_ce_2: 1.3238 (1.2795)  loss_bbox_2: 0.6706 (0.7041)  loss_giou_2: 1.7875 (1.8829)  loss_ce_3: 1.3320 (1.2793)  loss_bbox_3: 0.6812 (0.7241)  loss_giou_3: 1.8960 (1.9132)  loss_ce_4: 1.2913 (1.2764)  loss_bbox_4: 0.6621 (0.7517)  loss_giou_4: 1.8686 (1.9576)  loss_ce_unscaled: 1.2648 (1.2579)  class_error_unscaled: 58.0645 (51.5568)  loss_bbox_unscaled: 0.1381 (0.1545)  loss_giou_unscaled: 0.9249 (0.9953)  cardinality_error_unscaled: 38.0000 (47.2213)  loss_ce_0_unscaled: 1.2753 (1.2562)  loss_bbox_0_unscaled: 0.1448 (0.1541)  loss_giou_0_unscaled: 0.9636 (1.0103)  cardinality_error_0_unscaled: 57.0000 (48.9918)  loss_ce_1_unscaled: 1.2921 (1.2752)  loss_bbox_1_unscaled: 0.1453 (0.1510)  loss_giou_1_unscaled: 0.9394 (0.9849)  cardinality_error_1_unscaled: 50.0000 (50.6475)  loss_ce_2_unscaled: 1.3238 (1.2795)  loss_bbox_2_unscaled: 0.1341 (0.1408)  loss_giou_2_unscaled: 0.8938 (0.9414)  cardinality_error_2_unscaled: 49.5000 (53.0574)  loss_ce_3_unscaled: 1.3320 (1.2793)  loss_bbox_3_unscaled: 0.1362 (0.1448)  loss_giou_3_unscaled: 0.9480 (0.9566)  cardinality_error_3_unscaled: 57.5000 (51.2459)  loss_ce_4_unscaled: 1.2913 (1.2764)  loss_bbox_4_unscaled: 0.1324 (0.1503)  loss_giou_4_unscaled: 0.9343 (0.9788)  cardinality_error_4_unscaled: 58.0000 (55.9508)  time: 0.2875  data: 0.0111  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.2023, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(55.5556, device='cuda:0'), 'loss_bbox': tensor(0.2003, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1186, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(16., device='cuda:0'), 'loss_ce_0': tensor(1.2077, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2088, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0492, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(14., device='cuda:0'), 'loss_ce_1': tensor(1.2938, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1716, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9196, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(14.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3073, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1540, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9176, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(20., device='cuda:0'), 'loss_ce_3': tensor(1.2372, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1905, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0992, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(14., device='cuda:0'), 'loss_ce_4': tensor(1.3139, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2308, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2840, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(13.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.9485, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0810, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(62.5000, device='cuda:0'), 'loss_bbox': tensor(0.1151, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8598, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(12.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1201, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1662, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9130, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(8.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1917, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1231, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8229, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(6., device='cuda:0'), 'loss_ce_2': tensor(1.1219, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1102, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7984, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(26.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1189, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1292, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8986, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(15., device='cuda:0'), 'loss_ce_4': tensor(1.0865, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1275, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9381, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(14., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.7811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0985, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(50., device='cuda:0'), 'loss_bbox': tensor(0.1783, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9418, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(23., device='cuda:0'), 'loss_ce_0': tensor(1.0892, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2045, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0114, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(9., device='cuda:0'), 'loss_ce_1': tensor(1.2128, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1659, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8745, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(10., device='cuda:0'), 'loss_ce_2': tensor(1.2130, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1642, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9340, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(36.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1834, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1640, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9388, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(15.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1929, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1825, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0985, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(17.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.6781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4381, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(56., device='cuda:0'), 'loss_bbox': tensor(0.2487, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3813, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(27., device='cuda:0'), 'loss_ce_0': tensor(1.4194, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2119, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9557, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(4.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4153, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2038, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9715, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(14., device='cuda:0'), 'loss_ce_2': tensor(1.4280, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1951, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9168, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(45., device='cuda:0'), 'loss_ce_3': tensor(1.4447, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2598, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3535, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(14., device='cuda:0'), 'loss_ce_4': tensor(1.4951, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.4208, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.6295, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(11.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.5518, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4736, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75., device='cuda:0'), 'loss_bbox': tensor(0.1332, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8763, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(19., device='cuda:0'), 'loss_ce_0': tensor(1.5981, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1414, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8208, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(12., device='cuda:0'), 'loss_ce_1': tensor(1.5490, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1335, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8197, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(9.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5757, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1190, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8065, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(41.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4862, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1697, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9517, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(5., device='cuda:0'), 'loss_ce_4': tensor(1.4547, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3005, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.3349, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(3.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.1816, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9820, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(25., device='cuda:0'), 'loss_bbox': tensor(0.2307, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7504, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(51., device='cuda:0'), 'loss_ce_0': tensor(1.0480, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2085, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7330, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(7.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0217, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2106, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7139, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(29.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0699, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2199, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8189, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9860, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2241, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7909, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(26., device='cuda:0'), 'loss_ce_4': tensor(1.0348, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2374, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8322, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(32.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.8486, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5920, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(80.4348, device='cuda:0'), 'loss_bbox': tensor(0.1403, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8675, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(44.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5639, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1682, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8750, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(14.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5800, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1485, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8592, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(28.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6342, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1517, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9582, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(53., device='cuda:0'), 'loss_ce_3': tensor(1.5086, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1437, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8482, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(32.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5183, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1435, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8662, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(38.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.2652, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4316, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(72.9167, device='cuda:0'), 'loss_bbox': tensor(0.1276, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8897, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(29.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4730, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1665, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9113, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(16., device='cuda:0'), 'loss_ce_1': tensor(1.4077, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1476, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8968, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(33.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4373, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1863, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1230, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62., device='cuda:0'), 'loss_ce_3': tensor(1.3638, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1403, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8891, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(37., device='cuda:0'), 'loss_ce_4': tensor(1.3394, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1343, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8742, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(49.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.8946, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4523, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(73.9130, device='cuda:0'), 'loss_bbox': tensor(0.1794, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8995, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(38.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3482, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1862, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8968, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(5.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4187, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1746, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8921, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(53., device='cuda:0'), 'loss_ce_2': tensor(1.5013, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2332, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1055, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(75., device='cuda:0'), 'loss_ce_3': tensor(1.5780, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1921, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9239, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(55.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5322, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1772, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8989, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(65.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.5502, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1625, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(47.3684, device='cuda:0'), 'loss_bbox': tensor(0.2221, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7890, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(29.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2071, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2174, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8054, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(4.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2892, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2113, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8043, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(49., device='cuda:0'), 'loss_ce_2': tensor(1.3970, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2047, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8216, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(75.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4165, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2123, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8321, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(57.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3428, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2231, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8368, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(68., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.8670, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [2]  [ 70/100]  eta: 0:00:08  lr: 0.000100  class_error: 47.37  loss: 24.1336 (23.9667)  loss_ce: 1.2338 (1.2626)  loss_bbox: 0.8677 (0.7887)  loss_giou: 1.7793 (1.9742)  loss_ce_0: 1.2753 (1.2634)  loss_bbox_0: 0.8411 (0.7942)  loss_giou_0: 1.8259 (1.9887)  loss_ce_1: 1.2938 (1.2840)  loss_bbox_1: 0.7380 (0.7675)  loss_giou_1: 1.7491 (1.9338)  loss_ce_2: 1.3238 (1.2921)  loss_bbox_2: 0.7584 (0.7274)  loss_giou_2: 1.8336 (1.8769)  loss_ce_3: 1.2852 (1.2868)  loss_bbox_3: 0.7755 (0.7507)  loss_giou_3: 1.8776 (1.9121)  loss_ce_4: 1.3394 (1.2841)  loss_bbox_4: 0.7975 (0.7992)  loss_giou_4: 1.8761 (1.9803)  loss_ce_unscaled: 1.2338 (1.2626)  class_error_unscaled: 55.5556 (52.7275)  loss_bbox_unscaled: 0.1735 (0.1577)  loss_giou_unscaled: 0.8897 (0.9871)  cardinality_error_unscaled: 34.5000 (44.6620)  loss_ce_0_unscaled: 1.2753 (1.2634)  loss_bbox_0_unscaled: 0.1682 (0.1588)  loss_giou_0_unscaled: 0.9130 (0.9944)  cardinality_error_0_unscaled: 16.0000 (43.4437)  loss_ce_1_unscaled: 1.2938 (1.2840)  loss_bbox_1_unscaled: 0.1476 (0.1535)  loss_giou_1_unscaled: 0.8745 (0.9669)  cardinality_error_1_unscaled: 28.5000 (47.0000)  loss_ce_2_unscaled: 1.3238 (1.2921)  loss_bbox_2_unscaled: 0.1517 (0.1455)  loss_giou_2_unscaled: 0.9168 (0.9384)  cardinality_error_2_unscaled: 37.5000 (52.5915)  loss_ce_3_unscaled: 1.2852 (1.2868)  loss_bbox_3_unscaled: 0.1551 (0.1501)  loss_giou_3_unscaled: 0.9388 (0.9560)  cardinality_error_3_unscaled: 26.0000 (47.8592)  loss_ce_4_unscaled: 1.3394 (1.2841)  loss_bbox_4_unscaled: 0.1595 (0.1598)  loss_giou_4_unscaled: 0.9381 (0.9902)  cardinality_error_4_unscaled: 32.5000 (52.4930)  time: 0.2764  data: 0.0100  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.2018, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(61.0169, device='cuda:0'), 'loss_bbox': tensor(0.1929, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0169, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(5.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3276, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2147, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0217, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(14., device='cuda:0'), 'loss_ce_1': tensor(1.1572, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1975, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0425, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(34., device='cuda:0'), 'loss_ce_2': tensor(1.2131, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2542, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2336, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(57., device='cuda:0'), 'loss_ce_3': tensor(1.2491, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2009, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0110, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(46., device='cuda:0'), 'loss_ce_4': tensor(1.1711, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2049, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0736, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(56.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.2427, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0192, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60., device='cuda:0'), 'loss_bbox': tensor(0.1106, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.6597, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(26.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0698, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1307, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.6667, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(18., device='cuda:0'), 'loss_ce_1': tensor(1.1947, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1300, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7662, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(62., device='cuda:0'), 'loss_ce_2': tensor(1.3234, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1027, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.6581, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(72.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3030, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1222, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.6554, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(69., device='cuda:0'), 'loss_ce_4': tensor(1.2824, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1166, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7146, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(80., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.0073, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1417, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60., device='cuda:0'), 'loss_bbox': tensor(0.1289, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8641, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(20., device='cuda:0'), 'loss_ce_0': tensor(1.1363, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1689, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9903, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(12.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1915, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1476, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9583, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58., device='cuda:0'), 'loss_ce_2': tensor(1.2849, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1319, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9175, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(68., device='cuda:0'), 'loss_ce_3': tensor(1.3140, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1479, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0351, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(66., device='cuda:0'), 'loss_ce_4': tensor(1.2618, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1400, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9577, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(76., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.8984, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2775, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(91.6667, device='cuda:0'), 'loss_bbox': tensor(0.1249, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7778, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(25.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2106, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1392, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8529, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(31.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4377, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1196, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8072, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(65.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4562, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1172, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7976, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(74., device='cuda:0'), 'loss_ce_3': tensor(1.5593, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1442, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8502, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(70., device='cuda:0'), 'loss_ce_4': tensor(1.6402, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1380, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9222, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(86., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.3179, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1829, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(52.7273, device='cuda:0'), 'loss_bbox': tensor(0.1877, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9607, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(7., device='cuda:0'), 'loss_ce_0': tensor(1.1324, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2389, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0704, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(10.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2207, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1457, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8643, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(41.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1940, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1242, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7889, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(42.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2025, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1342, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8412, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(41., device='cuda:0'), 'loss_ce_4': tensor(1.2407, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1453, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8782, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(61., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.7765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3302, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(65.1163, device='cuda:0'), 'loss_bbox': tensor(0.2915, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1255, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(8., device='cuda:0'), 'loss_ce_0': tensor(1.3345, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3597, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2419, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(15., device='cuda:0'), 'loss_ce_1': tensor(1.4655, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2587, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0096, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(50.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4519, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2393, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0424, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(35.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4665, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2750, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0793, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(37.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4865, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2863, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1152, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(55.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(33.0565, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1246, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(31.2500, device='cuda:0'), 'loss_bbox': tensor(0.1720, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0999, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(7., device='cuda:0'), 'loss_ce_0': tensor(1.2330, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1965, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0651, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(13., device='cuda:0'), 'loss_ce_1': tensor(1.0918, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1427, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9313, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(38., device='cuda:0'), 'loss_ce_2': tensor(1.1684, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1358, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9322, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(28.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1337, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1754, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0372, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(29., device='cuda:0'), 'loss_ce_4': tensor(1.0440, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1597, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9713, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(51.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.5715, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3093, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(56.2500, device='cuda:0'), 'loss_bbox': tensor(0.2514, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0624, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(25.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4129, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2764, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0747, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(44.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4700, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2294, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8742, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(74.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3804, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2185, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8536, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(47., device='cuda:0'), 'loss_ce_3': tensor(1.4096, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2306, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8561, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(45.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4594, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2116, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8661, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(66., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.5157, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2037, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(54.5455, device='cuda:0'), 'loss_bbox': tensor(0.2007, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0396, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(23., device='cuda:0'), 'loss_ce_0': tensor(1.2590, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2055, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0088, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(26., device='cuda:0'), 'loss_ce_1': tensor(1.3255, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1474, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8834, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58., device='cuda:0'), 'loss_ce_2': tensor(1.3136, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1656, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9017, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(34.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2783, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1802, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9456, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(42., device='cuda:0'), 'loss_ce_4': tensor(1.3344, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1730, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9185, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(60.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.2347, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0118, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(35.4839, device='cuda:0'), 'loss_bbox': tensor(0.1722, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0749, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(35., device='cuda:0'), 'loss_ce_0': tensor(1.0731, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1776, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9913, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(46., device='cuda:0'), 'loss_ce_1': tensor(1.1431, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1461, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8746, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(67., device='cuda:0'), 'loss_ce_2': tensor(1.1575, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1498, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8856, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(29.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1032, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1700, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9316, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(45., device='cuda:0'), 'loss_ce_4': tensor(1.1208, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1521, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8816, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(61., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.4977, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [2]  [ 80/100]  eta: 0:00:05  lr: 0.000100  class_error: 35.48  loss: 24.0489 (23.9945)  loss_ce: 1.2018 (1.2524)  loss_bbox: 0.8913 (0.8045)  loss_giou: 1.7991 (1.9695)  loss_ce_0: 1.2106 (1.2579)  loss_bbox_0: 0.9825 (0.8263)  loss_giou_0: 1.9113 (1.9897)  loss_ce_1: 1.2892 (1.2823)  loss_bbox_1: 0.7381 (0.7755)  loss_giou_1: 1.7491 (1.9176)  loss_ce_2: 1.3136 (1.2923)  loss_bbox_2: 0.7701 (0.7388)  loss_giou_2: 1.8033 (1.8677)  loss_ce_3: 1.3030 (1.2886)  loss_bbox_3: 0.8501 (0.7679)  loss_giou_3: 1.8478 (1.9042)  loss_ce_4: 1.3139 (1.2866)  loss_bbox_4: 0.8649 (0.8071)  loss_giou_4: 1.8370 (1.9654)  loss_ce_unscaled: 1.2018 (1.2524)  class_error_unscaled: 56.2500 (53.2310)  loss_bbox_unscaled: 0.1783 (0.1609)  loss_giou_unscaled: 0.8995 (0.9848)  cardinality_error_unscaled: 23.0000 (41.4074)  loss_ce_0_unscaled: 1.2106 (1.2579)  loss_bbox_0_unscaled: 0.1965 (0.1653)  loss_giou_0_unscaled: 0.9557 (0.9949)  cardinality_error_0_unscaled: 13.0000 (40.9321)  loss_ce_1_unscaled: 1.2892 (1.2823)  loss_bbox_1_unscaled: 0.1476 (0.1551)  loss_giou_1_unscaled: 0.8745 (0.9588)  cardinality_error_1_unscaled: 38.0000 (47.9753)  loss_ce_2_unscaled: 1.3136 (1.2923)  loss_bbox_2_unscaled: 0.1540 (0.1478)  loss_giou_2_unscaled: 0.9017 (0.9338)  cardinality_error_2_unscaled: 45.0000 (52.1358)  loss_ce_3_unscaled: 1.3030 (1.2886)  loss_bbox_3_unscaled: 0.1700 (0.1536)  loss_giou_3_unscaled: 0.9239 (0.9521)  cardinality_error_3_unscaled: 37.5000 (48.0123)  loss_ce_4_unscaled: 1.3139 (1.2866)  loss_bbox_4_unscaled: 0.1730 (0.1614)  loss_giou_4_unscaled: 0.9185 (0.9827)  cardinality_error_4_unscaled: 55.5000 (54.0864)  time: 0.2722  data: 0.0097  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.0519, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(32., device='cuda:0'), 'loss_bbox': tensor(0.1353, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9413, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(42., device='cuda:0'), 'loss_ce_0': tensor(1.0506, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1266, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8634, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(49., device='cuda:0'), 'loss_ce_1': tensor(1.1625, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1074, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8623, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(72.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1510, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1187, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8998, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(45., device='cuda:0'), 'loss_ce_3': tensor(1.1398, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1318, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8853, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(55., device='cuda:0'), 'loss_ce_4': tensor(1.1718, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1461, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0197, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(67., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.2872, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3695, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(57.1429, device='cuda:0'), 'loss_bbox': tensor(0.1335, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8657, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(47.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3669, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1343, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8821, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(55.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4695, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1257, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9399, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(80., device='cuda:0'), 'loss_ce_2': tensor(1.3555, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1219, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8206, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(39.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4272, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1218, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7738, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(55.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4760, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1321, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8671, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(65., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.4712, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1599, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(43.3333, device='cuda:0'), 'loss_bbox': tensor(0.1177, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7863, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(44., device='cuda:0'), 'loss_ce_0': tensor(1.2293, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1110, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7893, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(66., device='cuda:0'), 'loss_ce_1': tensor(1.2949, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1227, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9162, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(68., device='cuda:0'), 'loss_ce_2': tensor(1.2733, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1215, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8461, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(37.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2489, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1210, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8100, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62., device='cuda:0'), 'loss_ce_4': tensor(1.2262, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1407, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8956, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(67.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.9901, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5170, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(70.7692, device='cuda:0'), 'loss_bbox': tensor(0.1121, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8605, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(31., device='cuda:0'), 'loss_ce_0': tensor(1.5576, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1328, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9821, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(53., device='cuda:0'), 'loss_ce_1': tensor(1.6879, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2296, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1924, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(55.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5825, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1607, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0243, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(17., device='cuda:0'), 'loss_ce_3': tensor(1.6425, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1581, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0258, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(41., device='cuda:0'), 'loss_ce_4': tensor(1.6122, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1499, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0057, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(44.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.2788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1999, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(42.8571, device='cuda:0'), 'loss_bbox': tensor(0.1535, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9634, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(46.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2568, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1684, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0636, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(76., device='cuda:0'), 'loss_ce_1': tensor(1.3633, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1956, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2022, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(69.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2854, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1725, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0584, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(26., device='cuda:0'), 'loss_ce_3': tensor(1.3159, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1612, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0443, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(57., device='cuda:0'), 'loss_ce_4': tensor(1.3076, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1811, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0843, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(64.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.5593, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.9488, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(79.5455, device='cuda:0'), 'loss_bbox': tensor(0.1306, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8749, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(31., device='cuda:0'), 'loss_ce_0': tensor(1.8414, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2267, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1808, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(58.5000, device='cuda:0'), 'loss_ce_1': tensor(1.8976, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3168, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4065, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(47., device='cuda:0'), 'loss_ce_2': tensor(1.8136, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1750, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9905, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(9.5000, device='cuda:0'), 'loss_ce_3': tensor(1.9978, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1449, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9370, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(41.5000, device='cuda:0'), 'loss_ce_4': tensor(2.0055, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1733, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0155, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(45.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.9551, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1480, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(45.7143, device='cuda:0'), 'loss_bbox': tensor(0.1623, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9401, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(31., device='cuda:0'), 'loss_ce_0': tensor(1.1323, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1495, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9286, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(63.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1891, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1557, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0338, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(43.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2244, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1611, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8995, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(8., device='cuda:0'), 'loss_ce_3': tensor(1.2450, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1555, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8890, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(41., device='cuda:0'), 'loss_ce_4': tensor(1.2375, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1576, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9056, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(51., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.9271, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1528, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(62.5000, device='cuda:0'), 'loss_bbox': tensor(0.1249, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7881, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(36., device='cuda:0'), 'loss_ce_0': tensor(1.1571, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1476, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9285, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(66., device='cuda:0'), 'loss_ce_1': tensor(1.1448, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1416, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8975, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(34.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2136, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1089, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7712, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(16., device='cuda:0'), 'loss_ce_3': tensor(1.2377, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1144, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7680, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(41.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2113, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1192, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7241, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(62., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.4968, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2382, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(71.7949, device='cuda:0'), 'loss_bbox': tensor(0.1014, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7858, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(20., device='cuda:0'), 'loss_ce_0': tensor(1.2036, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1168, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8325, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(51., device='cuda:0'), 'loss_ce_1': tensor(1.3213, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0996, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8199, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(11., device='cuda:0'), 'loss_ce_2': tensor(1.3138, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1020, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7283, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(8., device='cuda:0'), 'loss_ce_3': tensor(1.2855, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0988, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7279, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(24.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2568, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1178, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7897, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(46., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.9975, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2690, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(68.9655, device='cuda:0'), 'loss_bbox': tensor(0.1141, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8726, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(26.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2204, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1268, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9646, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(44.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2876, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1099, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9562, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(3., device='cuda:0'), 'loss_ce_2': tensor(1.2857, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1201, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8899, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(6., device='cuda:0'), 'loss_ce_3': tensor(1.2607, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1270, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9125, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(35., device='cuda:0'), 'loss_ce_4': tensor(1.2405, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1413, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9230, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(43.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.1566, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [2]  [ 90/100]  eta: 0:00:02  lr: 0.000100  class_error: 68.97  loss: 22.8605 (23.9278)  loss_ce: 1.1999 (1.2583)  loss_bbox: 0.6677 (0.7867)  loss_giou: 1.7497 (1.9438)  loss_ce_0: 1.2204 (1.2627)  loss_bbox_0: 0.7477 (0.8147)  loss_giou_0: 1.9641 (1.9780)  loss_ce_1: 1.2876 (1.2932)  loss_bbox_1: 0.7285 (0.7785)  loss_giou_1: 1.8324 (1.9317)  loss_ce_2: 1.2854 (1.2987)  loss_bbox_2: 0.6593 (0.7325)  loss_giou_2: 1.7798 (1.8586)  loss_ce_3: 1.2783 (1.2987)  loss_bbox_3: 0.7247 (0.7569)  loss_giou_3: 1.7781 (1.8878)  loss_ce_4: 1.2568 (1.2963)  loss_bbox_4: 0.7306 (0.7986)  loss_giou_4: 1.8370 (1.9523)  loss_ce_unscaled: 1.1999 (1.2583)  class_error_unscaled: 57.1429 (53.6959)  loss_bbox_unscaled: 0.1335 (0.1573)  loss_giou_unscaled: 0.8749 (0.9719)  cardinality_error_unscaled: 26.5000 (40.7637)  loss_ce_0_unscaled: 1.2204 (1.2627)  loss_bbox_0_unscaled: 0.1495 (0.1629)  loss_giou_0_unscaled: 0.9821 (0.9890)  cardinality_error_0_unscaled: 44.5000 (42.8407)  loss_ce_1_unscaled: 1.2876 (1.2932)  loss_bbox_1_unscaled: 0.1457 (0.1557)  loss_giou_1_unscaled: 0.9162 (0.9658)  cardinality_error_1_unscaled: 55.5000 (48.0275)  loss_ce_2_unscaled: 1.2854 (1.2987)  loss_bbox_2_unscaled: 0.1319 (0.1465)  loss_giou_2_unscaled: 0.8899 (0.9293)  cardinality_error_2_unscaled: 34.5000 (48.7418)  loss_ce_3_unscaled: 1.2783 (1.2987)  loss_bbox_3_unscaled: 0.1449 (0.1514)  loss_giou_3_unscaled: 0.8890 (0.9439)  cardinality_error_3_unscaled: 42.0000 (47.7253)  loss_ce_4_unscaled: 1.2568 (1.2963)  loss_bbox_4_unscaled: 0.1461 (0.1597)  loss_giou_4_unscaled: 0.9185 (0.9762)  cardinality_error_4_unscaled: 61.0000 (54.2582)  time: 0.2716  data: 0.0097  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.0840, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(36.3636, device='cuda:0'), 'loss_bbox': tensor(0.1456, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7817, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(20.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1226, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1477, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7958, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(30.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1764, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1534, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8012, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(11., device='cuda:0'), 'loss_ce_2': tensor(1.2300, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1564, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8681, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(11.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1768, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1510, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7736, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(22., device='cuda:0'), 'loss_ce_4': tensor(1.1276, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1472, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7570, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(40., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.7734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0442, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(35.7143, device='cuda:0'), 'loss_bbox': tensor(0.2565, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8933, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(46.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0579, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2289, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7719, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(44., device='cuda:0'), 'loss_ce_1': tensor(0.9690, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2437, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7868, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(12., device='cuda:0'), 'loss_ce_2': tensor(1.0680, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2546, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8779, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(23., device='cuda:0'), 'loss_ce_3': tensor(1.0953, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2373, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7970, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(54.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1032, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2226, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7553, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(71.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.1505, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0447, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(33.3333, device='cuda:0'), 'loss_bbox': tensor(0.2041, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1007, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(54.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0363, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2008, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0896, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(53.5000, device='cuda:0'), 'loss_ce_1': tensor(0.9628, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2066, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1485, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(6.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0304, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2344, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0959, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(33., device='cuda:0'), 'loss_ce_3': tensor(1.1071, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2246, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1013, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(72., device='cuda:0'), 'loss_ce_4': tensor(1.0845, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2229, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0664, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(75., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.7731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1845, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(38.0952, device='cuda:0'), 'loss_bbox': tensor(0.1381, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8888, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(43., device='cuda:0'), 'loss_ce_0': tensor(1.1392, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1503, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9207, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(32.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2232, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3018, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2775, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(16., device='cuda:0'), 'loss_ce_2': tensor(1.1970, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2011, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0419, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(21.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1848, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1321, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8260, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62., device='cuda:0'), 'loss_ce_4': tensor(1.2103, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1257, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7989, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(55., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.6949, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0671, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(18.4211, device='cuda:0'), 'loss_bbox': tensor(0.0804, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7444, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(51.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1248, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0960, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7959, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(23.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1008, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1672, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0201, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(9., device='cuda:0'), 'loss_ce_2': tensor(1.0719, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1225, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9668, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(33., device='cuda:0'), 'loss_ce_3': tensor(1.0789, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0874, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7533, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(68.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0466, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1059, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8556, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(62., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.8942, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1946, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(39.3939, device='cuda:0'), 'loss_bbox': tensor(0.1328, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9688, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(63.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2387, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1350, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9431, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(37., device='cuda:0'), 'loss_ce_1': tensor(1.1331, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1734, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0254, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(5.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1957, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1262, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9095, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(57.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1651, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1301, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8808, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(75., device='cuda:0'), 'loss_ce_4': tensor(1.1428, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1368, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9473, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(66., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.3490, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4902, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(56.2500, device='cuda:0'), 'loss_bbox': tensor(0.1502, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9424, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(70., device='cuda:0'), 'loss_ce_0': tensor(1.3943, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1739, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9988, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(39., device='cuda:0'), 'loss_ce_1': tensor(1.3790, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2020, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0934, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(18., device='cuda:0'), 'loss_ce_2': tensor(1.5027, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1554, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9016, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(75.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4414, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1515, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8737, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(82.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4064, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1469, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8921, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(65.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.6766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(2.0883, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(97.8261, device='cuda:0'), 'loss_bbox': tensor(0.1426, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9575, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(67., device='cuda:0'), 'loss_ce_0': tensor(1.9250, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1444, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9210, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(42., device='cuda:0'), 'loss_ce_1': tensor(1.9463, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2320, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1364, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(26., device='cuda:0'), 'loss_ce_2': tensor(2.1147, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1381, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9131, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(74., device='cuda:0'), 'loss_ce_3': tensor(2.0323, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1465, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9356, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(76.5000, device='cuda:0'), 'loss_ce_4': tensor(1.9795, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1350, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8903, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(56., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(31.0997, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2856, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(30., device='cuda:0'), 'loss_bbox': tensor(0.1770, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0680, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(79.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1964, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1340, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7929, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(48., device='cuda:0'), 'loss_ce_1': tensor(1.1514, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1402, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7936, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(34., device='cuda:0'), 'loss_ce_2': tensor(1.2156, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1360, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8375, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(88., device='cuda:0'), 'loss_ce_3': tensor(1.2262, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1384, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8032, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(89., device='cuda:0'), 'loss_ce_4': tensor(1.1875, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1398, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8105, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(63., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.6329, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [2]  [ 99/100]  eta: 0:00:00  lr: 0.000100  class_error: 30.00  loss: 22.6096 (23.8922)  loss_ce: 1.1845 (1.2598)  loss_bbox: 0.6765 (0.7873)  loss_giou: 1.7776 (1.9358)  loss_ce_0: 1.1964 (1.2614)  loss_bbox_0: 0.7222 (0.8119)  loss_giou_0: 1.8421 (1.9606)  loss_ce_1: 1.1891 (1.2872)  loss_bbox_1: 0.7787 (0.7994)  loss_giou_1: 1.9124 (1.9395)  loss_ce_2: 1.2244 (1.2980)  loss_bbox_2: 0.6904 (0.7428)  loss_giou_2: 1.7990 (1.8596)  loss_ce_3: 1.2377 (1.2969)  loss_bbox_3: 0.6920 (0.7587)  loss_giou_3: 1.7474 (1.8728)  loss_ce_4: 1.2113 (1.2925)  loss_bbox_4: 0.7064 (0.7959)  loss_giou_4: 1.7807 (1.9321)  loss_ce_unscaled: 1.1845 (1.2598)  class_error_unscaled: 42.8571 (52.7173)  loss_bbox_unscaled: 0.1353 (0.1575)  loss_giou_unscaled: 0.8888 (0.9679)  cardinality_error_unscaled: 43.0000 (42.0550)  loss_ce_0_unscaled: 1.1964 (1.2614)  loss_bbox_0_unscaled: 0.1444 (0.1624)  loss_giou_0_unscaled: 0.9210 (0.9803)  cardinality_error_0_unscaled: 48.0000 (42.4850)  loss_ce_1_unscaled: 1.1891 (1.2872)  loss_bbox_1_unscaled: 0.1557 (0.1599)  loss_giou_1_unscaled: 0.9562 (0.9697)  cardinality_error_1_unscaled: 26.0000 (45.0850)  loss_ce_2_unscaled: 1.2244 (1.2980)  loss_bbox_2_unscaled: 0.1381 (0.1486)  loss_giou_2_unscaled: 0.8995 (0.9298)  cardinality_error_2_unscaled: 26.0000 (48.5250)  loss_ce_3_unscaled: 1.2377 (1.2969)  loss_bbox_3_unscaled: 0.1384 (0.1517)  loss_giou_3_unscaled: 0.8737 (0.9364)  cardinality_error_3_unscaled: 55.0000 (49.4500)  loss_ce_4_unscaled: 1.2113 (1.2925)  loss_bbox_4_unscaled: 0.1413 (0.1592)  loss_giou_4_unscaled: 0.8903 (0.9660)  cardinality_error_4_unscaled: 62.0000 (54.9150)  time: 0.2712  data: 0.0101  max mem: 1537\n",
            "Epoch: [2] Total time: 0:00:28 (0.2843 s / it)\n",
            "Averaged stats: lr: 0.000100  class_error: 30.00  loss: 22.6096 (23.8922)  loss_ce: 1.1845 (1.2598)  loss_bbox: 0.6765 (0.7873)  loss_giou: 1.7776 (1.9358)  loss_ce_0: 1.1964 (1.2614)  loss_bbox_0: 0.7222 (0.8119)  loss_giou_0: 1.8421 (1.9606)  loss_ce_1: 1.1891 (1.2872)  loss_bbox_1: 0.7787 (0.7994)  loss_giou_1: 1.9124 (1.9395)  loss_ce_2: 1.2244 (1.2980)  loss_bbox_2: 0.6904 (0.7428)  loss_giou_2: 1.7990 (1.8596)  loss_ce_3: 1.2377 (1.2969)  loss_bbox_3: 0.6920 (0.7587)  loss_giou_3: 1.7474 (1.8728)  loss_ce_4: 1.2113 (1.2925)  loss_bbox_4: 0.7064 (0.7959)  loss_giou_4: 1.7807 (1.9321)  loss_ce_unscaled: 1.1845 (1.2598)  class_error_unscaled: 42.8571 (52.7173)  loss_bbox_unscaled: 0.1353 (0.1575)  loss_giou_unscaled: 0.8888 (0.9679)  cardinality_error_unscaled: 43.0000 (42.0550)  loss_ce_0_unscaled: 1.1964 (1.2614)  loss_bbox_0_unscaled: 0.1444 (0.1624)  loss_giou_0_unscaled: 0.9210 (0.9803)  cardinality_error_0_unscaled: 48.0000 (42.4850)  loss_ce_1_unscaled: 1.1891 (1.2872)  loss_bbox_1_unscaled: 0.1557 (0.1599)  loss_giou_1_unscaled: 0.9562 (0.9697)  cardinality_error_1_unscaled: 26.0000 (45.0850)  loss_ce_2_unscaled: 1.2244 (1.2980)  loss_bbox_2_unscaled: 0.1381 (0.1486)  loss_giou_2_unscaled: 0.8995 (0.9298)  cardinality_error_2_unscaled: 26.0000 (48.5250)  loss_ce_3_unscaled: 1.2377 (1.2969)  loss_bbox_3_unscaled: 0.1384 (0.1517)  loss_giou_3_unscaled: 0.8737 (0.9364)  cardinality_error_3_unscaled: 55.0000 (49.4500)  loss_ce_4_unscaled: 1.2113 (1.2925)  loss_bbox_4_unscaled: 0.1413 (0.1592)  loss_giou_4_unscaled: 0.8903 (0.9660)  cardinality_error_4_unscaled: 62.0000 (54.9150)\n",
            "Test:  [ 0/25]  eta: 0:00:08  class_error: 46.51  loss: 40.3386 (40.3386)  loss_ce: 1.4380 (1.4380)  loss_bbox: 1.8858 (1.8858)  loss_giou: 3.4448 (3.4448)  loss_ce_0: 1.4203 (1.4203)  loss_bbox_0: 1.9609 (1.9609)  loss_giou_0: 3.3903 (3.3903)  loss_ce_1: 1.3896 (1.3896)  loss_bbox_1: 2.2717 (2.2717)  loss_giou_1: 3.3675 (3.3675)  loss_ce_2: 1.4859 (1.4859)  loss_bbox_2: 1.7480 (1.7480)  loss_giou_2: 3.2541 (3.2541)  loss_ce_3: 1.4195 (1.4195)  loss_bbox_3: 1.8531 (1.8531)  loss_giou_3: 3.4286 (3.4286)  loss_ce_4: 1.4344 (1.4344)  loss_bbox_4: 1.8149 (1.8149)  loss_giou_4: 3.3312 (3.3312)  loss_ce_unscaled: 1.4380 (1.4380)  class_error_unscaled: 46.5116 (46.5116)  loss_bbox_unscaled: 0.3772 (0.3772)  loss_giou_unscaled: 1.7224 (1.7224)  cardinality_error_unscaled: 78.5000 (78.5000)  loss_ce_0_unscaled: 1.4203 (1.4203)  loss_bbox_0_unscaled: 0.3922 (0.3922)  loss_giou_0_unscaled: 1.6951 (1.6951)  cardinality_error_0_unscaled: 78.5000 (78.5000)  loss_ce_1_unscaled: 1.3896 (1.3896)  loss_bbox_1_unscaled: 0.4543 (0.4543)  loss_giou_1_unscaled: 1.6838 (1.6838)  cardinality_error_1_unscaled: 78.5000 (78.5000)  loss_ce_2_unscaled: 1.4859 (1.4859)  loss_bbox_2_unscaled: 0.3496 (0.3496)  loss_giou_2_unscaled: 1.6271 (1.6271)  cardinality_error_2_unscaled: 78.5000 (78.5000)  loss_ce_3_unscaled: 1.4195 (1.4195)  loss_bbox_3_unscaled: 0.3706 (0.3706)  loss_giou_3_unscaled: 1.7143 (1.7143)  cardinality_error_3_unscaled: 78.5000 (78.5000)  loss_ce_4_unscaled: 1.4344 (1.4344)  loss_bbox_4_unscaled: 0.3630 (0.3630)  loss_giou_4_unscaled: 1.6656 (1.6656)  cardinality_error_4_unscaled: 78.5000 (78.5000)  time: 0.3256  data: 0.1681  max mem: 1537\n",
            "Test:  [10/25]  eta: 0:00:02  class_error: 11.54  loss: 40.3386 (40.1901)  loss_ce: 1.7025 (1.7074)  loss_bbox: 2.0917 (2.0749)  loss_giou: 3.5018 (3.5266)  loss_ce_0: 1.7005 (1.6752)  loss_bbox_0: 1.4325 (1.4346)  loss_giou_0: 3.0763 (3.1016)  loss_ce_1: 1.6513 (1.6304)  loss_bbox_1: 1.6587 (1.6433)  loss_giou_1: 3.2794 (3.2122)  loss_ce_2: 1.7493 (1.7625)  loss_bbox_2: 1.6261 (1.6294)  loss_giou_2: 3.3087 (3.3120)  loss_ce_3: 1.6794 (1.6803)  loss_bbox_3: 2.0565 (2.0066)  loss_giou_3: 3.4366 (3.4841)  loss_ce_4: 1.6836 (1.6790)  loss_bbox_4: 1.4705 (1.4575)  loss_giou_4: 3.1612 (3.1725)  loss_ce_unscaled: 1.7025 (1.7074)  class_error_unscaled: 46.5116 (50.5515)  loss_bbox_unscaled: 0.4183 (0.4150)  loss_giou_unscaled: 1.7509 (1.7633)  cardinality_error_unscaled: 80.0000 (81.0000)  loss_ce_0_unscaled: 1.7005 (1.6752)  loss_bbox_0_unscaled: 0.2865 (0.2869)  loss_giou_0_unscaled: 1.5381 (1.5508)  cardinality_error_0_unscaled: 80.0000 (81.0000)  loss_ce_1_unscaled: 1.6513 (1.6304)  loss_bbox_1_unscaled: 0.3317 (0.3287)  loss_giou_1_unscaled: 1.6397 (1.6061)  cardinality_error_1_unscaled: 80.0000 (81.0000)  loss_ce_2_unscaled: 1.7493 (1.7625)  loss_bbox_2_unscaled: 0.3252 (0.3259)  loss_giou_2_unscaled: 1.6544 (1.6560)  cardinality_error_2_unscaled: 80.0000 (81.0000)  loss_ce_3_unscaled: 1.6794 (1.6803)  loss_bbox_3_unscaled: 0.4113 (0.4013)  loss_giou_3_unscaled: 1.7183 (1.7420)  cardinality_error_3_unscaled: 80.0000 (81.0000)  loss_ce_4_unscaled: 1.6836 (1.6790)  loss_bbox_4_unscaled: 0.2941 (0.2915)  loss_giou_4_unscaled: 1.5806 (1.5863)  cardinality_error_4_unscaled: 80.0000 (81.0000)  time: 0.1408  data: 0.0233  max mem: 1537\n",
            "Test:  [20/25]  eta: 0:00:00  class_error: 88.89  loss: 38.9589 (39.8192)  loss_ce: 1.4368 (1.6390)  loss_bbox: 1.9283 (2.0340)  loss_giou: 3.5018 (3.5263)  loss_ce_0: 1.4445 (1.6187)  loss_bbox_0: 1.4338 (1.4366)  loss_giou_0: 3.1191 (3.1454)  loss_ce_1: 1.4201 (1.5750)  loss_bbox_1: 1.7207 (1.6609)  loss_giou_1: 3.2794 (3.2748)  loss_ce_2: 1.4745 (1.6998)  loss_bbox_2: 1.5207 (1.5912)  loss_giou_2: 3.2903 (3.3004)  loss_ce_3: 1.4206 (1.6137)  loss_bbox_3: 1.8676 (1.9717)  loss_giou_3: 3.4630 (3.4791)  loss_ce_4: 1.4427 (1.6232)  loss_bbox_4: 1.4096 (1.4405)  loss_giou_4: 3.1853 (3.1889)  loss_ce_unscaled: 1.4368 (1.6390)  class_error_unscaled: 42.8571 (51.3765)  loss_bbox_unscaled: 0.3857 (0.4068)  loss_giou_unscaled: 1.7509 (1.7632)  cardinality_error_unscaled: 79.0000 (77.7857)  loss_ce_0_unscaled: 1.4445 (1.6187)  loss_bbox_0_unscaled: 0.2868 (0.2873)  loss_giou_0_unscaled: 1.5595 (1.5727)  cardinality_error_0_unscaled: 79.0000 (77.7857)  loss_ce_1_unscaled: 1.4201 (1.5750)  loss_bbox_1_unscaled: 0.3441 (0.3322)  loss_giou_1_unscaled: 1.6397 (1.6374)  cardinality_error_1_unscaled: 79.0000 (77.7857)  loss_ce_2_unscaled: 1.4745 (1.6998)  loss_bbox_2_unscaled: 0.3041 (0.3182)  loss_giou_2_unscaled: 1.6452 (1.6502)  cardinality_error_2_unscaled: 79.0000 (77.7857)  loss_ce_3_unscaled: 1.4206 (1.6137)  loss_bbox_3_unscaled: 0.3735 (0.3943)  loss_giou_3_unscaled: 1.7315 (1.7396)  cardinality_error_3_unscaled: 79.0000 (77.7857)  loss_ce_4_unscaled: 1.4427 (1.6232)  loss_bbox_4_unscaled: 0.2819 (0.2881)  loss_giou_4_unscaled: 1.5927 (1.5945)  cardinality_error_4_unscaled: 79.0000 (77.7857)  time: 0.1328  data: 0.0095  max mem: 1537\n",
            "Test:  [24/25]  eta: 0:00:00  class_error: 32.56  loss: 38.5809 (39.8071)  loss_ce: 1.4368 (1.6535)  loss_bbox: 1.9283 (2.0271)  loss_giou: 3.4945 (3.5053)  loss_ce_0: 1.4445 (1.6269)  loss_bbox_0: 1.4418 (1.4451)  loss_giou_0: 3.1191 (3.1342)  loss_ce_1: 1.4201 (1.5841)  loss_bbox_1: 1.7326 (1.6761)  loss_giou_1: 3.3357 (3.2742)  loss_ce_2: 1.4745 (1.7079)  loss_bbox_2: 1.5207 (1.5879)  loss_giou_2: 3.2549 (3.2868)  loss_ce_3: 1.4206 (1.6264)  loss_bbox_3: 1.8676 (1.9613)  loss_giou_3: 3.4246 (3.4562)  loss_ce_4: 1.4427 (1.6325)  loss_bbox_4: 1.4234 (1.4440)  loss_giou_4: 3.1853 (3.1777)  loss_ce_unscaled: 1.4368 (1.6535)  class_error_unscaled: 42.8571 (50.3626)  loss_bbox_unscaled: 0.3857 (0.4054)  loss_giou_unscaled: 1.7473 (1.7526)  cardinality_error_unscaled: 79.0000 (78.8000)  loss_ce_0_unscaled: 1.4445 (1.6269)  loss_bbox_0_unscaled: 0.2884 (0.2890)  loss_giou_0_unscaled: 1.5595 (1.5671)  cardinality_error_0_unscaled: 79.0000 (78.8000)  loss_ce_1_unscaled: 1.4201 (1.5841)  loss_bbox_1_unscaled: 0.3465 (0.3352)  loss_giou_1_unscaled: 1.6679 (1.6371)  cardinality_error_1_unscaled: 79.0000 (78.8000)  loss_ce_2_unscaled: 1.4745 (1.7079)  loss_bbox_2_unscaled: 0.3041 (0.3176)  loss_giou_2_unscaled: 1.6275 (1.6434)  cardinality_error_2_unscaled: 79.0000 (78.8000)  loss_ce_3_unscaled: 1.4206 (1.6264)  loss_bbox_3_unscaled: 0.3735 (0.3923)  loss_giou_3_unscaled: 1.7123 (1.7281)  cardinality_error_3_unscaled: 79.0000 (78.8000)  loss_ce_4_unscaled: 1.4427 (1.6325)  loss_bbox_4_unscaled: 0.2847 (0.2888)  loss_giou_4_unscaled: 1.5927 (1.5889)  cardinality_error_4_unscaled: 79.0000 (78.8000)  time: 0.1347  data: 0.0095  max mem: 1537\n",
            "Test: Total time: 0:00:03 (0.1419 s / it)\n",
            "Averaged stats: class_error: 32.56  loss: 38.5809 (39.8071)  loss_ce: 1.4368 (1.6535)  loss_bbox: 1.9283 (2.0271)  loss_giou: 3.4945 (3.5053)  loss_ce_0: 1.4445 (1.6269)  loss_bbox_0: 1.4418 (1.4451)  loss_giou_0: 3.1191 (3.1342)  loss_ce_1: 1.4201 (1.5841)  loss_bbox_1: 1.7326 (1.6761)  loss_giou_1: 3.3357 (3.2742)  loss_ce_2: 1.4745 (1.7079)  loss_bbox_2: 1.5207 (1.5879)  loss_giou_2: 3.2549 (3.2868)  loss_ce_3: 1.4206 (1.6264)  loss_bbox_3: 1.8676 (1.9613)  loss_giou_3: 3.4246 (3.4562)  loss_ce_4: 1.4427 (1.6325)  loss_bbox_4: 1.4234 (1.4440)  loss_giou_4: 3.1853 (3.1777)  loss_ce_unscaled: 1.4368 (1.6535)  class_error_unscaled: 42.8571 (50.3626)  loss_bbox_unscaled: 0.3857 (0.4054)  loss_giou_unscaled: 1.7473 (1.7526)  cardinality_error_unscaled: 79.0000 (78.8000)  loss_ce_0_unscaled: 1.4445 (1.6269)  loss_bbox_0_unscaled: 0.2884 (0.2890)  loss_giou_0_unscaled: 1.5595 (1.5671)  cardinality_error_0_unscaled: 79.0000 (78.8000)  loss_ce_1_unscaled: 1.4201 (1.5841)  loss_bbox_1_unscaled: 0.3465 (0.3352)  loss_giou_1_unscaled: 1.6679 (1.6371)  cardinality_error_1_unscaled: 79.0000 (78.8000)  loss_ce_2_unscaled: 1.4745 (1.7079)  loss_bbox_2_unscaled: 0.3041 (0.3176)  loss_giou_2_unscaled: 1.6275 (1.6434)  cardinality_error_2_unscaled: 79.0000 (78.8000)  loss_ce_3_unscaled: 1.4206 (1.6264)  loss_bbox_3_unscaled: 0.3735 (0.3923)  loss_giou_3_unscaled: 1.7123 (1.7281)  cardinality_error_3_unscaled: 79.0000 (78.8000)  loss_ce_4_unscaled: 1.4427 (1.6325)  loss_bbox_4_unscaled: 0.2847 (0.2888)  loss_giou_4_unscaled: 1.5927 (1.5889)  cardinality_error_4_unscaled: 79.0000 (78.8000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "loss_dict {'loss_ce': tensor(1.3432, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(50., device='cuda:0'), 'loss_bbox': tensor(0.1914, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9343, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(61.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2350, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1542, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8426, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(38., device='cuda:0'), 'loss_ce_1': tensor(1.2394, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1724, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9262, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(40.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2921, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1524, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8325, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(77.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2964, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1630, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8811, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(74.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3064, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1624, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8621, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(40., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.1176, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [3]  [  0/100]  eta: 0:00:49  lr: 0.000100  class_error: 50.00  loss: 23.2485 (23.2485)  loss_ce: 1.3432 (1.3432)  loss_bbox: 0.9570 (0.9570)  loss_giou: 1.8685 (1.8685)  loss_ce_0: 1.2350 (1.2350)  loss_bbox_0: 0.7710 (0.7710)  loss_giou_0: 1.6852 (1.6852)  loss_ce_1: 1.2394 (1.2394)  loss_bbox_1: 0.8619 (0.8619)  loss_giou_1: 1.8524 (1.8524)  loss_ce_2: 1.2921 (1.2921)  loss_bbox_2: 0.7621 (0.7621)  loss_giou_2: 1.6650 (1.6650)  loss_ce_3: 1.2964 (1.2964)  loss_bbox_3: 0.8149 (0.8149)  loss_giou_3: 1.7621 (1.7621)  loss_ce_4: 1.3064 (1.3064)  loss_bbox_4: 0.8118 (0.8118)  loss_giou_4: 1.7241 (1.7241)  loss_ce_unscaled: 1.3432 (1.3432)  class_error_unscaled: 50.0000 (50.0000)  loss_bbox_unscaled: 0.1914 (0.1914)  loss_giou_unscaled: 0.9343 (0.9343)  cardinality_error_unscaled: 61.5000 (61.5000)  loss_ce_0_unscaled: 1.2350 (1.2350)  loss_bbox_0_unscaled: 0.1542 (0.1542)  loss_giou_0_unscaled: 0.8426 (0.8426)  cardinality_error_0_unscaled: 38.0000 (38.0000)  loss_ce_1_unscaled: 1.2394 (1.2394)  loss_bbox_1_unscaled: 0.1724 (0.1724)  loss_giou_1_unscaled: 0.9262 (0.9262)  cardinality_error_1_unscaled: 40.5000 (40.5000)  loss_ce_2_unscaled: 1.2921 (1.2921)  loss_bbox_2_unscaled: 0.1524 (0.1524)  loss_giou_2_unscaled: 0.8325 (0.8325)  cardinality_error_2_unscaled: 77.5000 (77.5000)  loss_ce_3_unscaled: 1.2964 (1.2964)  loss_bbox_3_unscaled: 0.1630 (0.1630)  loss_giou_3_unscaled: 0.8811 (0.8811)  cardinality_error_3_unscaled: 74.5000 (74.5000)  loss_ce_4_unscaled: 1.3064 (1.3064)  loss_bbox_4_unscaled: 0.1624 (0.1624)  loss_giou_4_unscaled: 0.8621 (0.8621)  cardinality_error_4_unscaled: 40.0000 (40.0000)  time: 0.4945  data: 0.1391  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.1330, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(53.8462, device='cuda:0'), 'loss_bbox': tensor(0.1242, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8802, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(56.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0912, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1643, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0057, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(34.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0301, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1653, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9835, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(51., device='cuda:0'), 'loss_ce_2': tensor(1.1113, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1290, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8615, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(84.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1168, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1293, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8612, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(81.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1017, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1341, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9386, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(31.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.7093, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6489, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(72.7273, device='cuda:0'), 'loss_bbox': tensor(0.1160, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8726, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(28., device='cuda:0'), 'loss_ce_0': tensor(1.5297, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1093, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8272, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(16.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5065, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1059, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8205, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(41., device='cuda:0'), 'loss_ce_2': tensor(1.5545, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1190, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8613, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(70., device='cuda:0'), 'loss_ce_3': tensor(1.5657, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1306, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9345, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(59., device='cuda:0'), 'loss_ce_4': tensor(1.5571, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1031, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8014, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(10., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.7970, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9310, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75., device='cuda:0'), 'loss_bbox': tensor(0.1645, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8739, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(25.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9280, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1803, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8702, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(30., device='cuda:0'), 'loss_ce_1': tensor(0.8979, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1655, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8249, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(55.5000, device='cuda:0'), 'loss_ce_2': tensor(0.8463, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1848, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9625, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(81.5000, device='cuda:0'), 'loss_ce_3': tensor(0.8959, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1931, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9993, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(68., device='cuda:0'), 'loss_ce_4': tensor(0.9422, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1745, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9030, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(24.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.5183, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0401, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(76., device='cuda:0'), 'loss_bbox': tensor(0.1970, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9089, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(29.5000, device='cuda:0'), 'loss_ce_0': tensor(0.8926, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2032, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8898, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(28.5000, device='cuda:0'), 'loss_ce_1': tensor(0.9951, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1991, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8991, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(65., device='cuda:0'), 'loss_ce_2': tensor(0.9971, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1903, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8995, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(83., device='cuda:0'), 'loss_ce_3': tensor(1.0152, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1948, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8949, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(71.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9892, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1983, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9157, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(29., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.4868, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3503, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(78.2609, device='cuda:0'), 'loss_bbox': tensor(0.1232, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8472, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(31., device='cuda:0'), 'loss_ce_0': tensor(1.3720, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1295, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8184, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(33., device='cuda:0'), 'loss_ce_1': tensor(1.3901, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1280, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8335, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(70.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5149, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1153, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8182, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(84.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3735, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1239, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8436, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(71., device='cuda:0'), 'loss_ce_4': tensor(1.3108, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1324, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9143, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(30.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.1017, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2744, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(56.7568, device='cuda:0'), 'loss_bbox': tensor(0.1951, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1382, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(27., device='cuda:0'), 'loss_ce_0': tensor(1.2475, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1890, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1250, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(32.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2500, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1757, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1185, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(68., device='cuda:0'), 'loss_ce_2': tensor(1.2722, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1772, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1475, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(77., device='cuda:0'), 'loss_ce_3': tensor(1.2860, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1711, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1622, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(53., device='cuda:0'), 'loss_ce_4': tensor(1.3237, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2118, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2906, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(27.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.0051, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9583, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(35.2941, device='cuda:0'), 'loss_bbox': tensor(0.1623, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9059, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(58., device='cuda:0'), 'loss_ce_0': tensor(0.8603, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1844, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9465, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(32.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0037, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1676, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8941, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(75., device='cuda:0'), 'loss_ce_2': tensor(1.0352, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1676, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8845, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(84.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9714, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1758, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9149, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(59., device='cuda:0'), 'loss_ce_4': tensor(1.0279, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2042, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0149, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(50.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.0981, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5755, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(70.3704, device='cuda:0'), 'loss_bbox': tensor(0.1160, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7382, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(55.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5839, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1424, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8525, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(28., device='cuda:0'), 'loss_ce_1': tensor(1.6050, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1147, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7745, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(61.5000, device='cuda:0'), 'loss_ce_2': tensor(1.7678, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1073, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7128, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(77.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6073, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1240, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8193, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(46.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6373, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2282, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1676, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(42.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.8732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1298, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(47.0588, device='cuda:0'), 'loss_bbox': tensor(0.1425, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8400, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(38., device='cuda:0'), 'loss_ce_0': tensor(1.1718, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1295, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8550, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(17.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0860, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1218, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8319, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(50., device='cuda:0'), 'loss_ce_2': tensor(1.0969, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1309, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8632, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(55., device='cuda:0'), 'loss_ce_3': tensor(1.1160, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1187, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8210, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(17., device='cuda:0'), 'loss_ce_4': tensor(1.1345, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1757, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0602, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(28., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.2340, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0863, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(45.7143, device='cuda:0'), 'loss_bbox': tensor(0.1636, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8940, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(63., device='cuda:0'), 'loss_ce_0': tensor(1.0211, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1570, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9295, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(17.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0307, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1458, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8829, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(54.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0337, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1549, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9047, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(65., device='cuda:0'), 'loss_ce_3': tensor(1.0445, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1459, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8672, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(38., device='cuda:0'), 'loss_ce_4': tensor(1.0745, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1753, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0583, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(45., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.8605, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [3]  [ 10/100]  eta: 0:00:26  lr: 0.000100  class_error: 45.71  loss: 22.2872 (22.8792)  loss_ce: 1.1330 (1.2246)  loss_bbox: 0.8116 (0.7709)  loss_giou: 1.7604 (1.7879)  loss_ce_0: 1.1718 (1.1757)  loss_bbox_0: 0.7851 (0.7923)  loss_giou_0: 1.7404 (1.8113)  loss_ce_1: 1.0860 (1.1850)  loss_bbox_1: 0.8265 (0.7553)  loss_giou_1: 1.7659 (1.7799)  loss_ce_2: 1.1113 (1.2293)  loss_bbox_2: 0.7621 (0.7403)  loss_giou_2: 1.7265 (1.7724)  loss_ce_3: 1.1168 (1.2081)  loss_bbox_3: 0.7297 (0.7592)  loss_giou_3: 1.7621 (1.8180)  loss_ce_4: 1.1345 (1.2187)  loss_bbox_4: 0.8767 (0.8637)  loss_giou_4: 1.8772 (1.9867)  loss_ce_unscaled: 1.1330 (1.2246)  class_error_unscaled: 56.7568 (60.0935)  loss_bbox_unscaled: 0.1623 (0.1542)  loss_giou_unscaled: 0.8802 (0.8939)  cardinality_error_unscaled: 38.0000 (43.0455)  loss_ce_0_unscaled: 1.1718 (1.1757)  loss_bbox_0_unscaled: 0.1570 (0.1585)  loss_giou_0_unscaled: 0.8702 (0.9057)  cardinality_error_0_unscaled: 30.0000 (28.0455)  loss_ce_1_unscaled: 1.0860 (1.1850)  loss_bbox_1_unscaled: 0.1653 (0.1511)  loss_giou_1_unscaled: 0.8829 (0.8900)  cardinality_error_1_unscaled: 55.5000 (57.5000)  loss_ce_2_unscaled: 1.1113 (1.2293)  loss_bbox_2_unscaled: 0.1524 (0.1481)  loss_giou_2_unscaled: 0.8632 (0.8862)  cardinality_error_2_unscaled: 77.5000 (76.3636)  loss_ce_3_unscaled: 1.1168 (1.2081)  loss_bbox_3_unscaled: 0.1459 (0.1518)  loss_giou_3_unscaled: 0.8811 (0.9090)  cardinality_error_3_unscaled: 59.0000 (58.0909)  loss_ce_4_unscaled: 1.1345 (1.2187)  loss_bbox_4_unscaled: 0.1753 (0.1727)  loss_giou_4_unscaled: 0.9386 (0.9933)  cardinality_error_4_unscaled: 30.5000 (32.6364)  time: 0.2920  data: 0.0211  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.2696, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(36.3636, device='cuda:0'), 'loss_bbox': tensor(0.1464, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9109, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(65.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3300, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1990, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1270, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(18., device='cuda:0'), 'loss_ce_1': tensor(1.3039, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1451, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9467, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(50.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3804, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1405, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9241, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(56., device='cuda:0'), 'loss_ce_3': tensor(1.3449, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1349, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9318, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(39.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3591, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2159, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1591, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(52., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.7131, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.7531, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(67.8571, device='cuda:0'), 'loss_bbox': tensor(0.1658, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8495, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(81.5000, device='cuda:0'), 'loss_ce_0': tensor(1.6405, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1662, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9047, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(41.5000, device='cuda:0'), 'loss_ce_1': tensor(1.6466, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1576, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8811, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(70., device='cuda:0'), 'loss_ce_2': tensor(1.7545, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1746, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9275, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(77.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6963, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1606, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8843, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(63.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6717, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1612, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0365, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(75., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.9013, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0797, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(14.2857, device='cuda:0'), 'loss_bbox': tensor(0.1325, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0094, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(87.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9333, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1662, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1485, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(41.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0244, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1363, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0043, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(74., device='cuda:0'), 'loss_ce_2': tensor(1.0199, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1426, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0506, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(76.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9958, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1359, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0698, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(76., device='cuda:0'), 'loss_ce_4': tensor(1.1010, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1456, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0885, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(84.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.0501, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2171, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(25., device='cuda:0'), 'loss_bbox': tensor(0.1183, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8280, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(83., device='cuda:0'), 'loss_ce_0': tensor(1.1138, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1298, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8664, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(42., device='cuda:0'), 'loss_ce_1': tensor(1.2517, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1244, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8093, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(72., device='cuda:0'), 'loss_ce_2': tensor(1.2363, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1273, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8380, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(71., device='cuda:0'), 'loss_ce_3': tensor(1.2108, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1118, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7939, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(75.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2176, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1205, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9172, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(81., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.8354, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.9547, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(57.8947, device='cuda:0'), 'loss_bbox': tensor(0.1266, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8534, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(90.5000, device='cuda:0'), 'loss_ce_0': tensor(1.6629, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1179, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7781, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(59., device='cuda:0'), 'loss_ce_1': tensor(1.7744, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1250, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8045, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(79., device='cuda:0'), 'loss_ce_2': tensor(1.7989, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1503, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9500, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(81.5000, device='cuda:0'), 'loss_ce_3': tensor(1.8085, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1164, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7771, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(81., device='cuda:0'), 'loss_ce_4': tensor(1.8080, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1083, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7251, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(89., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.1347, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6262, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(67.8571, device='cuda:0'), 'loss_bbox': tensor(0.1745, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0554, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(81., device='cuda:0'), 'loss_ce_0': tensor(1.5101, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1349, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8787, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(45., device='cuda:0'), 'loss_ce_1': tensor(1.5433, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1430, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9041, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(60.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4905, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2768, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2154, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(63., device='cuda:0'), 'loss_ce_3': tensor(1.5074, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1571, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9711, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(63., device='cuda:0'), 'loss_ce_4': tensor(1.4867, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1566, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9878, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(81., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.1965, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.8965, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(66.6667, device='cuda:0'), 'loss_bbox': tensor(0.1361, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0410, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(78.5000, device='cuda:0'), 'loss_ce_0': tensor(1.7228, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1108, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7827, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(53., device='cuda:0'), 'loss_ce_1': tensor(1.7625, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1105, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7597, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(61., device='cuda:0'), 'loss_ce_2': tensor(1.6737, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2200, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2483, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(50., device='cuda:0'), 'loss_ce_3': tensor(1.7607, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1291, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0197, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(56., device='cuda:0'), 'loss_ce_4': tensor(1.8002, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0934, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7498, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(73., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.6577, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1602, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(46.1538, device='cuda:0'), 'loss_bbox': tensor(0.1342, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0766, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(47.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1958, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1201, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8744, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(28., device='cuda:0'), 'loss_ce_1': tensor(1.1953, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1290, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8933, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(34.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2030, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3365, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4451, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(16.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1426, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2755, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3763, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(29., device='cuda:0'), 'loss_ce_4': tensor(1.2452, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1285, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0113, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(33.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.9386, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2599, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(65.9091, device='cuda:0'), 'loss_bbox': tensor(0.1546, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0026, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(8.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1645, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1326, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8124, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(17.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2362, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1353, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8174, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(23.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2081, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4449, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4926, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(3.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1736, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3950, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4332, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(5., device='cuda:0'), 'loss_ce_4': tensor(1.2727, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1269, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7796, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(15., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.7323, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4890, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(80., device='cuda:0'), 'loss_bbox': tensor(0.1534, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0602, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(1.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4471, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1268, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8589, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(29.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5992, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1470, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9524, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(20., device='cuda:0'), 'loss_ce_2': tensor(1.4540, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.4587, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.5404, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(1.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4716, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.4133, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.5303, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(11., device='cuda:0'), 'loss_ce_4': tensor(1.4325, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1732, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0131, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(11., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.9730, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [3]  [ 20/100]  eta: 0:00:22  lr: 0.000100  class_error: 80.00  loss: 23.1918 (24.1228)  loss_ce: 1.2599 (1.3418)  loss_bbox: 0.7126 (0.7472)  loss_giou: 1.7880 (1.8591)  loss_ce_0: 1.1958 (1.2692)  loss_bbox_0: 0.6744 (0.7494)  loss_giou_0: 1.7404 (1.8090)  loss_ce_1: 1.2500 (1.3034)  loss_bbox_1: 0.6814 (0.7178)  loss_giou_1: 1.7622 (1.7678)  loss_ce_2: 1.2363 (1.3210)  loss_bbox_2: 0.7744 (0.9764)  loss_giou_2: 1.8481 (2.0362)  loss_ce_3: 1.2108 (1.3048)  loss_bbox_3: 0.6794 (0.8809)  loss_giou_3: 1.8297 (1.9797)  loss_ce_4: 1.2727 (1.3238)  loss_bbox_4: 0.7830 (0.7929)  loss_giou_4: 1.9757 (1.9424)  loss_ce_unscaled: 1.2599 (1.3418)  class_error_unscaled: 57.8947 (56.6198)  loss_bbox_unscaled: 0.1425 (0.1494)  loss_giou_unscaled: 0.8940 (0.9295)  cardinality_error_unscaled: 55.5000 (52.3095)  loss_ce_0_unscaled: 1.1958 (1.2692)  loss_bbox_0_unscaled: 0.1349 (0.1499)  loss_giou_0_unscaled: 0.8702 (0.9045)  cardinality_error_0_unscaled: 30.0000 (32.5476)  loss_ce_1_unscaled: 1.2500 (1.3034)  loss_bbox_1_unscaled: 0.1363 (0.1436)  loss_giou_1_unscaled: 0.8811 (0.8839)  cardinality_error_1_unscaled: 60.5000 (56.0714)  loss_ce_2_unscaled: 1.2363 (1.3210)  loss_bbox_2_unscaled: 0.1549 (0.1953)  loss_giou_2_unscaled: 0.9241 (1.0181)  cardinality_error_2_unscaled: 71.0000 (63.6667)  loss_ce_3_unscaled: 1.2108 (1.3048)  loss_bbox_3_unscaled: 0.1359 (0.1762)  loss_giou_3_unscaled: 0.9149 (0.9898)  cardinality_error_3_unscaled: 59.0000 (54.2143)  loss_ce_4_unscaled: 1.2727 (1.3238)  loss_bbox_4_unscaled: 0.1566 (0.1586)  loss_giou_4_unscaled: 0.9878 (0.9712)  cardinality_error_4_unscaled: 33.5000 (45.4286)  time: 0.2747  data: 0.0108  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.0648, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(76.9231, device='cuda:0'), 'loss_bbox': tensor(0.1448, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0667, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(6.5000, device='cuda:0'), 'loss_ce_0': tensor(0.8981, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1372, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9716, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(34.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0276, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1472, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9921, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(25.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0368, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3098, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.4452, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(6., device='cuda:0'), 'loss_ce_3': tensor(0.9874, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3911, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(14., device='cuda:0'), 'loss_ce_4': tensor(1.1166, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2153, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.2165, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(18., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.3479, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2732, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(70.5882, device='cuda:0'), 'loss_bbox': tensor(0.1415, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9158, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(5., device='cuda:0'), 'loss_ce_0': tensor(1.2540, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1558, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9080, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(30., device='cuda:0'), 'loss_ce_1': tensor(1.2341, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1400, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8799, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(20., device='cuda:0'), 'loss_ce_2': tensor(1.2769, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2211, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2356, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(3., device='cuda:0'), 'loss_ce_3': tensor(1.2135, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2240, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2116, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(11.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2701, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3390, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4544, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(17., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.6737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1531, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75., device='cuda:0'), 'loss_bbox': tensor(0.1747, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8435, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(19.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1878, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1761, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8865, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(40.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1657, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1730, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8638, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(26., device='cuda:0'), 'loss_ce_2': tensor(1.1107, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2696, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.2680, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(5.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1175, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2648, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2132, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(22., device='cuda:0'), 'loss_ce_4': tensor(1.2108, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2509, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0922, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(40.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.7138, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1773, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60.5263, device='cuda:0'), 'loss_bbox': tensor(0.1479, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9505, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(7., device='cuda:0'), 'loss_ce_0': tensor(0.9776, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1602, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0266, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(28.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1323, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1445, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8908, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(14.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1603, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1918, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1633, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(8., device='cuda:0'), 'loss_ce_3': tensor(1.0808, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2190, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1712, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(16.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0991, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.3284, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.4017, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(37.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.5555, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0177, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(48.1481, device='cuda:0'), 'loss_bbox': tensor(0.1443, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9469, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(23., device='cuda:0'), 'loss_ce_0': tensor(1.0040, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1406, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8122, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(26., device='cuda:0'), 'loss_ce_1': tensor(1.1120, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1344, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8087, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(16.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1608, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2396, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3745, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(13., device='cuda:0'), 'loss_ce_3': tensor(1.0163, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2754, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.4162, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(16., device='cuda:0'), 'loss_ce_4': tensor(1.0800, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1733, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0258, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(50.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.5586, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0555, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(25.5319, device='cuda:0'), 'loss_bbox': tensor(0.1797, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2221, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(44.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1165, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1575, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1196, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(41., device='cuda:0'), 'loss_ce_1': tensor(1.1800, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1491, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1421, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(34.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1067, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1882, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.3837, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(5.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0587, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2110, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.3740, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(32., device='cuda:0'), 'loss_ce_4': tensor(1.0759, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1682, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1624, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(70., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.5086, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3808, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(43.7500, device='cuda:0'), 'loss_bbox': tensor(0.1306, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0311, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(75.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3654, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1134, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8671, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(70.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3360, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1132, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8996, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(51.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3304, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1207, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0566, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(15., device='cuda:0'), 'loss_ce_3': tensor(1.3115, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1246, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9836, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(51.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3677, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1201, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8730, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(83.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.9654, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0841, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(15.5555, device='cuda:0'), 'loss_bbox': tensor(0.1047, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9608, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(72.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1558, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0940, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8593, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(65.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1094, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1308, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1525, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(54.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1657, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0918, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9027, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(27., device='cuda:0'), 'loss_ce_3': tensor(1.0755, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1064, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9035, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(53., device='cuda:0'), 'loss_ce_4': tensor(1.1132, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1065, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9381, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(77.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.1404, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.7966, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(58.6207, device='cuda:0'), 'loss_bbox': tensor(0.1271, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0386, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(70.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5507, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1146, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0210, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(64.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5481, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1135, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0712, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(57., device='cuda:0'), 'loss_ce_2': tensor(1.5016, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1039, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0437, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(39.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5181, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1117, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9830, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(59., device='cuda:0'), 'loss_ce_4': tensor(1.6343, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1187, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9988, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(71., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.1204, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4325, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(38.4615, device='cuda:0'), 'loss_bbox': tensor(0.2741, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8880, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(86.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2896, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2819, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9246, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(84., device='cuda:0'), 'loss_ce_1': tensor(1.2523, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2893, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9306, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(82.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2551, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2864, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9383, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(70.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3437, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2769, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8900, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(78.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4111, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2880, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9336, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(87., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.3788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [3]  [ 30/100]  eta: 0:00:19  lr: 0.000100  class_error: 38.46  loss: 25.8184 (24.5222)  loss_ce: 1.2599 (1.3101)  loss_bbox: 0.7215 (0.7593)  loss_giou: 1.9217 (1.8958)  loss_ce_0: 1.1958 (1.2404)  loss_bbox_0: 0.6744 (0.7547)  loss_giou_0: 1.7574 (1.8317)  loss_ce_1: 1.2362 (1.2732)  loss_bbox_1: 0.6814 (0.7339)  loss_giou_1: 1.7867 (1.8189)  loss_ce_2: 1.2363 (1.2854)  loss_bbox_2: 0.9591 (0.9877)  loss_giou_2: 2.3265 (2.1414)  loss_ce_3: 1.2108 (1.2621)  loss_bbox_3: 0.8028 (0.9372)  loss_giou_3: 2.0394 (2.0854)  loss_ce_4: 1.2701 (1.2961)  loss_bbox_4: 0.7830 (0.8772)  loss_giou_4: 2.0227 (2.0317)  loss_ce_unscaled: 1.2599 (1.3101)  class_error_unscaled: 57.8947 (54.9072)  loss_bbox_unscaled: 0.1443 (0.1519)  loss_giou_unscaled: 0.9608 (0.9479)  cardinality_error_unscaled: 65.5000 (48.6774)  loss_ce_0_unscaled: 1.1958 (1.2404)  loss_bbox_0_unscaled: 0.1349 (0.1509)  loss_giou_0_unscaled: 0.8787 (0.9158)  cardinality_error_0_unscaled: 41.0000 (37.6935)  loss_ce_1_unscaled: 1.2362 (1.2732)  loss_bbox_1_unscaled: 0.1363 (0.1468)  loss_giou_1_unscaled: 0.8933 (0.9095)  cardinality_error_1_unscaled: 50.5000 (50.3226)  loss_ce_2_unscaled: 1.2363 (1.2854)  loss_bbox_2_unscaled: 0.1918 (0.1975)  loss_giou_2_unscaled: 1.1633 (1.0707)  cardinality_error_2_unscaled: 16.5000 (49.3548)  loss_ce_3_unscaled: 1.2108 (1.2621)  loss_bbox_3_unscaled: 0.1606 (0.1874)  loss_giou_3_unscaled: 1.0197 (1.0427)  cardinality_error_3_unscaled: 39.5000 (48.1452)  loss_ce_4_unscaled: 1.2701 (1.2961)  loss_bbox_4_unscaled: 0.1566 (0.1754)  loss_giou_4_unscaled: 1.0113 (1.0158)  cardinality_error_4_unscaled: 70.0000 (48.5968)  time: 0.2753  data: 0.0117  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.1549, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(22.8571, device='cuda:0'), 'loss_bbox': tensor(0.1289, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7942, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(81.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1321, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1432, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9508, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(81., device='cuda:0'), 'loss_ce_1': tensor(1.1032, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1785, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1710, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(79., device='cuda:0'), 'loss_ce_2': tensor(1.1377, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1346, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8996, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(73.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1144, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1370, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8626, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(78., device='cuda:0'), 'loss_ce_4': tensor(1.1802, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1379, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8586, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(82.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.0931, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9325, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(9.0909, device='cuda:0'), 'loss_bbox': tensor(0.1210, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8742, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(82.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9864, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0988, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7939, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(82.5000, device='cuda:0'), 'loss_ce_1': tensor(0.8959, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1754, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1325, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(81.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9980, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1008, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8339, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(72.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9587, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1049, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8118, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(76., device='cuda:0'), 'loss_ce_4': tensor(0.9819, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1071, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7875, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(83.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.5594, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(2.0734, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(71.4286, device='cuda:0'), 'loss_bbox': tensor(0.1436, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0339, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(68.5000, device='cuda:0'), 'loss_ce_0': tensor(1.7307, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0935, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8983, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(68.5000, device='cuda:0'), 'loss_ce_1': tensor(1.8319, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3238, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4348, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(64., device='cuda:0'), 'loss_ce_2': tensor(1.7391, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1028, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9141, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(59.5000, device='cuda:0'), 'loss_ce_3': tensor(1.7849, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1097, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8956, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(63.5000, device='cuda:0'), 'loss_ce_4': tensor(1.9695, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1140, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9032, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(68.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.5784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6707, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60., device='cuda:0'), 'loss_bbox': tensor(0.3118, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9187, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(92.5000, device='cuda:0'), 'loss_ce_0': tensor(1.6333, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.3322, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9023, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(94.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5616, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3602, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0338, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(89.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5341, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.3045, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8709, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(88., device='cuda:0'), 'loss_ce_3': tensor(1.5969, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.3013, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8684, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(89., device='cuda:0'), 'loss_ce_4': tensor(1.7507, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2987, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8599, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(95., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(32.9802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(2.1886, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(93.9394, device='cuda:0'), 'loss_bbox': tensor(0.1799, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0250, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(53., device='cuda:0'), 'loss_ce_0': tensor(2.1553, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1832, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0583, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(66.5000, device='cuda:0'), 'loss_ce_1': tensor(2.2270, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3288, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2894, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(61., device='cuda:0'), 'loss_ce_2': tensor(2.1369, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1909, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0514, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(55.5000, device='cuda:0'), 'loss_ce_3': tensor(2.1700, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1690, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9497, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(53., device='cuda:0'), 'loss_ce_4': tensor(2.2507, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1568, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8839, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(65.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(34.4801, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2074, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(58.3333, device='cuda:0'), 'loss_bbox': tensor(0.1722, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8822, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30., device='cuda:0'), 'loss_ce_0': tensor(1.3701, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1999, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0967, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(85., device='cuda:0'), 'loss_ce_1': tensor(1.3125, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2232, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0961, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(75.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3157, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1676, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9231, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(67.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3109, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1724, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9425, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62., device='cuda:0'), 'loss_ce_4': tensor(1.2319, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1825, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8871, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(77., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.8241, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2116, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(90., device='cuda:0'), 'loss_bbox': tensor(0.1329, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7982, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(8.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3536, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1617, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9238, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(82.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3760, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2210, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0676, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(67.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3146, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1633, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8069, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(57., device='cuda:0'), 'loss_ce_3': tensor(1.2423, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1436, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7336, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(31., device='cuda:0'), 'loss_ce_4': tensor(1.2941, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1476, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7808, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(51., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.6614, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2771, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(91.4286, device='cuda:0'), 'loss_bbox': tensor(0.1314, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8346, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(8.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5354, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1256, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8453, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(60., device='cuda:0'), 'loss_ce_1': tensor(1.5148, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3550, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3846, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(42., device='cuda:0'), 'loss_ce_2': tensor(1.5072, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1766, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0588, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(22., device='cuda:0'), 'loss_ce_3': tensor(1.4691, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1598, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9813, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(6.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3306, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1323, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8290, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(11., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.6726, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1786, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(86.7924, device='cuda:0'), 'loss_bbox': tensor(0.1653, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9540, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(15., device='cuda:0'), 'loss_ce_0': tensor(1.4876, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1853, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1412, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(21.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5264, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2711, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2757, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(12.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4549, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1746, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0062, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(14.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3941, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1767, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9693, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(17., device='cuda:0'), 'loss_ce_4': tensor(1.2805, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1699, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9509, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(15.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.4799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.7344, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(88.8889, device='cuda:0'), 'loss_bbox': tensor(0.1566, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8946, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(29.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4159, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1481, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8759, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(24.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2896, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3438, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3255, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(11., device='cuda:0'), 'loss_ce_2': tensor(1.4053, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1676, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9519, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(10., device='cuda:0'), 'loss_ce_3': tensor(1.4711, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1699, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9168, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(8., device='cuda:0'), 'loss_ce_4': tensor(1.5396, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1574, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8529, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(12., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.0362, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [3]  [ 40/100]  eta: 0:00:16  lr: 0.000100  class_error: 88.89  loss: 25.8265 (24.8381)  loss_ce: 1.2074 (1.3474)  loss_bbox: 0.7215 (0.7746)  loss_giou: 1.8374 (1.8729)  loss_ce_0: 1.2896 (1.2989)  loss_bbox_0: 0.7407 (0.7744)  loss_giou_0: 1.8160 (1.8477)  loss_ce_1: 1.2523 (1.3197)  loss_bbox_1: 0.8772 (0.8940)  loss_giou_1: 2.1424 (1.9710)  loss_ce_2: 1.2769 (1.3266)  loss_bbox_2: 0.8728 (0.9521)  loss_giou_2: 2.0124 (2.0736)  loss_ce_3: 1.2423 (1.3082)  loss_bbox_3: 0.8493 (0.9092)  loss_giou_3: 1.8994 (2.0125)  loss_ce_4: 1.2701 (1.3412)  loss_bbox_4: 0.7871 (0.8589)  loss_giou_4: 1.8064 (1.9554)  loss_ce_unscaled: 1.2074 (1.3474)  class_error_unscaled: 60.0000 (57.9239)  loss_bbox_unscaled: 0.1443 (0.1549)  loss_giou_unscaled: 0.9187 (0.9364)  cardinality_error_unscaled: 30.0000 (48.2561)  loss_ce_0_unscaled: 1.2896 (1.2989)  loss_bbox_0_unscaled: 0.1481 (0.1549)  loss_giou_0_unscaled: 0.9080 (0.9238)  cardinality_error_0_unscaled: 64.5000 (44.7561)  loss_ce_1_unscaled: 1.2523 (1.3197)  loss_bbox_1_unscaled: 0.1754 (0.1788)  loss_giou_1_unscaled: 1.0712 (0.9855)  cardinality_error_1_unscaled: 51.5000 (52.2805)  loss_ce_2_unscaled: 1.2769 (1.3266)  loss_bbox_2_unscaled: 0.1746 (0.1904)  loss_giou_2_unscaled: 1.0062 (1.0368)  cardinality_error_2_unscaled: 22.0000 (50.0000)  loss_ce_3_unscaled: 1.2423 (1.3082)  loss_bbox_3_unscaled: 0.1699 (0.1818)  loss_giou_3_unscaled: 0.9497 (1.0062)  cardinality_error_3_unscaled: 32.0000 (48.2073)  loss_ce_4_unscaled: 1.2701 (1.3412)  loss_bbox_4_unscaled: 0.1574 (0.1718)  loss_giou_4_unscaled: 0.9032 (0.9777)  cardinality_error_4_unscaled: 65.5000 (50.4390)  time: 0.2729  data: 0.0106  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.6698, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(87.0968, device='cuda:0'), 'loss_bbox': tensor(0.1443, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0193, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(36.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3257, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1481, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1217, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(20.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3181, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2035, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1849, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(15., device='cuda:0'), 'loss_ce_2': tensor(1.4446, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1290, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9149, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(6., device='cuda:0'), 'loss_ce_3': tensor(1.4701, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1290, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9288, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(4.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4869, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1382, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9642, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(12.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.3375, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.9797, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(88.3721, device='cuda:0'), 'loss_bbox': tensor(0.1167, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8492, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(40., device='cuda:0'), 'loss_ce_0': tensor(1.4744, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1049, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8475, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(4.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3834, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1891, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1537, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(19., device='cuda:0'), 'loss_ce_2': tensor(1.4634, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1217, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8263, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(10.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5799, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1115, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8410, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(7.5000, device='cuda:0'), 'loss_ce_4': tensor(1.7393, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1164, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8529, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(15., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.9503, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(2.2620, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(95.7447, device='cuda:0'), 'loss_bbox': tensor(0.1543, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9331, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(50.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5777, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1768, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0505, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(9.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5641, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2040, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1550, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(15., device='cuda:0'), 'loss_ce_2': tensor(1.5904, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1376, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8076, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(9.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6472, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1410, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8567, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(4.5000, device='cuda:0'), 'loss_ce_4': tensor(1.9070, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1502, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9273, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(14.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.6764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.9432, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(90.6977, device='cuda:0'), 'loss_bbox': tensor(0.1354, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9491, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(44., device='cuda:0'), 'loss_ce_0': tensor(1.3912, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1939, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1037, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(5., device='cuda:0'), 'loss_ce_1': tensor(1.4259, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1361, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9112, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(21., device='cuda:0'), 'loss_ce_2': tensor(1.4383, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1184, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8081, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(7.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3840, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1304, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8941, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(11.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6688, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1285, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8582, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(20., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.3876, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3683, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(64.5161, device='cuda:0'), 'loss_bbox': tensor(0.1440, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9263, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(48., device='cuda:0'), 'loss_ce_0': tensor(1.2770, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1487, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0093, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(1.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2845, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1463, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0014, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(37., device='cuda:0'), 'loss_ce_2': tensor(1.2837, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1560, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9481, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(21.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2678, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1459, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9135, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(19., device='cuda:0'), 'loss_ce_4': tensor(1.2717, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1400, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9426, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(19., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.4052, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4367, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(89.4737, device='cuda:0'), 'loss_bbox': tensor(0.1224, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8952, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(42.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1856, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1344, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9223, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(4., device='cuda:0'), 'loss_ce_1': tensor(1.2676, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1167, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8620, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(39.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2127, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1335, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9455, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(27.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1215, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1231, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9092, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(29.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2942, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1165, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8756, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(25., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.8956, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5113, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60.7843, device='cuda:0'), 'loss_bbox': tensor(0.1288, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8548, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(22.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5390, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1439, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9655, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(17., device='cuda:0'), 'loss_ce_1': tensor(1.4467, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1479, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8982, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(22., device='cuda:0'), 'loss_ce_2': tensor(1.5206, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1482, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8645, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(17.5000, device='cuda:0'), 'loss_ce_3': tensor(1.5791, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1350, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8688, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(18., device='cuda:0'), 'loss_ce_4': tensor(1.5030, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1328, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8723, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(14.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.7067, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3869, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(83.3333, device='cuda:0'), 'loss_bbox': tensor(0.1446, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8707, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(32., device='cuda:0'), 'loss_ce_0': tensor(1.2746, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1465, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9152, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(2.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3054, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1413, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9602, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(38.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2352, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1475, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8529, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(37., device='cuda:0'), 'loss_ce_3': tensor(1.2076, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1389, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8792, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(49., device='cuda:0'), 'loss_ce_4': tensor(1.3106, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1435, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9004, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(20., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.6613, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1541, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(51.2195, device='cuda:0'), 'loss_bbox': tensor(0.1629, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1004, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(40., device='cuda:0'), 'loss_ce_0': tensor(1.3892, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1642, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1992, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(9.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3297, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1348, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9896, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(28.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1323, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1689, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0882, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(43.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0470, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1511, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1054, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(55.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2218, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1365, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9610, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(20.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.5825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0780, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(23.8095, device='cuda:0'), 'loss_bbox': tensor(0.1481, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9521, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(57., device='cuda:0'), 'loss_ce_0': tensor(1.0479, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1582, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9333, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(5.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1370, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1494, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9199, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(43., device='cuda:0'), 'loss_ce_2': tensor(1.0846, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1566, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9651, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(67., device='cuda:0'), 'loss_ce_3': tensor(1.1259, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1464, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9176, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(81., device='cuda:0'), 'loss_ce_4': tensor(1.0707, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1367, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9166, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(42., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.0157, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [3]  [ 50/100]  eta: 0:00:13  lr: 0.000100  class_error: 23.81  loss: 24.5138 (24.6808)  loss_ce: 1.3869 (1.3928)  loss_bbox: 0.7201 (0.7601)  loss_giou: 1.7905 (1.8723)  loss_ce_0: 1.3892 (1.3085)  loss_bbox_0: 0.7407 (0.7716)  loss_giou_0: 1.8666 (1.8802)  loss_ce_1: 1.3297 (1.3249)  loss_bbox_1: 0.9456 (0.8725)  loss_giou_1: 2.1922 (1.9781)  loss_ce_2: 1.4053 (1.3293)  loss_bbox_2: 0.7412 (0.9044)  loss_giou_2: 1.8282 (2.0208)  loss_ce_3: 1.3840 (1.3150)  loss_bbox_3: 0.7050 (0.8635)  loss_giou_3: 1.7912 (1.9753)  loss_ce_4: 1.3106 (1.3620)  loss_bbox_4: 0.6897 (0.8218)  loss_giou_4: 1.7512 (1.9277)  loss_ce_unscaled: 1.3869 (1.3928)  class_error_unscaled: 83.3333 (60.9790)  loss_bbox_unscaled: 0.1440 (0.1520)  loss_giou_unscaled: 0.8952 (0.9362)  cardinality_error_unscaled: 40.0000 (46.8922)  loss_ce_0_unscaled: 1.3892 (1.3085)  loss_bbox_0_unscaled: 0.1481 (0.1543)  loss_giou_0_unscaled: 0.9333 (0.9401)  cardinality_error_0_unscaled: 20.5000 (37.5392)  loss_ce_1_unscaled: 1.3297 (1.3249)  loss_bbox_1_unscaled: 0.1891 (0.1745)  loss_giou_1_unscaled: 1.0961 (0.9890)  cardinality_error_1_unscaled: 38.5000 (47.4902)  loss_ce_2_unscaled: 1.4053 (1.3293)  loss_bbox_2_unscaled: 0.1482 (0.1809)  loss_giou_2_unscaled: 0.9141 (1.0104)  cardinality_error_2_unscaled: 27.5000 (45.0490)  loss_ce_3_unscaled: 1.3840 (1.3150)  loss_bbox_3_unscaled: 0.1410 (0.1727)  loss_giou_3_unscaled: 0.8956 (0.9876)  cardinality_error_3_unscaled: 29.5000 (44.2451)  loss_ce_4_unscaled: 1.3106 (1.3620)  loss_bbox_4_unscaled: 0.1379 (0.1644)  loss_giou_4_unscaled: 0.8756 (0.9638)  cardinality_error_4_unscaled: 20.0000 (44.5294)  time: 0.2749  data: 0.0119  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.3018, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(26.1905, device='cuda:0'), 'loss_bbox': tensor(0.1663, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9194, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(63.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4988, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2186, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1441, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(12., device='cuda:0'), 'loss_ce_1': tensor(1.4408, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2080, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1075, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(38., device='cuda:0'), 'loss_ce_2': tensor(1.3091, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1759, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9090, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(64., device='cuda:0'), 'loss_ce_3': tensor(1.3006, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1811, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9760, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(75., device='cuda:0'), 'loss_ce_4': tensor(1.4351, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1593, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8662, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(47., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.4892, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1445, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(38.0952, device='cuda:0'), 'loss_bbox': tensor(0.1482, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0172, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(82., device='cuda:0'), 'loss_ce_0': tensor(1.1771, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1540, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9410, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(6.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2186, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1659, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9283, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58., device='cuda:0'), 'loss_ce_2': tensor(1.1426, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1750, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9866, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(82., device='cuda:0'), 'loss_ce_3': tensor(1.1294, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1587, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0131, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(85.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1213, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1496, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0691, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(78.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.4008, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1560, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(36.3636, device='cuda:0'), 'loss_bbox': tensor(0.1485, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7879, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(85.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1377, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1446, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8076, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(3.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1679, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1545, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8681, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(57.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1982, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1527, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8328, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(83., device='cuda:0'), 'loss_ce_3': tensor(1.1686, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1786, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9504, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(87., device='cuda:0'), 'loss_ce_4': tensor(1.1617, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1518, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8273, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(82., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.6007, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9245, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(0., device='cuda:0'), 'loss_bbox': tensor(0.1739, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0250, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(88., device='cuda:0'), 'loss_ce_0': tensor(0.9836, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1563, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8077, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(5.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0634, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1613, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9927, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(60.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9961, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1621, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8138, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(84., device='cuda:0'), 'loss_ce_3': tensor(0.9329, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1838, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9919, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(87.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9626, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1522, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7924, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(83.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.5090, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6510, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(62.7907, device='cuda:0'), 'loss_bbox': tensor(0.2176, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1692, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(76., device='cuda:0'), 'loss_ce_0': tensor(1.5101, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1659, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0310, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(8., device='cuda:0'), 'loss_ce_1': tensor(1.6175, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1902, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0836, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(57.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5631, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1755, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0528, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(72.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6071, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2453, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2782, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(74.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5309, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0924, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7907, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(71., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.4389, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2542, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(35.2941, device='cuda:0'), 'loss_bbox': tensor(0.1736, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0778, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(79.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3622, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1325, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9336, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(7., device='cuda:0'), 'loss_ce_1': tensor(1.3133, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1407, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9828, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(55.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2952, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1455, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8967, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(75.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2615, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1452, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9687, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(78., device='cuda:0'), 'loss_ce_4': tensor(1.2810, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1276, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9460, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(73., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.5060, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3359, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(45.4545, device='cuda:0'), 'loss_bbox': tensor(0.1400, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8976, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(82., device='cuda:0'), 'loss_ce_0': tensor(1.1959, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1464, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9237, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(6., device='cuda:0'), 'loss_ce_1': tensor(1.2578, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1426, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8886, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48., device='cuda:0'), 'loss_ce_2': tensor(1.2706, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1529, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8669, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(77.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2632, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1399, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8749, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(77., device='cuda:0'), 'loss_ce_4': tensor(1.2754, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1412, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8967, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(79., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.4305, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.8380, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75., device='cuda:0'), 'loss_bbox': tensor(0.1677, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1479, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(73., device='cuda:0'), 'loss_ce_0': tensor(1.7819, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1029, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8080, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(9.5000, device='cuda:0'), 'loss_ce_1': tensor(1.8639, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0944, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7898, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(37., device='cuda:0'), 'loss_ce_2': tensor(1.8617, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0950, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7563, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(68.5000, device='cuda:0'), 'loss_ce_3': tensor(1.8511, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1000, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7834, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62., device='cuda:0'), 'loss_ce_4': tensor(1.8744, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0912, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7375, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(67.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.2999, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4003, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(57.8947, device='cuda:0'), 'loss_bbox': tensor(0.3114, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.4010, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(64.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3650, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1305, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8734, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(12., device='cuda:0'), 'loss_ce_1': tensor(1.4847, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1188, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8670, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(33., device='cuda:0'), 'loss_ce_2': tensor(1.3646, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1301, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8601, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(54.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3871, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1487, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9555, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(47., device='cuda:0'), 'loss_ce_4': tensor(1.4171, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1523, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0219, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(38.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.1126, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.5681, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(85.7143, device='cuda:0'), 'loss_bbox': tensor(0.1922, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3419, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(63.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4041, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0774, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7032, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(25., device='cuda:0'), 'loss_ce_1': tensor(1.5087, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0730, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.6822, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(39.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5555, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0729, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.6673, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(55., device='cuda:0'), 'loss_ce_3': tensor(1.4931, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0793, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7936, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(45.5000, device='cuda:0'), 'loss_ce_4': tensor(1.5144, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.0714, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7414, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(46., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.6354, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [3]  [ 60/100]  eta: 0:00:11  lr: 0.000100  class_error: 85.71  loss: 23.7039 (24.5399)  loss_ce: 1.3869 (1.3870)  loss_bbox: 0.7410 (0.7863)  loss_giou: 1.8983 (1.9190)  loss_ce_0: 1.3622 (1.3140)  loss_bbox_0: 0.7325 (0.7622)  loss_giou_0: 1.8666 (1.8662)  loss_ce_1: 1.3181 (1.3362)  loss_bbox_1: 0.7314 (0.8483)  loss_giou_1: 1.8566 (1.9551)  loss_ce_2: 1.2952 (1.3336)  loss_bbox_2: 0.7377 (0.8740)  loss_giou_2: 1.7290 (1.9729)  loss_ce_3: 1.2678 (1.3190)  loss_bbox_3: 0.7050 (0.8498)  loss_giou_3: 1.8269 (1.9658)  loss_ce_4: 1.3106 (1.3613)  loss_bbox_4: 0.6834 (0.7927)  loss_giou_4: 1.7512 (1.8966)  loss_ce_unscaled: 1.3869 (1.3870)  class_error_unscaled: 60.7843 (58.5693)  loss_bbox_unscaled: 0.1482 (0.1573)  loss_giou_unscaled: 0.9491 (0.9595)  cardinality_error_unscaled: 57.0000 (51.6230)  loss_ce_0_unscaled: 1.3622 (1.3140)  loss_bbox_0_unscaled: 0.1465 (0.1524)  loss_giou_0_unscaled: 0.9333 (0.9331)  cardinality_error_0_unscaled: 6.5000 (32.9426)  loss_ce_1_unscaled: 1.3181 (1.3362)  loss_bbox_1_unscaled: 0.1463 (0.1697)  loss_giou_1_unscaled: 0.9283 (0.9776)  cardinality_error_1_unscaled: 38.0000 (47.6475)  loss_ce_2_unscaled: 1.2952 (1.3336)  loss_bbox_2_unscaled: 0.1475 (0.1748)  loss_giou_2_unscaled: 0.8645 (0.9864)  cardinality_error_2_unscaled: 54.5000 (49.4098)  loss_ce_3_unscaled: 1.2678 (1.3190)  loss_bbox_3_unscaled: 0.1410 (0.1700)  loss_giou_3_unscaled: 0.9135 (0.9829)  cardinality_error_3_unscaled: 49.0000 (48.7787)  loss_ce_4_unscaled: 1.3106 (1.3613)  loss_bbox_4_unscaled: 0.1367 (0.1585)  loss_giou_4_unscaled: 0.8756 (0.9483)  cardinality_error_4_unscaled: 38.5000 (48.1475)  time: 0.2774  data: 0.0136  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.6469, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(90.9091, device='cuda:0'), 'loss_bbox': tensor(0.2577, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3719, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5402, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1301, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9019, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(9., device='cuda:0'), 'loss_ce_1': tensor(1.6956, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1228, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8574, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(17.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6405, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1257, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9035, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(28.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6019, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1363, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9791, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(15., device='cuda:0'), 'loss_ce_4': tensor(1.6485, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1233, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8624, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(11., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.8095, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9970, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(60., device='cuda:0'), 'loss_bbox': tensor(0.2328, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1856, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(11.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1673, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1306, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8052, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(11., device='cuda:0'), 'loss_ce_1': tensor(1.1282, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1301, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8786, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(13., device='cuda:0'), 'loss_ce_2': tensor(1.0766, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1212, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8876, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(13., device='cuda:0'), 'loss_ce_3': tensor(1.1572, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1264, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9116, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(14., device='cuda:0'), 'loss_ce_4': tensor(1.1643, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1330, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8708, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(13.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.0079, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0050, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(51.1111, device='cuda:0'), 'loss_bbox': tensor(0.2312, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1945, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(14.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1857, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1265, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8020, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(21., device='cuda:0'), 'loss_ce_1': tensor(1.1500, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1396, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8989, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(14.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1358, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1480, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9781, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(17.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1813, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1624, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0482, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(15., device='cuda:0'), 'loss_ce_4': tensor(1.1320, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1505, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9424, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(17.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.1827, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3654, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(83.8710, device='cuda:0'), 'loss_bbox': tensor(0.1994, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1088, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(1., device='cuda:0'), 'loss_ce_0': tensor(1.3411, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1809, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9848, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(23.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4360, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1625, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0694, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(4., device='cuda:0'), 'loss_ce_2': tensor(1.4512, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1711, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0870, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(5.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4711, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1820, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1452, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(11., device='cuda:0'), 'loss_ce_4': tensor(1.4103, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1676, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0386, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(9.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.4714, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2784, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(78.3784, device='cuda:0'), 'loss_bbox': tensor(0.2562, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1307, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(4., device='cuda:0'), 'loss_ce_0': tensor(1.3353, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2004, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8851, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(14., device='cuda:0'), 'loss_ce_1': tensor(1.3528, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1780, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8462, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(3., device='cuda:0'), 'loss_ce_2': tensor(1.3071, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1993, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9276, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(3., device='cuda:0'), 'loss_ce_3': tensor(1.4035, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2114, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9665, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(16., device='cuda:0'), 'loss_ce_4': tensor(1.3686, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2065, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9471, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(16.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.5527, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4889, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75.6757, device='cuda:0'), 'loss_bbox': tensor(0.1307, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9217, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(6.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5361, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1103, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7889, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(17.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5437, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1079, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8198, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(9., device='cuda:0'), 'loss_ce_2': tensor(1.5345, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1130, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8811, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(4.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6230, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1356, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0433, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(13.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6106, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1153, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8129, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(13., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.2443, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1830, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(88.5714, device='cuda:0'), 'loss_bbox': tensor(0.1139, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7216, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(14.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0478, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1394, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9211, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(25., device='cuda:0'), 'loss_ce_1': tensor(1.0802, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1489, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0103, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(14.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2164, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1332, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0210, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(13.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3672, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1456, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9888, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(10.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2175, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1125, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7269, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(15., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.6432, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0220, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75.8621, device='cuda:0'), 'loss_bbox': tensor(0.1454, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8515, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(4., device='cuda:0'), 'loss_ce_0': tensor(1.0015, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1323, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8161, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(37., device='cuda:0'), 'loss_ce_1': tensor(0.9972, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1206, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7751, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(36., device='cuda:0'), 'loss_ce_2': tensor(1.0032, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1253, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8203, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(11.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1514, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1326, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9349, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(6., device='cuda:0'), 'loss_ce_4': tensor(1.0510, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1310, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8165, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(4.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.9874, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2603, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(76.3636, device='cuda:0'), 'loss_bbox': tensor(0.1299, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8314, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(10.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1588, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1184, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8117, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(40.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2302, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1251, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8575, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(36., device='cuda:0'), 'loss_ce_2': tensor(1.2574, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1199, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8392, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(12.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3771, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1233, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8550, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(13.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2997, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1236, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8309, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(16., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.1445, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3510, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(81.0811, device='cuda:0'), 'loss_bbox': tensor(0.1208, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8372, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(16., device='cuda:0'), 'loss_ce_0': tensor(1.2538, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1166, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8316, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(62.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2796, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1148, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7596, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(53., device='cuda:0'), 'loss_ce_2': tensor(1.1990, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1187, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8433, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(20., device='cuda:0'), 'loss_ce_3': tensor(1.3835, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1289, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9514, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(13.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2850, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1161, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8528, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(16., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.2836, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [3]  [ 70/100]  eta: 0:00:08  lr: 0.000100  class_error: 81.08  loss: 23.3093 (24.3530)  loss_ce: 1.2784 (1.3691)  loss_bbox: 0.8385 (0.8036)  loss_giou: 2.0501 (1.9348)  loss_ce_0: 1.2538 (1.3059)  loss_bbox_0: 0.6615 (0.7524)  loss_giou_0: 1.6633 (1.8442)  loss_ce_1: 1.2796 (1.3296)  loss_bbox_1: 0.6981 (0.8239)  loss_giou_1: 1.7362 (1.9269)  loss_ce_2: 1.2706 (1.3264)  loss_bbox_2: 0.6660 (0.8477)  loss_giou_2: 1.7623 (1.9538)  loss_ce_3: 1.3672 (1.3265)  loss_bbox_3: 0.7261 (0.8347)  loss_giou_3: 1.9330 (1.9656)  loss_ce_4: 1.2850 (1.3553)  loss_bbox_4: 0.6548 (0.7782)  loss_giou_4: 1.7057 (1.8745)  loss_ce_unscaled: 1.2784 (1.3691)  class_error_unscaled: 62.7907 (61.0500)  loss_bbox_unscaled: 0.1677 (0.1607)  loss_giou_unscaled: 1.0250 (0.9674)  cardinality_error_unscaled: 30.5000 (45.9437)  loss_ce_0_unscaled: 1.2538 (1.3059)  loss_bbox_0_unscaled: 0.1323 (0.1505)  loss_giou_0_unscaled: 0.8316 (0.9221)  cardinality_error_0_unscaled: 12.0000 (31.9789)  loss_ce_1_unscaled: 1.2796 (1.3296)  loss_bbox_1_unscaled: 0.1396 (0.1648)  loss_giou_1_unscaled: 0.8681 (0.9634)  cardinality_error_1_unscaled: 36.0000 (43.7606)  loss_ce_2_unscaled: 1.2706 (1.3264)  loss_bbox_2_unscaled: 0.1332 (0.1695)  loss_giou_2_unscaled: 0.8811 (0.9769)  cardinality_error_2_unscaled: 28.5000 (44.2746)  loss_ce_3_unscaled: 1.3672 (1.3265)  loss_bbox_3_unscaled: 0.1452 (0.1669)  loss_giou_3_unscaled: 0.9665 (0.9828)  cardinality_error_3_unscaled: 16.0000 (43.7113)  loss_ce_4_unscaled: 1.2850 (1.3553)  loss_bbox_4_unscaled: 0.1310 (0.1556)  loss_giou_4_unscaled: 0.8528 (0.9373)  cardinality_error_4_unscaled: 17.5000 (43.2324)  time: 0.2755  data: 0.0120  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.1423, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(77.4194, device='cuda:0'), 'loss_bbox': tensor(0.1242, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7938, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(13., device='cuda:0'), 'loss_ce_0': tensor(1.0930, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1199, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8748, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(69.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1642, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1100, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7281, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(80.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1457, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1118, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8282, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(36.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2416, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1539, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9427, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(19., device='cuda:0'), 'loss_ce_4': tensor(1.1644, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1238, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8185, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(17., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.4551, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2780, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(86.1111, device='cuda:0'), 'loss_bbox': tensor(0.1239, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8453, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(19.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3107, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1129, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8374, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(74., device='cuda:0'), 'loss_ce_1': tensor(1.3506, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1371, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8656, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(79.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2753, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1149, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8624, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(40.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2970, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1367, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0450, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(30.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2795, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1228, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8663, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(48., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.9490, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0764, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(57.6923, device='cuda:0'), 'loss_bbox': tensor(0.1013, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7626, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0688, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0976, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7792, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(83., device='cuda:0'), 'loss_ce_1': tensor(1.1240, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1146, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7956, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(84.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0860, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1052, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8158, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1334, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1041, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7974, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(50., device='cuda:0'), 'loss_ce_4': tensor(1.0512, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1041, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8314, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(65., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.2013, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1991, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(33.3333, device='cuda:0'), 'loss_bbox': tensor(0.1262, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8400, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(34., device='cuda:0'), 'loss_ce_0': tensor(1.3263, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1132, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8361, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(78.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2903, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1482, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0490, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(83., device='cuda:0'), 'loss_ce_2': tensor(1.2710, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1193, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8895, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(67.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2173, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1171, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8400, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2054, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1201, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8224, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(76., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.6061, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0487, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(36.1111, device='cuda:0'), 'loss_bbox': tensor(0.1552, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8782, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(45., device='cuda:0'), 'loss_ce_0': tensor(1.0107, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1579, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8719, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(79.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0789, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1541, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8584, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(82., device='cuda:0'), 'loss_ce_2': tensor(1.0358, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1402, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8864, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(73., device='cuda:0'), 'loss_ce_3': tensor(1.0475, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1581, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9031, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(72.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0182, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1574, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8956, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(77., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.2880, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1378, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(42.8571, device='cuda:0'), 'loss_bbox': tensor(0.1351, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8443, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(58.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2072, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1272, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8809, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(84., device='cuda:0'), 'loss_ce_1': tensor(1.2639, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1414, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9370, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(86., device='cuda:0'), 'loss_ce_2': tensor(1.1960, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1287, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8951, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(81.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1894, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1257, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8389, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(80.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1796, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1328, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8672, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(85., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.4062, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1127, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(26.9231, device='cuda:0'), 'loss_bbox': tensor(0.1470, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9999, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(65.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0776, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1355, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9503, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(86., device='cuda:0'), 'loss_ce_1': tensor(1.1554, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1649, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1129, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(87., device='cuda:0'), 'loss_ce_2': tensor(1.0889, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1449, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0114, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(84.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1294, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1289, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8844, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(85., device='cuda:0'), 'loss_ce_4': tensor(1.0857, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1399, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9418, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(87., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.6583, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0936, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(30.4348, device='cuda:0'), 'loss_bbox': tensor(0.1394, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9263, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(75.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1408, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1334, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9940, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(86., device='cuda:0'), 'loss_ce_1': tensor(1.2512, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1430, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0317, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(88., device='cuda:0'), 'loss_ce_2': tensor(1.1371, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1400, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0288, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(85., device='cuda:0'), 'loss_ce_3': tensor(1.1694, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1287, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8660, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(87., device='cuda:0'), 'loss_ce_4': tensor(1.1409, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1361, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9415, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(88.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.4348, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.9527, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(19.2308, device='cuda:0'), 'loss_bbox': tensor(0.1017, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7545, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(72.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9675, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1107, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7922, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(85.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0468, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1361, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8557, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(87., device='cuda:0'), 'loss_ce_2': tensor(0.9824, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1139, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8052, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(84.5000, device='cuda:0'), 'loss_ce_3': tensor(0.9899, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1065, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7670, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(85.5000, device='cuda:0'), 'loss_ce_4': tensor(0.9643, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1106, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8225, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(87., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.7412, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(0.8778, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(16.1290, device='cuda:0'), 'loss_bbox': tensor(0.2079, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9011, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(73.5000, device='cuda:0'), 'loss_ce_0': tensor(0.8215, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1795, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8468, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(83., device='cuda:0'), 'loss_ce_1': tensor(0.9505, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2015, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9612, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(84.5000, device='cuda:0'), 'loss_ce_2': tensor(0.8971, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1901, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8146, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(81., device='cuda:0'), 'loss_ce_3': tensor(0.8715, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1840, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7728, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(84.5000, device='cuda:0'), 'loss_ce_4': tensor(0.8524, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1753, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7879, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(83.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.9226, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [3]  [ 80/100]  eta: 0:00:05  lr: 0.000100  class_error: 16.13  loss: 21.7842 (23.9678)  loss_ce: 1.1378 (1.3349)  loss_bbox: 0.6754 (0.7884)  loss_giou: 1.7030 (1.9069)  loss_ce_0: 1.1588 (1.2808)  loss_bbox_0: 0.6361 (0.7390)  loss_giou_0: 1.6749 (1.8304)  loss_ce_1: 1.1642 (1.3096)  loss_bbox_1: 0.6855 (0.8118)  loss_giou_1: 1.7169 (1.9160)  loss_ce_2: 1.1457 (1.2999)  loss_bbox_2: 0.6265 (0.8239)  loss_giou_2: 1.7728 (1.9308)  loss_ce_3: 1.1894 (1.3020)  loss_bbox_3: 0.6628 (0.8146)  loss_giou_3: 1.8232 (1.9367)  loss_ce_4: 1.1644 (1.3230)  loss_bbox_4: 0.6192 (0.7638)  loss_giou_4: 1.7057 (1.8553)  loss_ce_unscaled: 1.1378 (1.3349)  class_error_unscaled: 60.0000 (58.7752)  loss_bbox_unscaled: 0.1351 (0.1577)  loss_giou_unscaled: 0.8515 (0.9535)  cardinality_error_unscaled: 16.0000 (46.2901)  loss_ce_0_unscaled: 1.1588 (1.2808)  loss_bbox_0_unscaled: 0.1272 (0.1478)  loss_giou_0_unscaled: 0.8374 (0.9152)  cardinality_error_0_unscaled: 62.5000 (38.0185)  loss_ce_1_unscaled: 1.1642 (1.3096)  loss_bbox_1_unscaled: 0.1371 (0.1624)  loss_giou_1_unscaled: 0.8584 (0.9580)  cardinality_error_1_unscaled: 53.0000 (48.7531)  loss_ce_2_unscaled: 1.1457 (1.2999)  loss_bbox_2_unscaled: 0.1253 (0.1648)  loss_giou_2_unscaled: 0.8864 (0.9654)  cardinality_error_2_unscaled: 28.5000 (47.4074)  loss_ce_3_unscaled: 1.1894 (1.3020)  loss_bbox_3_unscaled: 0.1326 (0.1629)  loss_giou_3_unscaled: 0.9116 (0.9684)  cardinality_error_3_unscaled: 16.0000 (46.4259)  loss_ce_4_unscaled: 1.1644 (1.3230)  loss_bbox_4_unscaled: 0.1238 (0.1528)  loss_giou_4_unscaled: 0.8528 (0.9277)  cardinality_error_4_unscaled: 17.0000 (46.7099)  time: 0.2746  data: 0.0117  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.3658, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(42.8571, device='cuda:0'), 'loss_bbox': tensor(0.1358, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9576, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(76.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3541, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1617, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1108, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(79.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3964, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.3522, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.4521, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(82.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3415, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1727, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9579, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(78.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4042, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1363, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8662, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(80., device='cuda:0'), 'loss_ce_4': tensor(1.4352, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1542, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0541, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(82.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.4744, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3635, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(48.8372, device='cuda:0'), 'loss_bbox': tensor(0.1252, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8846, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(70., device='cuda:0'), 'loss_ce_0': tensor(1.3763, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1262, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9295, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(75., device='cuda:0'), 'loss_ce_1': tensor(1.3811, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2787, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2608, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(78.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3486, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1383, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9395, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(74.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3655, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1246, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8649, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(77.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3973, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1344, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9538, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(78., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.3513, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0082, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(21.4286, device='cuda:0'), 'loss_bbox': tensor(0.1324, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0220, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(73.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0130, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1113, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8623, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(74.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0040, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1541, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9649, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(78.5000, device='cuda:0'), 'loss_ce_2': tensor(0.9563, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1249, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8716, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(72.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0006, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1282, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8860, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(76., device='cuda:0'), 'loss_ce_4': tensor(0.9826, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1080, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8429, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(79., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.4456, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4991, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(62.0690, device='cuda:0'), 'loss_bbox': tensor(0.1366, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9113, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(75., device='cuda:0'), 'loss_ce_0': tensor(1.4448, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1638, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0817, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(72., device='cuda:0'), 'loss_ce_1': tensor(1.5269, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2455, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.2433, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(85.5000, device='cuda:0'), 'loss_ce_2': tensor(1.5415, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1610, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0636, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(78., device='cuda:0'), 'loss_ce_3': tensor(1.5556, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1348, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9186, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(83., device='cuda:0'), 'loss_ce_4': tensor(1.6477, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1752, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0774, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(85.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.6642, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1145, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(25., device='cuda:0'), 'loss_bbox': tensor(0.1016, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8276, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(82., device='cuda:0'), 'loss_ce_0': tensor(1.0642, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0842, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7166, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(67., device='cuda:0'), 'loss_ce_1': tensor(1.1204, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1045, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8724, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(86.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1114, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0999, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7363, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(78., device='cuda:0'), 'loss_ce_3': tensor(1.0680, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1103, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8137, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(86.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0523, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1076, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8120, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(89.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(21.9335, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0680, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(15.1515, device='cuda:0'), 'loss_bbox': tensor(0.1380, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0048, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(66., device='cuda:0'), 'loss_ce_0': tensor(1.1025, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1240, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9357, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(47.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0602, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1537, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0930, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(74., device='cuda:0'), 'loss_ce_2': tensor(1.0865, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1298, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9308, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(69., device='cuda:0'), 'loss_ce_3': tensor(1.0543, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1365, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0101, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(77.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0214, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1333, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9602, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(80.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.1557, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3378, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(42.1053, device='cuda:0'), 'loss_bbox': tensor(0.1008, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7856, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(67.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2571, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1081, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8625, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(37.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3454, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1053, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8300, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(73., device='cuda:0'), 'loss_ce_2': tensor(1.3214, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0944, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7993, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(66., device='cuda:0'), 'loss_ce_3': tensor(1.3406, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1006, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7987, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(82., device='cuda:0'), 'loss_ce_4': tensor(1.3653, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1088, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8339, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(86.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.6682, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2281, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(55.8140, device='cuda:0'), 'loss_bbox': tensor(0.1153, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7315, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(54., device='cuda:0'), 'loss_ce_0': tensor(1.1590, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1208, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8127, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(25., device='cuda:0'), 'loss_ce_1': tensor(1.2070, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1210, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8492, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(55.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2277, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1072, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7058, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(43.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2382, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1120, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7567, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(62., device='cuda:0'), 'loss_ce_4': tensor(1.2019, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1220, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8008, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(70., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.8190, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0923, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(56.4103, device='cuda:0'), 'loss_bbox': tensor(0.1918, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3926, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(55., device='cuda:0'), 'loss_ce_0': tensor(1.0601, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1745, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1258, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(19.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0646, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1921, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1517, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48., device='cuda:0'), 'loss_ce_2': tensor(1.0881, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1752, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1431, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(41., device='cuda:0'), 'loss_ce_3': tensor(1.0628, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1902, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.2251, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(61.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0674, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1727, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1548, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(65.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.1529, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3896, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(53.4884, device='cuda:0'), 'loss_bbox': tensor(0.1360, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9087, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(55., device='cuda:0'), 'loss_ce_0': tensor(1.3647, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1859, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1678, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(10.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3555, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2527, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.3179, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(30.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3192, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1457, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0101, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(33.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3627, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1359, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9241, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(63., device='cuda:0'), 'loss_ce_4': tensor(1.3667, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1750, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0551, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(61.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.9175, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [3]  [ 90/100]  eta: 0:00:02  lr: 0.000100  class_error: 53.49  loss: 21.6550 (23.9004)  loss_ce: 1.1145 (1.3252)  loss_bbox: 0.6622 (0.7740)  loss_giou: 1.7565 (1.9045)  loss_ce_0: 1.1025 (1.2741)  loss_bbox_0: 0.6198 (0.7326)  loss_giou_0: 1.7437 (1.8404)  loss_ce_1: 1.1642 (1.3026)  loss_bbox_1: 0.7410 (0.8302)  loss_giou_1: 1.9225 (1.9480)  loss_ce_2: 1.1371 (1.2927)  loss_bbox_2: 0.6434 (0.8075)  loss_giou_2: 1.7728 (1.9199)  loss_ce_3: 1.1694 (1.2958)  loss_bbox_3: 0.6433 (0.7970)  loss_giou_3: 1.7319 (1.9231)  loss_ce_4: 1.1409 (1.3154)  loss_bbox_4: 0.6638 (0.7563)  loss_giou_4: 1.7326 (1.8612)  loss_ce_unscaled: 1.1145 (1.3252)  class_error_unscaled: 42.1053 (56.9665)  loss_bbox_unscaled: 0.1324 (0.1548)  loss_giou_unscaled: 0.8782 (0.9523)  cardinality_error_unscaled: 65.5000 (48.6154)  loss_ce_0_unscaled: 1.1025 (1.2741)  loss_bbox_0_unscaled: 0.1240 (0.1465)  loss_giou_0_unscaled: 0.8719 (0.9202)  cardinality_error_0_unscaled: 74.5000 (39.4231)  loss_ce_1_unscaled: 1.1642 (1.3026)  loss_bbox_1_unscaled: 0.1482 (0.1660)  loss_giou_1_unscaled: 0.9612 (0.9740)  cardinality_error_1_unscaled: 82.0000 (51.0055)  loss_ce_2_unscaled: 1.1371 (1.2927)  loss_bbox_2_unscaled: 0.1287 (0.1615)  loss_giou_2_unscaled: 0.8864 (0.9600)  cardinality_error_2_unscaled: 72.5000 (49.1703)  loss_ce_3_unscaled: 1.1694 (1.2958)  loss_bbox_3_unscaled: 0.1287 (0.1594)  loss_giou_3_unscaled: 0.8660 (0.9615)  cardinality_error_3_unscaled: 77.5000 (49.5549)  loss_ce_4_unscaled: 1.1409 (1.3154)  loss_bbox_4_unscaled: 0.1328 (0.1513)  loss_giou_4_unscaled: 0.8663 (0.9306)  cardinality_error_4_unscaled: 79.0000 (50.1319)  time: 0.2756  data: 0.0119  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.4365, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(68.4211, device='cuda:0'), 'loss_bbox': tensor(0.1414, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9095, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(54., device='cuda:0'), 'loss_ce_0': tensor(1.4244, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1373, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8861, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(12.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3647, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1563, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9929, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(25., device='cuda:0'), 'loss_ce_2': tensor(1.3667, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1274, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8308, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(39., device='cuda:0'), 'loss_ce_3': tensor(1.4169, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1255, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8210, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(57.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4288, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1345, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8344, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(54.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.9143, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0525, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(54.5455, device='cuda:0'), 'loss_bbox': tensor(0.1384, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9330, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(60., device='cuda:0'), 'loss_ce_0': tensor(1.0933, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1371, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9022, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(15.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0519, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1506, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9854, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(26.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0404, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1467, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9438, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(39.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0504, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1428, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9292, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(67., device='cuda:0'), 'loss_ce_4': tensor(1.0509, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1401, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8674, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(62.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.6129, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.6549, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(81.0811, device='cuda:0'), 'loss_bbox': tensor(0.1188, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9930, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(53.5000, device='cuda:0'), 'loss_ce_0': tensor(1.6526, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1010, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9009, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(2., device='cuda:0'), 'loss_ce_1': tensor(1.6282, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1122, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9252, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(16.5000, device='cuda:0'), 'loss_ce_2': tensor(1.6116, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1026, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8460, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(27.5000, device='cuda:0'), 'loss_ce_3': tensor(1.6435, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1059, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8829, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(51.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6143, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1067, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8443, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(47.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.6140, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1048, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(68.7500, device='cuda:0'), 'loss_bbox': tensor(0.1692, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2407, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(48., device='cuda:0'), 'loss_ce_0': tensor(1.0847, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1692, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1890, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(4.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0778, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1629, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0954, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(8., device='cuda:0'), 'loss_ce_2': tensor(1.0948, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1698, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0878, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(28.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0283, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1792, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1651, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(40., device='cuda:0'), 'loss_ce_4': tensor(0.9957, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1685, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1126, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(37., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.0839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3108, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(69.6970, device='cuda:0'), 'loss_bbox': tensor(0.1756, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1158, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(43., device='cuda:0'), 'loss_ce_0': tensor(1.3278, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1804, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1846, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(11.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3254, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1727, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1486, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(14.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2911, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1812, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1009, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(27.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2381, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1861, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1016, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(45.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2553, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1964, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1203, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(27., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.6115, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2910, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(52.7778, device='cuda:0'), 'loss_bbox': tensor(0.1434, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9261, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(46.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2873, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1374, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8646, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(11., device='cuda:0'), 'loss_ce_1': tensor(1.2664, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1314, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8017, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(15.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3089, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1352, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8468, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(34.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3000, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1373, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8499, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(40., device='cuda:0'), 'loss_ce_4': tensor(1.3061, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1374, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8324, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(30.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.9299, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3905, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(73.9130, device='cuda:0'), 'loss_bbox': tensor(0.1103, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8338, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(52., device='cuda:0'), 'loss_ce_0': tensor(1.2318, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.0999, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8278, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(13.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3410, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1029, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7939, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(28.5000, device='cuda:0'), 'loss_ce_2': tensor(1.3648, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1181, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8507, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(45.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3939, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1165, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8143, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(53., device='cuda:0'), 'loss_ce_4': tensor(1.3491, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1103, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8396, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(32.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.1242, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0462, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(57.1429, device='cuda:0'), 'loss_bbox': tensor(0.1620, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9182, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(50., device='cuda:0'), 'loss_ce_0': tensor(0.9848, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1798, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0051, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(18.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0376, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1763, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9066, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(34., device='cuda:0'), 'loss_ce_2': tensor(1.0532, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1748, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8674, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(46.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0361, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1875, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8944, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(44.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0189, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1789, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9049, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(29.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.3713, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3014, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(66.6667, device='cuda:0'), 'loss_bbox': tensor(0.2324, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1706, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(43., device='cuda:0'), 'loss_ce_0': tensor(1.1811, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2698, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.2169, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(8., device='cuda:0'), 'loss_ce_1': tensor(1.2163, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1320, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8163, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(28.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2431, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1585, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9286, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(40.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2631, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1324, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8548, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(44.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2104, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1123, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.7612, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(19.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.9664, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [3]  [ 99/100]  eta: 0:00:00  lr: 0.000100  class_error: 66.67  loss: 22.4669 (23.8558)  loss_ce: 1.2910 (1.3218)  loss_bbox: 0.6829 (0.7739)  loss_giou: 1.8364 (1.9139)  loss_ce_0: 1.1811 (1.2721)  loss_bbox_0: 0.6863 (0.7372)  loss_giou_0: 1.8045 (1.8543)  loss_ce_1: 1.2163 (1.2985)  loss_bbox_1: 0.7703 (0.8204)  loss_giou_1: 1.9298 (1.9420)  loss_ce_2: 1.2431 (1.2901)  loss_bbox_2: 0.6913 (0.8005)  loss_giou_2: 1.7433 (1.9132)  loss_ce_3: 1.2382 (1.2929)  loss_bbox_3: 0.6741 (0.7909)  loss_giou_3: 1.7325 (1.9163)  loss_ce_4: 1.2104 (1.3093)  loss_bbox_4: 0.6723 (0.7525)  loss_giou_4: 1.6886 (1.8561)  loss_ce_unscaled: 1.2910 (1.3218)  class_error_unscaled: 54.5455 (57.7695)  loss_bbox_unscaled: 0.1366 (0.1548)  loss_giou_unscaled: 0.9182 (0.9570)  cardinality_error_unscaled: 55.0000 (48.7400)  loss_ce_0_unscaled: 1.1811 (1.2721)  loss_bbox_0_unscaled: 0.1373 (0.1474)  loss_giou_0_unscaled: 0.9022 (0.9271)  cardinality_error_0_unscaled: 18.5000 (36.8450)  loss_ce_1_unscaled: 1.2163 (1.2985)  loss_bbox_1_unscaled: 0.1541 (0.1641)  loss_giou_1_unscaled: 0.9649 (0.9710)  cardinality_error_1_unscaled: 34.0000 (48.3850)  loss_ce_2_unscaled: 1.2431 (1.2901)  loss_bbox_2_unscaled: 0.1383 (0.1601)  loss_giou_2_unscaled: 0.8716 (0.9566)  cardinality_error_2_unscaled: 43.5000 (48.0350)  loss_ce_3_unscaled: 1.2382 (1.2929)  loss_bbox_3_unscaled: 0.1348 (0.1582)  loss_giou_3_unscaled: 0.8662 (0.9581)  cardinality_error_3_unscaled: 62.0000 (49.5300)  loss_ce_4_unscaled: 1.2104 (1.3093)  loss_bbox_4_unscaled: 0.1345 (0.1505)  loss_giou_4_unscaled: 0.8443 (0.9280)  cardinality_error_4_unscaled: 62.5000 (49.0250)  time: 0.2741  data: 0.0114  max mem: 1537\n",
            "Epoch: [3] Total time: 0:00:27 (0.2779 s / it)\n",
            "Averaged stats: lr: 0.000100  class_error: 66.67  loss: 22.4669 (23.8558)  loss_ce: 1.2910 (1.3218)  loss_bbox: 0.6829 (0.7739)  loss_giou: 1.8364 (1.9139)  loss_ce_0: 1.1811 (1.2721)  loss_bbox_0: 0.6863 (0.7372)  loss_giou_0: 1.8045 (1.8543)  loss_ce_1: 1.2163 (1.2985)  loss_bbox_1: 0.7703 (0.8204)  loss_giou_1: 1.9298 (1.9420)  loss_ce_2: 1.2431 (1.2901)  loss_bbox_2: 0.6913 (0.8005)  loss_giou_2: 1.7433 (1.9132)  loss_ce_3: 1.2382 (1.2929)  loss_bbox_3: 0.6741 (0.7909)  loss_giou_3: 1.7325 (1.9163)  loss_ce_4: 1.2104 (1.3093)  loss_bbox_4: 0.6723 (0.7525)  loss_giou_4: 1.6886 (1.8561)  loss_ce_unscaled: 1.2910 (1.3218)  class_error_unscaled: 54.5455 (57.7695)  loss_bbox_unscaled: 0.1366 (0.1548)  loss_giou_unscaled: 0.9182 (0.9570)  cardinality_error_unscaled: 55.0000 (48.7400)  loss_ce_0_unscaled: 1.1811 (1.2721)  loss_bbox_0_unscaled: 0.1373 (0.1474)  loss_giou_0_unscaled: 0.9022 (0.9271)  cardinality_error_0_unscaled: 18.5000 (36.8450)  loss_ce_1_unscaled: 1.2163 (1.2985)  loss_bbox_1_unscaled: 0.1541 (0.1641)  loss_giou_1_unscaled: 0.9649 (0.9710)  cardinality_error_1_unscaled: 34.0000 (48.3850)  loss_ce_2_unscaled: 1.2431 (1.2901)  loss_bbox_2_unscaled: 0.1383 (0.1601)  loss_giou_2_unscaled: 0.8716 (0.9566)  cardinality_error_2_unscaled: 43.5000 (48.0350)  loss_ce_3_unscaled: 1.2382 (1.2929)  loss_bbox_3_unscaled: 0.1348 (0.1582)  loss_giou_3_unscaled: 0.8662 (0.9581)  cardinality_error_3_unscaled: 62.0000 (49.5300)  loss_ce_4_unscaled: 1.2104 (1.3093)  loss_bbox_4_unscaled: 0.1345 (0.1505)  loss_giou_4_unscaled: 0.8443 (0.9280)  cardinality_error_4_unscaled: 62.5000 (49.0250)\n",
            "Test:  [ 0/25]  eta: 0:00:07  class_error: 46.51  loss: 44.1413 (44.1413)  loss_ce: 1.2970 (1.2970)  loss_bbox: 3.0357 (3.0357)  loss_giou: 3.6251 (3.6251)  loss_ce_0: 1.3137 (1.3137)  loss_bbox_0: 3.0818 (3.0818)  loss_giou_0: 3.6453 (3.6453)  loss_ce_1: 1.2827 (1.2827)  loss_bbox_1: 2.2964 (2.2964)  loss_giou_1: 3.4170 (3.4170)  loss_ce_2: 1.2898 (1.2898)  loss_bbox_2: 2.3920 (2.3920)  loss_giou_2: 3.4665 (3.4665)  loss_ce_3: 1.2849 (1.2849)  loss_bbox_3: 2.5006 (2.5006)  loss_giou_3: 3.4992 (3.4992)  loss_ce_4: 1.3133 (1.3133)  loss_bbox_4: 1.9753 (1.9753)  loss_giou_4: 3.4250 (3.4250)  loss_ce_unscaled: 1.2970 (1.2970)  class_error_unscaled: 46.5116 (46.5116)  loss_bbox_unscaled: 0.6071 (0.6071)  loss_giou_unscaled: 1.8126 (1.8126)  cardinality_error_unscaled: 78.5000 (78.5000)  loss_ce_0_unscaled: 1.3137 (1.3137)  loss_bbox_0_unscaled: 0.6164 (0.6164)  loss_giou_0_unscaled: 1.8227 (1.8227)  cardinality_error_0_unscaled: 21.5000 (21.5000)  loss_ce_1_unscaled: 1.2827 (1.2827)  loss_bbox_1_unscaled: 0.4593 (0.4593)  loss_giou_1_unscaled: 1.7085 (1.7085)  cardinality_error_1_unscaled: 78.5000 (78.5000)  loss_ce_2_unscaled: 1.2898 (1.2898)  loss_bbox_2_unscaled: 0.4784 (0.4784)  loss_giou_2_unscaled: 1.7333 (1.7333)  cardinality_error_2_unscaled: 78.5000 (78.5000)  loss_ce_3_unscaled: 1.2849 (1.2849)  loss_bbox_3_unscaled: 0.5001 (0.5001)  loss_giou_3_unscaled: 1.7496 (1.7496)  cardinality_error_3_unscaled: 78.5000 (78.5000)  loss_ce_4_unscaled: 1.3133 (1.3133)  loss_bbox_4_unscaled: 0.3951 (0.3951)  loss_giou_4_unscaled: 1.7125 (1.7125)  cardinality_error_4_unscaled: 21.5000 (21.5000)  time: 0.2928  data: 0.1616  max mem: 1537\n",
            "Test:  [10/25]  eta: 0:00:02  class_error: 11.54  loss: 40.1069 (41.3253)  loss_ce: 1.6038 (1.6128)  loss_bbox: 2.4575 (2.3542)  loss_giou: 3.5499 (3.5363)  loss_ce_0: 1.6126 (1.6139)  loss_bbox_0: 2.4795 (2.3791)  loss_giou_0: 3.5735 (3.5611)  loss_ce_1: 1.5870 (1.5914)  loss_bbox_1: 1.7072 (1.6698)  loss_giou_1: 3.3025 (3.2719)  loss_ce_2: 1.5899 (1.5982)  loss_bbox_2: 1.7995 (1.7500)  loss_giou_2: 3.3343 (3.3330)  loss_ce_3: 1.6039 (1.6093)  loss_bbox_3: 1.8926 (1.8476)  loss_giou_3: 3.4018 (3.3756)  loss_ce_4: 1.6037 (1.6068)  loss_bbox_4: 1.4305 (1.4518)  loss_giou_4: 3.1473 (3.1626)  loss_ce_unscaled: 1.6038 (1.6128)  class_error_unscaled: 46.5116 (50.5515)  loss_bbox_unscaled: 0.4915 (0.4708)  loss_giou_unscaled: 1.7749 (1.7682)  cardinality_error_unscaled: 80.0000 (81.0000)  loss_ce_0_unscaled: 1.6126 (1.6139)  loss_bbox_0_unscaled: 0.4959 (0.4758)  loss_giou_0_unscaled: 1.7868 (1.7805)  cardinality_error_0_unscaled: 20.0000 (19.0000)  loss_ce_1_unscaled: 1.5870 (1.5914)  loss_bbox_1_unscaled: 0.3414 (0.3340)  loss_giou_1_unscaled: 1.6512 (1.6359)  cardinality_error_1_unscaled: 80.0000 (81.0000)  loss_ce_2_unscaled: 1.5899 (1.5982)  loss_bbox_2_unscaled: 0.3599 (0.3500)  loss_giou_2_unscaled: 1.6672 (1.6665)  cardinality_error_2_unscaled: 80.0000 (81.0000)  loss_ce_3_unscaled: 1.6039 (1.6093)  loss_bbox_3_unscaled: 0.3785 (0.3695)  loss_giou_3_unscaled: 1.7009 (1.6878)  cardinality_error_3_unscaled: 80.0000 (81.0000)  loss_ce_4_unscaled: 1.6037 (1.6068)  loss_bbox_4_unscaled: 0.2861 (0.2904)  loss_giou_4_unscaled: 1.5736 (1.5813)  cardinality_error_4_unscaled: 20.0000 (19.0000)  time: 0.1369  data: 0.0228  max mem: 1537\n",
            "Test:  [20/25]  eta: 0:00:00  class_error: 88.89  loss: 40.1069 (40.9185)  loss_ce: 1.3510 (1.5103)  loss_bbox: 2.4148 (2.3489)  loss_giou: 3.5507 (3.5455)  loss_ce_0: 1.4148 (1.5160)  loss_bbox_0: 2.4472 (2.3711)  loss_giou_0: 3.5785 (3.5687)  loss_ce_1: 1.3514 (1.4867)  loss_bbox_1: 1.7072 (1.6782)  loss_giou_1: 3.3025 (3.3218)  loss_ce_2: 1.3389 (1.4979)  loss_bbox_2: 1.7995 (1.7582)  loss_giou_2: 3.4245 (3.3826)  loss_ce_3: 1.3467 (1.5027)  loss_bbox_3: 1.9056 (1.8543)  loss_giou_3: 3.4467 (3.4125)  loss_ce_4: 1.3811 (1.5163)  loss_bbox_4: 1.4327 (1.4467)  loss_giou_4: 3.1503 (3.2002)  loss_ce_unscaled: 1.3510 (1.5103)  class_error_unscaled: 42.8571 (51.3765)  loss_bbox_unscaled: 0.4830 (0.4698)  loss_giou_unscaled: 1.7753 (1.7727)  cardinality_error_unscaled: 79.0000 (77.7857)  loss_ce_0_unscaled: 1.4148 (1.5160)  loss_bbox_0_unscaled: 0.4894 (0.4742)  loss_giou_0_unscaled: 1.7892 (1.7843)  cardinality_error_0_unscaled: 21.0000 (22.2143)  loss_ce_1_unscaled: 1.3514 (1.4867)  loss_bbox_1_unscaled: 0.3414 (0.3356)  loss_giou_1_unscaled: 1.6512 (1.6609)  cardinality_error_1_unscaled: 79.0000 (77.7857)  loss_ce_2_unscaled: 1.3389 (1.4979)  loss_bbox_2_unscaled: 0.3599 (0.3516)  loss_giou_2_unscaled: 1.7122 (1.6913)  cardinality_error_2_unscaled: 79.0000 (77.7857)  loss_ce_3_unscaled: 1.3467 (1.5027)  loss_bbox_3_unscaled: 0.3811 (0.3709)  loss_giou_3_unscaled: 1.7233 (1.7062)  cardinality_error_3_unscaled: 79.0000 (77.7857)  loss_ce_4_unscaled: 1.3811 (1.5163)  loss_bbox_4_unscaled: 0.2865 (0.2893)  loss_giou_4_unscaled: 1.5752 (1.6001)  cardinality_error_4_unscaled: 21.0000 (22.2143)  time: 0.1302  data: 0.0101  max mem: 1537\n",
            "Test:  [24/25]  eta: 0:00:00  class_error: 32.56  loss: 40.3309 (41.1791)  loss_ce: 1.3868 (1.5401)  loss_bbox: 2.4148 (2.3649)  loss_giou: 3.5496 (3.5421)  loss_ce_0: 1.4320 (1.5459)  loss_bbox_0: 2.4311 (2.3882)  loss_giou_0: 3.5651 (3.5653)  loss_ce_1: 1.3919 (1.5187)  loss_bbox_1: 1.7492 (1.6967)  loss_giou_1: 3.3602 (3.3210)  loss_ce_2: 1.3793 (1.5266)  loss_bbox_2: 1.8257 (1.7786)  loss_giou_2: 3.4313 (3.3835)  loss_ce_3: 1.3891 (1.5337)  loss_bbox_3: 1.9245 (1.8738)  loss_giou_3: 3.4581 (3.4108)  loss_ce_4: 1.4041 (1.5415)  loss_bbox_4: 1.4375 (1.4580)  loss_giou_4: 3.1777 (3.1898)  loss_ce_unscaled: 1.3868 (1.5401)  class_error_unscaled: 42.8571 (50.3626)  loss_bbox_unscaled: 0.4830 (0.4730)  loss_giou_unscaled: 1.7748 (1.7711)  cardinality_error_unscaled: 79.0000 (78.8000)  loss_ce_0_unscaled: 1.4320 (1.5459)  loss_bbox_0_unscaled: 0.4862 (0.4776)  loss_giou_0_unscaled: 1.7826 (1.7827)  cardinality_error_0_unscaled: 21.0000 (21.2000)  loss_ce_1_unscaled: 1.3919 (1.5187)  loss_bbox_1_unscaled: 0.3498 (0.3393)  loss_giou_1_unscaled: 1.6801 (1.6605)  cardinality_error_1_unscaled: 79.0000 (78.8000)  loss_ce_2_unscaled: 1.3793 (1.5266)  loss_bbox_2_unscaled: 0.3651 (0.3557)  loss_giou_2_unscaled: 1.7157 (1.6917)  cardinality_error_2_unscaled: 79.0000 (78.8000)  loss_ce_3_unscaled: 1.3891 (1.5337)  loss_bbox_3_unscaled: 0.3849 (0.3748)  loss_giou_3_unscaled: 1.7291 (1.7054)  cardinality_error_3_unscaled: 79.0000 (78.8000)  loss_ce_4_unscaled: 1.4041 (1.5415)  loss_bbox_4_unscaled: 0.2875 (0.2916)  loss_giou_4_unscaled: 1.5888 (1.5949)  cardinality_error_4_unscaled: 21.0000 (21.2000)  time: 0.1313  data: 0.0100  max mem: 1537\n",
            "Test: Total time: 0:00:03 (0.1383 s / it)\n",
            "Averaged stats: class_error: 32.56  loss: 40.3309 (41.1791)  loss_ce: 1.3868 (1.5401)  loss_bbox: 2.4148 (2.3649)  loss_giou: 3.5496 (3.5421)  loss_ce_0: 1.4320 (1.5459)  loss_bbox_0: 2.4311 (2.3882)  loss_giou_0: 3.5651 (3.5653)  loss_ce_1: 1.3919 (1.5187)  loss_bbox_1: 1.7492 (1.6967)  loss_giou_1: 3.3602 (3.3210)  loss_ce_2: 1.3793 (1.5266)  loss_bbox_2: 1.8257 (1.7786)  loss_giou_2: 3.4313 (3.3835)  loss_ce_3: 1.3891 (1.5337)  loss_bbox_3: 1.9245 (1.8738)  loss_giou_3: 3.4581 (3.4108)  loss_ce_4: 1.4041 (1.5415)  loss_bbox_4: 1.4375 (1.4580)  loss_giou_4: 3.1777 (3.1898)  loss_ce_unscaled: 1.3868 (1.5401)  class_error_unscaled: 42.8571 (50.3626)  loss_bbox_unscaled: 0.4830 (0.4730)  loss_giou_unscaled: 1.7748 (1.7711)  cardinality_error_unscaled: 79.0000 (78.8000)  loss_ce_0_unscaled: 1.4320 (1.5459)  loss_bbox_0_unscaled: 0.4862 (0.4776)  loss_giou_0_unscaled: 1.7826 (1.7827)  cardinality_error_0_unscaled: 21.0000 (21.2000)  loss_ce_1_unscaled: 1.3919 (1.5187)  loss_bbox_1_unscaled: 0.3498 (0.3393)  loss_giou_1_unscaled: 1.6801 (1.6605)  cardinality_error_1_unscaled: 79.0000 (78.8000)  loss_ce_2_unscaled: 1.3793 (1.5266)  loss_bbox_2_unscaled: 0.3651 (0.3557)  loss_giou_2_unscaled: 1.7157 (1.6917)  cardinality_error_2_unscaled: 79.0000 (78.8000)  loss_ce_3_unscaled: 1.3891 (1.5337)  loss_bbox_3_unscaled: 0.3849 (0.3748)  loss_giou_3_unscaled: 1.7291 (1.7054)  cardinality_error_3_unscaled: 79.0000 (78.8000)  loss_ce_4_unscaled: 1.4041 (1.5415)  loss_bbox_4_unscaled: 0.2875 (0.2916)  loss_giou_4_unscaled: 1.5888 (1.5949)  cardinality_error_4_unscaled: 21.0000 (21.2000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "loss_dict {'loss_ce': tensor(1.4801, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(79.3103, device='cuda:0'), 'loss_bbox': tensor(0.1325, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0435, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(49.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4262, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1497, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1112, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(19.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4461, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1395, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9976, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(32., device='cuda:0'), 'loss_ce_2': tensor(1.5016, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1361, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9914, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(48.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4819, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1328, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9859, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(44., device='cuda:0'), 'loss_ce_4': tensor(1.4592, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1363, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9377, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(16., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.8832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [4]  [  0/100]  eta: 0:01:10  lr: 0.000100  class_error: 79.31  loss: 25.0643 (25.0643)  loss_ce: 1.4801 (1.4801)  loss_bbox: 0.6625 (0.6625)  loss_giou: 2.0871 (2.0871)  loss_ce_0: 1.4262 (1.4262)  loss_bbox_0: 0.7483 (0.7483)  loss_giou_0: 2.2225 (2.2225)  loss_ce_1: 1.4461 (1.4461)  loss_bbox_1: 0.6975 (0.6975)  loss_giou_1: 1.9953 (1.9953)  loss_ce_2: 1.5016 (1.5016)  loss_bbox_2: 0.6806 (0.6806)  loss_giou_2: 1.9828 (1.9828)  loss_ce_3: 1.4819 (1.4819)  loss_bbox_3: 0.6640 (0.6640)  loss_giou_3: 1.9719 (1.9719)  loss_ce_4: 1.4592 (1.4592)  loss_bbox_4: 0.6814 (0.6814)  loss_giou_4: 1.8754 (1.8754)  loss_ce_unscaled: 1.4801 (1.4801)  class_error_unscaled: 79.3103 (79.3103)  loss_bbox_unscaled: 0.1325 (0.1325)  loss_giou_unscaled: 1.0435 (1.0435)  cardinality_error_unscaled: 49.5000 (49.5000)  loss_ce_0_unscaled: 1.4262 (1.4262)  loss_bbox_0_unscaled: 0.1497 (0.1497)  loss_giou_0_unscaled: 1.1112 (1.1112)  cardinality_error_0_unscaled: 19.5000 (19.5000)  loss_ce_1_unscaled: 1.4461 (1.4461)  loss_bbox_1_unscaled: 0.1395 (0.1395)  loss_giou_1_unscaled: 0.9976 (0.9976)  cardinality_error_1_unscaled: 32.0000 (32.0000)  loss_ce_2_unscaled: 1.5016 (1.5016)  loss_bbox_2_unscaled: 0.1361 (0.1361)  loss_giou_2_unscaled: 0.9914 (0.9914)  cardinality_error_2_unscaled: 48.5000 (48.5000)  loss_ce_3_unscaled: 1.4819 (1.4819)  loss_bbox_3_unscaled: 0.1328 (0.1328)  loss_giou_3_unscaled: 0.9859 (0.9859)  cardinality_error_3_unscaled: 44.0000 (44.0000)  loss_ce_4_unscaled: 1.4592 (1.4592)  loss_bbox_4_unscaled: 0.1363 (0.1363)  loss_giou_4_unscaled: 0.9377 (0.9377)  cardinality_error_4_unscaled: 16.0000 (16.0000)  time: 0.7012  data: 0.3403  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.2466, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(75., device='cuda:0'), 'loss_bbox': tensor(0.1997, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1947, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(26.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3153, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2569, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.3679, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(10., device='cuda:0'), 'loss_ce_1': tensor(1.2507, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1804, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0930, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(20., device='cuda:0'), 'loss_ce_2': tensor(1.2721, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1502, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9663, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(32.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2312, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1590, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0283, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(20.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2985, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1526, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9719, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(4.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(29.2433, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3101, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(81.8182, device='cuda:0'), 'loss_bbox': tensor(0.2048, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9492, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(34.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2301, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2365, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1169, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(26.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2998, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1881, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9309, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(39., device='cuda:0'), 'loss_ce_2': tensor(1.3474, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1909, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8943, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(43.5000, device='cuda:0'), 'loss_ce_3': tensor(1.3105, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1915, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9034, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(41.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3012, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1881, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8701, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(11., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.9257, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2489, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(69.5652, device='cuda:0'), 'loss_bbox': tensor(0.2187, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.2510, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(28., device='cuda:0'), 'loss_ce_0': tensor(1.2120, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2981, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.4010, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(21.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2955, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1952, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.1638, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(36.5000, device='cuda:0'), 'loss_ce_2': tensor(1.2965, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1743, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0913, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(42., device='cuda:0'), 'loss_ce_3': tensor(1.2647, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1678, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0903, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(29., device='cuda:0'), 'loss_ce_4': tensor(1.2746, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1719, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.1021, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(6., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(30.7183, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1380, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(71.1111, device='cuda:0'), 'loss_bbox': tensor(0.1340, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9716, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(30., device='cuda:0'), 'loss_ce_0': tensor(1.0487, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1856, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.1588, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(35., device='cuda:0'), 'loss_ce_1': tensor(1.0512, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1557, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(1.0032, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(50., device='cuda:0'), 'loss_ce_2': tensor(1.0652, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1379, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9261, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(54., device='cuda:0'), 'loss_ce_3': tensor(1.0693, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1464, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9764, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(31.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1332, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1508, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0094, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(7.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.9052, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3053, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(30.7692, device='cuda:0'), 'loss_bbox': tensor(0.1887, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8747, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(52.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3296, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1824, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8878, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(38., device='cuda:0'), 'loss_ce_1': tensor(1.3761, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1811, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8263, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(63., device='cuda:0'), 'loss_ce_2': tensor(1.2964, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1838, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8518, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62., device='cuda:0'), 'loss_ce_3': tensor(1.3608, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1847, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8153, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(56.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3402, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1977, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9148, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(25.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.7899, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1859, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(46.4286, device='cuda:0'), 'loss_bbox': tensor(0.1584, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8990, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(55.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1717, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1571, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9473, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(54.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2159, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1551, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8203, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(71.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1971, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1489, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8423, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(69., device='cuda:0'), 'loss_ce_3': tensor(1.2164, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1503, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8321, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(59., device='cuda:0'), 'loss_ce_4': tensor(1.2243, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1593, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8504, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(34., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.0466, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3549, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(78.9474, device='cuda:0'), 'loss_bbox': tensor(0.1003, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7205, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(59., device='cuda:0'), 'loss_ce_0': tensor(1.3450, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1522, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9877, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(55.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3794, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1132, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7508, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(66.5000, device='cuda:0'), 'loss_ce_2': tensor(1.4087, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1057, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.7292, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(69., device='cuda:0'), 'loss_ce_3': tensor(1.3511, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1079, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.7692, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(55., device='cuda:0'), 'loss_ce_4': tensor(1.3517, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1276, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8273, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(34., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.1113, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4474, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(45.1613, device='cuda:0'), 'loss_bbox': tensor(0.1411, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8243, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(67., device='cuda:0'), 'loss_ce_0': tensor(1.3533, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1663, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9870, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(63.5000, device='cuda:0'), 'loss_ce_1': tensor(1.3675, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1510, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8980, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(71., device='cuda:0'), 'loss_ce_2': tensor(1.3565, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1311, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8180, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(70., device='cuda:0'), 'loss_ce_3': tensor(1.3737, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1352, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8242, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(72.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3854, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1382, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8855, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(51., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.8518, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.4258, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(50., device='cuda:0'), 'loss_bbox': tensor(0.2441, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8529, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(75.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4453, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.2318, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8806, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(68.5000, device='cuda:0'), 'loss_ce_1': tensor(1.4255, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.2320, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8077, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(77., device='cuda:0'), 'loss_ce_2': tensor(1.3563, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.2322, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8491, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(73., device='cuda:0'), 'loss_ce_3': tensor(1.3780, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.2431, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8773, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(68., device='cuda:0'), 'loss_ce_4': tensor(1.3848, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.2393, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8304, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(51.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.5561, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3196, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(33.3333, device='cuda:0'), 'loss_bbox': tensor(0.1825, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9657, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(70., device='cuda:0'), 'loss_ce_0': tensor(1.2800, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1887, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9252, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(70.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2767, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1866, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9421, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(72., device='cuda:0'), 'loss_ce_2': tensor(1.2235, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1811, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9068, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(68.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2419, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1912, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9718, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(67.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3010, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1823, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9349, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(50.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.2468, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [4]  [ 10/100]  eta: 0:00:29  lr: 0.000100  class_error: 33.33  loss: 24.4976 (24.3985)  loss_ce: 1.3101 (1.3148)  loss_bbox: 0.9126 (0.8658)  loss_giou: 1.8985 (1.9177)  loss_ce_0: 1.3153 (1.2870)  loss_bbox_0: 0.9282 (1.0024)  loss_giou_0: 1.9754 (2.1402)  loss_ce_1: 1.2998 (1.3077)  loss_bbox_1: 0.9020 (0.8536)  loss_giou_1: 1.8617 (1.8607)  loss_ce_2: 1.2965 (1.3019)  loss_bbox_2: 0.7508 (0.8054)  loss_giou_2: 1.7887 (1.7939)  loss_ce_3: 1.3105 (1.2981)  loss_bbox_3: 0.7948 (0.8226)  loss_giou_3: 1.8069 (1.8317)  loss_ce_4: 1.3012 (1.3140)  loss_bbox_4: 0.7964 (0.8382)  loss_giou_4: 1.8297 (1.8426)  loss_ce_unscaled: 1.3101 (1.3148)  class_error_unscaled: 69.5652 (60.1313)  loss_bbox_unscaled: 0.1825 (0.1732)  loss_giou_unscaled: 0.9492 (0.9589)  cardinality_error_unscaled: 52.5000 (49.8182)  loss_ce_0_unscaled: 1.3153 (1.2870)  loss_bbox_0_unscaled: 0.1856 (0.2005)  loss_giou_0_unscaled: 0.9877 (1.0701)  cardinality_error_0_unscaled: 38.0000 (42.0909)  loss_ce_1_unscaled: 1.2998 (1.3077)  loss_bbox_1_unscaled: 0.1804 (0.1707)  loss_giou_1_unscaled: 0.9309 (0.9303)  cardinality_error_1_unscaled: 63.0000 (54.4091)  loss_ce_2_unscaled: 1.2965 (1.3019)  loss_bbox_2_unscaled: 0.1502 (0.1611)  loss_giou_2_unscaled: 0.8943 (0.8970)  cardinality_error_2_unscaled: 62.0000 (57.4545)  loss_ce_3_unscaled: 1.3105 (1.2981)  loss_bbox_3_unscaled: 0.1590 (0.1645)  loss_giou_3_unscaled: 0.9034 (0.9158)  cardinality_error_3_unscaled: 55.0000 (49.5455)  loss_ce_4_unscaled: 1.3012 (1.3140)  loss_bbox_4_unscaled: 0.1593 (0.1676)  loss_giou_4_unscaled: 0.9148 (0.9213)  cardinality_error_4_unscaled: 25.5000 (26.5000)  time: 0.3231  data: 0.0397  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.1994, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(32.1429, device='cuda:0'), 'loss_bbox': tensor(0.1626, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9798, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(79.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1798, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1694, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9606, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(70.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1958, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1673, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9221, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(69., device='cuda:0'), 'loss_ce_2': tensor(1.1300, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1927, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1703, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(64.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1559, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1961, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1179, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(68., device='cuda:0'), 'loss_ce_4': tensor(1.1845, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1799, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0149, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(57., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(27.5157, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0595, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(10.6383, device='cuda:0'), 'loss_bbox': tensor(0.1599, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9778, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(67.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1294, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1166, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8448, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(66.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0609, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1327, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9057, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(62., device='cuda:0'), 'loss_ce_2': tensor(1.0130, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1412, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9864, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(57.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0120, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1627, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0288, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(54.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0500, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1545, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9571, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(46.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(24.8663, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2495, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(32.5581, device='cuda:0'), 'loss_bbox': tensor(0.1123, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7772, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(71., device='cuda:0'), 'loss_ce_0': tensor(1.2935, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1100, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8139, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(64., device='cuda:0'), 'loss_ce_1': tensor(1.2535, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1131, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8049, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(67., device='cuda:0'), 'loss_ce_2': tensor(1.2056, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1773, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.0931, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(56., device='cuda:0'), 'loss_ce_3': tensor(1.2267, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1702, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0666, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(57.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2140, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1457, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8936, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(57., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.2817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3217, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(33.3333, device='cuda:0'), 'loss_bbox': tensor(0.1657, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.8539, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(88.5000, device='cuda:0'), 'loss_ce_0': tensor(1.3382, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1333, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8052, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(85.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2473, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1353, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7998, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(80., device='cuda:0'), 'loss_ce_2': tensor(1.2439, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1568, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9661, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(70.5000, device='cuda:0'), 'loss_ce_3': tensor(1.2611, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1601, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.0046, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(80.5000, device='cuda:0'), 'loss_ce_4': tensor(1.3068, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1549, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8800, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(85.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.6754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2092, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(21.7391, device='cuda:0'), 'loss_bbox': tensor(0.1335, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9658, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(72.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2478, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1269, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8993, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(67., device='cuda:0'), 'loss_ce_1': tensor(1.2443, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1378, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9859, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(57.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1709, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1557, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(1.1192, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(53.5000, device='cuda:0'), 'loss_ce_3': tensor(1.1856, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1639, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(1.1223, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(56.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1667, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1657, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(1.0815, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(62.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(26.8797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0260, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(18.1818, device='cuda:0'), 'loss_bbox': tensor(0.0995, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7811, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(83.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0762, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1003, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8291, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(79., device='cuda:0'), 'loss_ce_1': tensor(0.9920, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1034, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.7803, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(74., device='cuda:0'), 'loss_ce_2': tensor(0.9741, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1083, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8480, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(66., device='cuda:0'), 'loss_ce_3': tensor(0.9767, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1105, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9101, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(76.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0062, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1093, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8468, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(81.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(22.0411, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.2051, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(39.3939, device='cuda:0'), 'loss_bbox': tensor(0.0877, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7900, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(77.5000, device='cuda:0'), 'loss_ce_0': tensor(1.2812, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1005, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9174, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(74.5000, device='cuda:0'), 'loss_ce_1': tensor(1.2627, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.0856, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8341, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(63., device='cuda:0'), 'loss_ce_2': tensor(1.2327, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.0859, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8645, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(59., device='cuda:0'), 'loss_ce_3': tensor(1.1640, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.0931, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8956, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(71.5000, device='cuda:0'), 'loss_ce_4': tensor(1.2136, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1123, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8745, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(77.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.3701, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0225, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(17.1429, device='cuda:0'), 'loss_bbox': tensor(0.1407, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.7962, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(76.5000, device='cuda:0'), 'loss_ce_0': tensor(0.9933, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1356, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8933, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(67., device='cuda:0'), 'loss_ce_1': tensor(1.0709, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1419, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9150, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58.5000, device='cuda:0'), 'loss_ce_2': tensor(1.0347, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1332, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8920, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(56., device='cuda:0'), 'loss_ce_3': tensor(1.0492, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1313, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8155, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(70., device='cuda:0'), 'loss_ce_4': tensor(0.9989, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1461, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8365, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(78., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.4004, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0627, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(25.4902, device='cuda:0'), 'loss_bbox': tensor(0.1605, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.0268, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(67.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0627, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1700, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(1.0141, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(60., device='cuda:0'), 'loss_ce_1': tensor(1.1268, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1558, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9722, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(48.5000, device='cuda:0'), 'loss_ce_2': tensor(1.1002, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1485, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9819, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(47.5000, device='cuda:0'), 'loss_ce_3': tensor(1.0802, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1513, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.9622, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(60.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0381, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1605, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9586, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(68., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.8368, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.0585, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(29.4118, device='cuda:0'), 'loss_bbox': tensor(0.1440, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(0.9559, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(74.5000, device='cuda:0'), 'loss_ce_0': tensor(1.0500, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1177, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8367, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(67.5000, device='cuda:0'), 'loss_ce_1': tensor(1.0853, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1204, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8485, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(67., device='cuda:0'), 'loss_ce_2': tensor(1.0442, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1185, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8500, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(60., device='cuda:0'), 'loss_ce_3': tensor(1.0642, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1195, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8376, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(77.5000, device='cuda:0'), 'loss_ce_4': tensor(1.0796, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1182, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8053, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(81.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(23.1824, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: [4]  [ 20/100]  eta: 0:00:24  lr: 0.000100  class_error: 29.41  loss: 23.0356 (23.2492)  loss_ce: 1.2092 (1.2322)  loss_bbox: 0.7918 (0.7789)  loss_giou: 1.7980 (1.8526)  loss_ce_0: 1.2301 (1.2290)  loss_bbox_0: 0.7853 (0.8299)  loss_giou_0: 1.8347 (1.9605)  loss_ce_1: 1.2473 (1.2345)  loss_bbox_1: 0.7551 (0.7551)  loss_giou_1: 1.7960 (1.8097)  loss_ce_2: 1.2056 (1.2129)  loss_bbox_2: 0.7443 (0.7595)  loss_giou_2: 1.7887 (1.8703)  loss_ce_3: 1.2164 (1.2121)  loss_bbox_3: 0.7948 (0.7782)  loss_giou_3: 1.8202 (1.8891)  loss_ce_4: 1.2140 (1.2244)  loss_bbox_4: 0.7727 (0.7836)  loss_giou_4: 1.7711 (1.8365)  loss_ce_unscaled: 1.2092 (1.2322)  class_error_unscaled: 33.3333 (43.8799)  loss_bbox_unscaled: 0.1584 (0.1558)  loss_giou_unscaled: 0.8990 (0.9263)  cardinality_error_unscaled: 67.5000 (62.2143)  loss_ce_0_unscaled: 1.2301 (1.2290)  loss_bbox_0_unscaled: 0.1571 (0.1660)  loss_giou_0_unscaled: 0.9174 (0.9803)  cardinality_error_0_unscaled: 64.0000 (55.4524)  loss_ce_1_unscaled: 1.2473 (1.2345)  loss_bbox_1_unscaled: 0.1510 (0.1510)  loss_giou_1_unscaled: 0.8980 (0.9049)  cardinality_error_1_unscaled: 63.0000 (59.2857)  loss_ce_2_unscaled: 1.2056 (1.2129)  loss_bbox_2_unscaled: 0.1489 (0.1519)  loss_giou_2_unscaled: 0.8943 (0.9352)  cardinality_error_2_unscaled: 59.0000 (58.2143)  loss_ce_3_unscaled: 1.2164 (1.2121)  loss_bbox_3_unscaled: 0.1590 (0.1556)  loss_giou_3_unscaled: 0.9101 (0.9446)  cardinality_error_3_unscaled: 59.0000 (58.0000)  loss_ce_4_unscaled: 1.2140 (1.2244)  loss_bbox_4_unscaled: 0.1545 (0.1567)  loss_giou_4_unscaled: 0.8855 (0.9183)  cardinality_error_4_unscaled: 51.0000 (46.9762)  time: 0.2806  data: 0.0112  max mem: 1537\n",
            "loss_dict {'loss_ce': tensor(1.5428, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(65.3846, device='cuda:0'), 'loss_bbox': tensor(0.2266, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1127, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(79.5000, device='cuda:0'), 'loss_ce_0': tensor(1.5264, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1805, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.8480, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(71.5000, device='cuda:0'), 'loss_ce_1': tensor(1.5246, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1762, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8603, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(64., device='cuda:0'), 'loss_ce_2': tensor(1.5422, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1750, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8864, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(71., device='cuda:0'), 'loss_ce_3': tensor(1.5802, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1777, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8718, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(76.5000, device='cuda:0'), 'loss_ce_4': tensor(1.6097, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1774, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.8936, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(84.5000, device='cuda:0')}\n",
            "total loss with DAN is:  tensor(28.6499, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.1104, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(34.6939, device='cuda:0'), 'loss_bbox': tensor(0.2210, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.1413, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(63.5000, device='cuda:0'), 'loss_ce_0': tensor(1.1040, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1523, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.9509, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(57.5000, device='cuda:0'), 'loss_ce_1': tensor(1.1394, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1447, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.9719, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(58., device='cuda:0'), 'loss_ce_2': tensor(1.1059, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1434, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.9106, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(57., device='cuda:0'), 'loss_ce_3': tensor(1.1189, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1478, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8807, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(71.5000, device='cuda:0'), 'loss_ce_4': tensor(1.1179, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1626, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9322, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(73., device='cuda:0')}\n",
            "total loss with DAN is:  tensor(25.8961, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss_dict {'loss_ce': tensor(1.3815, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'class_error': tensor(48.7805, device='cuda:0'), 'loss_bbox': tensor(0.2989, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou': tensor(1.3092, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error': tensor(73.5000, device='cuda:0'), 'loss_ce_0': tensor(1.4187, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_0': tensor(0.1129, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_0': tensor(0.7997, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_0': tensor(65., device='cuda:0'), 'loss_ce_1': tensor(1.4093, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_1': tensor(0.1231, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_1': tensor(0.8555, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_1': tensor(61., device='cuda:0'), 'loss_ce_2': tensor(1.4382, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_2': tensor(0.1202, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_2': tensor(0.8191, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_2': tensor(62.5000, device='cuda:0'), 'loss_ce_3': tensor(1.4335, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_3': tensor(0.1296, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_3': tensor(0.8789, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_3': tensor(74.5000, device='cuda:0'), 'loss_ce_4': tensor(1.4775, device='cuda:0', grad_fn=<NllLoss2DBackward0>), 'loss_bbox_4': tensor(0.1621, device='cuda:0', grad_fn=<DivBackward0>), 'loss_giou_4': tensor(0.9600, device='cuda:0', grad_fn=<DivBackward0>), 'cardinality_error_4': tensor(78.5000, device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-cba15418f2a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmain_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-1465182eeffc>\u001b[0m in \u001b[0;36mmain_train\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    184\u001b[0m         train_stats = train_one_epoch(\n\u001b[1;32m    185\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             args.clip_max_norm)\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-fe3422d6af02>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, data_loader1, optimizer, device, epoch, max_norm)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moutputs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_domain11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_domain12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m# weight_dict1 = criterion.weight_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# losses1 = sum(loss_dict1[k] * weight_dict1[k] for k in loss_dict1.keys() if k in weight_dict1 and k != 'domain_loss')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9551048082b6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_tensor_from_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-6b3c349bd113>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# position encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-8b3f22ab249c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdim_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_pos_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mdim_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim_t\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_pos_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mpos_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_embed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdim_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__rpow__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rpow__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# See https://github.com/pytorch/pytorch/issues/75462\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}